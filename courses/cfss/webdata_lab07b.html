<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Joshua G. Mausolf" />


<title>Topic Modeling for Text and Tweets in Python</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<p><strong>This content is from the fall 2016 version of this course. Please go <a href = "https://uc-cfss.github.io">here</a> for the most recent version.</strong></p>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Topic Modeling for Text and Tweets in Python</h1>
<h4 class="author"><em>Joshua G. Mausolf</em></h4>
<h4 class="date"><em>November 8, 2016</em></h4>

</div>


<div id="prerequisites" class="section level1">
<h1>Prerequisites:</h1>
<p>If you have not already done so, you will need to properly install an Anaconda distribution of Python, following the installation <a href="https://uc-cfss.github.io/setup00.html">instructions from the first week</a>.</p>
<p>I would also recommend installing a friendly text editor for editing scripts such as <a href="https://atom.io">Atom</a>. Once installed, you can start a new script by simply typing in bash <code>atom name_of_your_new_script</code>. You can edit an existing script by using <code>atom name_of_script</code>. <a href="https://www.sublimetext.com">SublimeText</a> also works similar to Atom. Alternatively, you may use a native text editor such as <a href="https://www.linux.com/learn/vim-101-beginners-guide-vim">Vim</a>, but this has a higher learning curve.</p>
<p><a href="http://stackoverflow.com/questions/22390709/open-atom-editor-from-command-line"><em>Note: If <code>atom</code> does not automatically work, try these solutions</em></a>.</p>
<div id="installing-new-python-packages" class="section level3">
<h3>Installing New Python Packages</h3>
<p>If you do not have a package, you may use the <a href="https://packaging.python.org/installing/">Python package manager <code>pip</code></a> (a default python program) to install it. Note that pip is called directly from the Shell (not in a python interpreter).</p>
<p>To begin, <strong>update pip</strong>.</p>
<div id="on-mac-or-linux" class="section level4">
<h4>On Mac or Linux</h4>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">pip</span> install -U pip setuptools</code></pre></div>
</div>
<div id="on-windows" class="section level4">
<h4>On Windows</h4>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">python</span> -m pip install -U pip setuptools</code></pre></div>
<p>To see further prerequisites, please visit the <a href="https://github.com/jmausolf/Python_Tutorials/tree/master/Topic_Models_for_Text">tutorial README</a>.</p>
<hr />
</div>
</div>
</div>
<div id="using-python-for-topic-modeling" class="section level1">
<h1>Using Python for Topic Modeling</h1>
<p>In short, topic models are a form of unsupervised algorithms that are used to discover hidden patterns or topic clusters in text data. Today, we will be exploring the application of topic modeling in Python on previously collected raw text data and Twitter data.</p>
<p>The primary package used for these topic modeling comes from the Sci-Kit Learn (Sklearn) a Python package frequently used for machine learning. In particular, we are using Sklearn’s <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition">Matrix Decomposition</a> and <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction">Feature Extraction</a> modules.</p>
<hr />
</div>
<div id="quick-start-guide" class="section level1">
<h1>Quick Start Guide</h1>
<div id="clone-the-repository" class="section level3">
<h3>Clone the Repository</h3>
<div id="begin-by-cloning-the-tutorials-repository-and-navigating-to-the-topic-model-tutorial" class="section level5">
<h5>Begin by <a href="https://github.com/jmausolf/Python_Tutorials">Cloning the Tutorials Repository</a> and Navigating to the Topic Model Tutorial</h5>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">git</span> clone https://github.com/jmausolf/Python_Tutorials
<span class="kw">cd</span> Python_Tutorials/Topic_Models_for_Text/</code></pre></div>
</div>
</div>
<div id="run-a-model-examples" class="section level3">
<h3>Run a Model (Examples)</h3>
<p>Some sample data has already been included in the repo. Try running the below example commands:</p>
<div id="run-a-non-negative-matrix-factorization-nmf-topic-model-using-a-tfidf-vectorizer-with-custom-tokenization" class="section level5">
<h5>Run a Non-Negative Matrix Factorization (NMF) topic model using a TFIDF vectorizer with custom tokenization</h5>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># Run the NMF Model on Presidential Speech</span>
<span class="kw">python</span> topic_modelr.py <span class="st">&quot;text_tfidf_custom&quot;</span> <span class="st">&quot;nmf&quot;</span> 15 10 2 4 <span class="st">&quot;data/president&quot;</span></code></pre></div>
</div>
<div id="partial-results" class="section level4">
<h4><em>Partial Results</em>:</h4>
<pre><code>Topic 0: make sure, need help, pass jobs, fair share, tax breaks, congress pass, pay fair, pay fair share, higher tax, fair shake
Topic 1: prime minister, welcome prime, welcome prime minister, september prime, september prime minister, looking forward, bilateral meeting, want welcome prime, want welcome prime minister, forward productive discussion
Topic 2: make sure, making sure, health care, sure got, got lot, half years, going able, vision america, health insurance, tax code
</code></pre>
<div id="run-a-latent-dirichlet-allocation-lda-topic-model-using-a-tfidf-vectorizer-with-custom-tokenization" class="section level5">
<h5>Run a Latent Dirichlet Allocation (LDA) topic model using a TFIDF vectorizer with custom tokenization</h5>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># Run the LDA Model on Clinton Tweets</span>
<span class="kw">python</span> topic_modelr.py <span class="st">&quot;tweet_tfidf_custom&quot;</span> <span class="st">&quot;lda&quot;</span> 15 5 1 4 <span class="st">&quot;data/twitter&quot;</span></code></pre></div>
</div>
</div>
</div>
<div id="what-is-going-on-here" class="section level3">
<h3>What is going on here?</h3>
<p>First, understand what is going on here. You are calling a Python script that utilizes various Python libraries, particularly Sklearn, to analyze text data that is in your cloned repo. This script is an example of what you could write on your own using Python.</p>
<p>To get a better idea of the script’s parameters, query the help function from the command line.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">python</span> topic_modelr.py --help</code></pre></div>
<div id="this-yields-the-following" class="section level5">
<h5>This yields the following:</h5>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">
<span class="kw">usage</span>: topic_modelr.py [-h]
                       <span class="kw">vectorizer_type</span> topic_clf n_topics n_top_terms
                       <span class="kw">req_ngram_range</span> [req_ngram_range ...] file_path

<span class="kw">Prepare</span> input file

<span class="kw">positional</span> arguments:
  <span class="kw">vectorizer_type</span>  Select the desired vectorizer for either text or tweet
                   <span class="kw">@</span> text_tfidf_std       <span class="kw">|</span> <span class="kw">TFIDF</span> Vectorizer (for text)
                   <span class="kw">@</span> text_tfidf_custom    <span class="kw">|</span> <span class="kw">TFIDF</span> Vectorizer with Custom Tokenizer (for text)
                   <span class="kw">@</span> text_count_std       <span class="kw">|</span> <span class="kw">Count</span> Vectorizer
                   
                   <span class="kw">@</span> tweet_tfidf_std      <span class="kw">|</span> <span class="kw">TFIDF</span> Vectorizer (for tweets)
                   <span class="kw">@</span> tweet_tfidf_custom   <span class="kw">|</span> <span class="kw">TFIDF</span> Vectorizer with Custom Tokenizer (for tweets)
                   
  <span class="kw">topic_clf</span>        Select the desired topic model classifier (clf)
                   <span class="kw">@</span> lda     <span class="kw">|</span> <span class="kw">Topic</span> Model: LatentDirichletAllocation (LDA)
                   <span class="kw">@</span> nmf     <span class="kw">|</span> <span class="kw">Topic</span> Model: Non-Negative Matrix Factorization (NMF)
                   <span class="kw">@</span> pca     <span class="kw">|</span> <span class="kw">Topic</span> Model: Principal Components Analysis (PCA)
                   
  <span class="kw">n_topics</span>         Select the number of topics to return (as integer)
                   <span class="kw">Note</span>: requires n <span class="kw">&gt;</span>= number of text files or tweets
                   
                   <span class="kw">Consider</span> the following examples:
                   
                   <span class="kw">@</span> 10     <span class="kw">|</span> <span class="kw">Example</span>: Returns 5 topics
                   <span class="kw">@</span> 15     <span class="kw">|</span> <span class="kw">Example</span>: Returns 10 topics
                   
  <span class="kw">n_top_terms</span>      Select the number of top terms to return for each topic (as integer)
                   <span class="kw">Consider</span> the following examples:
                   
                   <span class="kw">@</span> 10     <span class="kw">|</span> <span class="kw">Example</span>: Returns 10 terms for each topic
                   <span class="kw">@</span> 15     <span class="kw">|</span> <span class="kw">Example</span>: Returns 15 terms for each topic
                   
  <span class="kw">req_ngram_range</span>  Select the requested <span class="st">&#39;ngram&#39;</span> or number of words per term
                   <span class="kw">@</span> NG-1:  <span class="kw">|</span> <span class="kw">ngram</span> of length 1, e.g. <span class="st">&quot;pay&quot;</span>
                   <span class="kw">@</span> NG-2:  <span class="kw">|</span> <span class="kw">ngram</span> of length 2, e.g. <span class="st">&quot;fair share&quot;</span>
                   <span class="kw">@</span> NG-3:  <span class="kw">|</span> <span class="kw">ngram</span> of length 3, e.g. <span class="st">&quot;pay fair share&quot;</span>
                   
                   <span class="kw">Consider</span> the following ngram range examples:
                   
                   <span class="kw">@</span> [1, 2] <span class="kw">|</span> <span class="kw">Return</span> ngrams of lengths 1 and 2
                   <span class="kw">@</span> [2, 5] <span class="kw">|</span> <span class="kw">Return</span> ngrams of lengths 2 through 5
                   
  <span class="kw">file_path</span>        Select the desired file path for the data
                   
                   <span class="kw">Consider</span> the following ngram range examples:
                   
                   <span class="kw">@</span> data/twitter      <span class="kw">|</span> <span class="kw">Uses</span> data in the data/twitter subdirectory
                   <span class="kw">@</span> data/president    <span class="kw">|</span> <span class="kw">Uses</span> data in the data/president subdirectory
                   <span class="kw">@</span> .                 <span class="kw">|</span> <span class="kw">Uses</span> data in the current directory
                   

<span class="kw">optional</span> arguments:
  <span class="kw">-h</span>, --help       show this help message and exit</code></pre></div>
</div>
<div id="returning-to-our-past-example" class="section level5">
<h5>Returning to our past example:</h5>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">python</span> topic_modelr.py <span class="st">&quot;text_tfidf_custom&quot;</span> <span class="st">&quot;nmf&quot;</span> 15 10 2 4 <span class="st">&quot;data/president&quot;</span></code></pre></div>
</div>
<div id="breaking-this-down" class="section level5">
<h5>Breaking this down:</h5>
<ul>
<li><code>python topic_modelr.py</code>: We initialize the model with this statement.</li>
<li><code>&quot;text_tfidf_custom&quot;</code>: The next statement selects the vectorizer, which follows the format <code>&lt;doc_type&gt;_&lt;vectorizer_method&gt;_&lt;tokenizer&gt;</code>, thus <code>text_tfidf_custom</code>. We are analyzing text files using the tfidf vectorizer and a custom tokenizer. The custom tokenizer can remove additional stop-words from your topic model. You can modify this list in the <code>custom_stopword_tokens.py</code> file.</li>
<li><code>&quot;nmf&quot;</code>: This specifies the topic model, in this case a Non-Negative Matrix Factorization (NMF)</li>
<li><code>15</code> : 15 topics</li>
<li><code>10</code> : 10 terms (ngrams) per topic. An ngram is one or more words</li>
<li><code>2 4</code>: The ngram range. Get all ngrams between 2 and 4 words in length (excludes single words). Thus, “fair share” and “pay fair share” are examples of 2grams and 3grams.</li>
<li><code>&quot;data/president&quot;</code>: The relative file path to the data.</li>
</ul>
<p>For example, you can list the above data files using the following command:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ls</span> data/president</code></pre></div>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">2011-09-17_ID1.txt</span> 2011-09-21_ID2.txt 2011-09-24_ID2.txt 2011-09-28_ID1.txt 2011-10-04_ID5.txt
<span class="kw">2011-09-19_ID1.txt</span> 2011-09-21_ID3.txt 2011-09-25_ID1.txt 2011-09-28_ID2.txt 2011-10-04_ID6.txt
<span class="kw">2011-09-19_ID2.txt</span> 2011-09-21_ID4.txt 2011-09-25_ID2.txt 2011-09-30_ID1.txt 2011-10-05_ID1.txt
<span class="kw">2011-09-19_ID3.txt</span> 2011-09-21_ID5.txt 2011-09-25_ID3.txt 2011-09-30_ID2.txt 2011-10-05_ID2.txt
<span class="kw">2011-09-20_ID1.txt</span> 2011-09-21_ID6.txt 2011-09-25_ID4.txt 2011-10-01_ID1.txt 2011-10-05_ID3.txt
<span class="kw">2011-09-20_ID2.txt</span> 2011-09-21_ID7.txt 2011-09-26_ID1.txt 2011-10-01_ID2.txt 2011-10-06_ID1.txt
<span class="kw">2011-09-20_ID3.txt</span> 2011-09-21_ID8.txt 2011-09-26_ID2.txt 2011-10-03_ID1.txt 2011-10-06_ID2.txt
<span class="kw">2011-09-20_ID4.txt</span> 2011-09-21_ID9.txt 2011-09-26_ID3.txt 2011-10-03_ID2.txt 2011-10-07_ID1.txt
<span class="kw">2011-09-20_ID5.txt</span> 2011-09-22_ID1.txt 2011-09-26_ID4.txt 2011-10-04_ID1.txt results
<span class="kw">2011-09-20_ID6.txt</span> 2011-09-23_ID1.txt 2011-09-26_ID6.txt 2011-10-04_ID2.txt
<span class="kw">2011-09-21_ID0.txt</span> 2011-09-23_ID2.txt 2011-09-27_ID1.txt 2011-10-04_ID3.txt
<span class="kw">2011-09-21_ID1.txt</span> 2011-09-24_ID1.txt 2011-09-27_ID2.txt 2011-10-04_ID4.txt</code></pre></div>
<hr />
</div>
</div>
</div>
<div id="whats-under-the-hood" class="section level1">
<h1>What’s Under the Hood?</h1>
<p>Remember that this script is a simple Python script using Sklearn’s models. At first glance, the code may appear complex given it’s ability to handle various input sources (text or tweet), use different vectorizers, tokenizers, and models. The key components can be seen in the <code>topic_modeler</code> function:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
    <span class="co"># SPECIFY VECTORIZER ALGORITHM</span>
    vectorizer <span class="op">=</span> select_vectorizer(vectorizer_type, ngram_lengths)


    <span class="co"># Vectorizer Results</span>
    dtm <span class="op">=</span> vectorizer.fit_transform(filenames).toarray()
    vocab <span class="op">=</span> np.array(vectorizer.get_feature_names())
    <span class="bu">print</span>(<span class="st">&quot;Evaluating vocabulary...&quot;</span>)
    <span class="bu">print</span>(<span class="st">&quot;Found {} terms in {} files...&quot;</span>.<span class="bu">format</span>(dtm.shape[<span class="dv">1</span>], dtm.shape[<span class="dv">0</span>]))


    <span class="co"># DEFINE and BUILD MODEL</span>
    <span class="co">#---------------------------------#</span>

    <span class="cf">if</span> topic_clf <span class="op">==</span> <span class="st">&quot;lda&quot;</span>:

        <span class="co">#Define Topic Model: LatentDirichletAllocation (LDA)</span>
        clf <span class="op">=</span> decomposition.LatentDirichletAllocation(n_topics<span class="op">=</span>num_topics<span class="dv">+1</span>, random_state<span class="op">=</span><span class="dv">3</span>)

  <span class="co">#Other model options ommitted from this snippet (see full code)</span>

    <span class="co">#Fit Topic Model</span>
    doctopic <span class="op">=</span> clf.fit_transform(dtm)
    topic_words <span class="op">=</span> []
    <span class="cf">for</span> topic <span class="op">in</span> clf.components_:
        word_idx <span class="op">=</span> np.argsort(topic)[::<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>:num_top_words]
        topic_words.append([vocab[i] <span class="cf">for</span> i <span class="op">in</span> word_idx])</code></pre></div>
<p>You may notice that this code snippet calls a <code>select_vectorizer()</code> function. This function simply selects the appropriate vectorizer based on user input. An example includes:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
vectorizer <span class="op">=</span> text.TfidfVectorizer(<span class="bu">input</span><span class="op">=</span><span class="st">&#39;filename&#39;</span>, analyzer<span class="op">=</span><span class="st">&#39;word&#39;</span>, ngram_range<span class="op">=</span>(ngram_lengths), stop_words<span class="op">=</span><span class="st">&#39;english&#39;</span>, min_df<span class="op">=</span><span class="dv">2</span>)</code></pre></div>
<p>Note that the structure is in place that this function could be easily modified is you would like to add additional models or classifiers by consulting the <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition">SKlearn Documentation</a></p>
<div id="the-full-topic_modelr.py-script-is-available-on-github." class="section level5">
<h5><a href="https://github.com/jmausolf/Python_Tutorials/blob/master/Topic_Models_for_Text/topic_modelr.py">The full <code>topic_modelr.py</code> script is available on Github.</a></h5>
<hr />
</div>
</div>
<div id="custom-stopwords" class="section level1">
<h1>Custom Stopwords</h1>
<p>In short, stop-words are routine words that we want to exclude from the analysis. They may include common articles like <code>the</code> or <code>a</code>. The Python script uses NLTK to exclude English stop-words and consider only alphabetical words versus numbers and punctuation.</p>
<div id="this-function-is-run-when-a-vectorizer-with-the-custom-extension-is-implemented." class="section level5">
<h5>This function is run when a vectorizer with the <code>custom</code> extension is implemented.</h5>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
<span class="co"># Import Custom User Stopwords (If Any)</span>
<span class="im">from</span> custom_stopword_tokens <span class="im">import</span> custom_stopwords

<span class="kw">def</span> tokenize_nltk(text):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    Note:   This function imports a list of custom stopwords from the user</span>
<span class="co">            If the user does not modify custom stopwords (default=[]),</span>
<span class="co">            there is no substantive update to the stopwords.</span>
<span class="co">    &quot;&quot;&quot;</span>
    tokens <span class="op">=</span> word_tokenize(text)
    text <span class="op">=</span> nltk.Text(tokens)
    stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">&#39;english&#39;</span>))
    stop_words.update(custom_stopwords)
    words <span class="op">=</span> [w.lower() <span class="cf">for</span> w <span class="op">in</span> text <span class="cf">if</span> w.isalpha() <span class="op">and</span> w.lower() <span class="op">not</span> <span class="op">in</span> stop_words]
    <span class="cf">return</span> words
    </code></pre></div>
</div>
<div id="modifying-the-custom-stopwords" class="section level3">
<h3>Modifying the Custom Stopwords</h3>
<p>To modify the custom stop-words, open the <code>custom_stopword_tokens.py</code> file with your favorite text editor, e.g. do one of the following:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#On Mac</span>
<span class="kw">atom</span> custom_stopword_tokens.py <span class="co">#Open with Atom</span>
<span class="kw">open</span> -a SublimeText2 custom_stopword_tokens.py <span class="co">#Open with SublimeText2</span>
<span class="kw">vi</span> custom_stopword_tokens.py <span class="co">#Open with vim</span></code></pre></div>
<p>Once open, simply feel free to add or delete keywords from one of the example lists, or create your own custom keyword list following the template. Save the result, and when you run the script, your custom stop-words will be excluded.</p>
<hr />
</div>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Statistical learning: basics and classification problems</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<p><strong>This content is from the fall 2016 version of this course. Please go <a href = "https://uc-cfss.github.io">here</a> for the most recent version.</strong></p>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical learning: basics and classification problems</h1>

</div>


<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</code></pre></div>
<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define statistical learning</li>
<li>Review the major goals of statistical learning</li>
<li>Explain the difference between parametric and non-parametric methods</li>
<li>Identify the difference between statistical learning and machine learning</li>
<li>Distinguish regression from classification</li>
<li>Demonstrate the use of logistic regression for classification</li>
<li>Identify methods for assessing classification model accuracy</li>
</ul>
</div>
<div id="statistical-learning" class="section level1">
<h1>Statistical learning</h1>
<div id="what-is-statistical-learning" class="section level2">
<h2>What is statistical learning?</h2>
<p>Statistical models attempt to summarize relationships between variables by reducing the dimensionality of the data. For example, here we have some simulated data on sales of <a href="https://www.shamwow.com/">Shamwow</a> in 200 different markets. Our goal is to improve sales of the Shamwow. Since we cannot directly increase sales of the product (unless we go out and buy it ourselves), our only option is to increase advertising across three potential mediums: newspaper, radio, and TV.</p>
<p>In this example, the advertising budgets are our <em>input</em> variables, also called <em>independent variables</em>, <em>features</em>, or <em>predictors</em>. The sales of Shamwows is the <em>output</em>, also called the <em>dependent variable</em> or <em>response</em>.</p>
<p>By plotting the variables against one another using a scatterplot, we can see there is some sort of relationship between each medium’s advertising spending and Shamwow sales:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get advertising data</span>
advertising &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/Advertising.csv&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>() %&gt;%
<span class="st">  </span><span class="kw">select</span>(-X1)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot separate facets for relationship between ad spending and sales</span>
plot_ad &lt;-<span class="st"> </span>advertising %&gt;%
<span class="st">  </span><span class="kw">gather</span>(method, spend, -Sales) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(spend, Sales)) +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>method, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Spending (in thousands of dollars)&quot;</span>)
plot_ad</code></pre></div>
<p><img src="stat_learning01_files/figure-html/plot_ad-1.png" width="672" /></p>
<p>But there seems to be a lot of noise in the data. How can we summarize this? We can do so by estimating a mathematical equation following the general form</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<p>where <span class="math inline">\(f\)</span> is some fixed, unknown function of the relationship between the independent variable(s) <span class="math inline">\(X\)</span> and the dependent variable <span class="math inline">\(Y\)</span>, with some random error <span class="math inline">\(\epsilon\)</span>.</p>
<p>Statistical learning refers to the set of approaches for estimating <span class="math inline">\(f\)</span>. There are many potential approaches to defining the functional form of <span class="math inline">\(f\)</span>. We already saw one method for doing so, <em>ordinary least squares</em> (OLS). Applied here, the results would look like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plot_ad +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/plot_ad_fit-1.png" width="672" /></p>
<p>However statistical learning (and machine learning) allows us to use a wide range of functional forms beyond a simple linear model.</p>
</div>
<div id="why-estimate-f" class="section level2">
<h2>Why estimate <span class="math inline">\(f\)</span>?</h2>
<p>We’ve already discussed the two major goals of statistical modeling:</p>
<ol style="list-style-type: decimal">
<li>Prediction - use our knowledge of the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to predict <span class="math inline">\(Y\)</span> for given values of <span class="math inline">\(X\)</span>. Often the function <span class="math inline">\(f\)</span> is treated as a <em>black box</em> - we don’t care what the function is, as long as it makes accurate predictions. If we are trying to boost sales of Shamwow, we may not care why specific factors drive an increase in sales - we just want to know how to adjust our advertising budgets to maximize sales.</li>
<li>Inference - use our knowledge of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to understand the relationship between the variables. Here we are most interested in the explanation, not the prediction. So in the Shamwow example, we may not care about actual sales of the product - instead, we may be economists who wish to understand how advertising spending influences product sales. We don’t care about the actual product, we simply want to learn more about the process and <em>generalize</em> it to a wider range of settings.</li>
</ol>
</div>
<div id="how-do-we-estimate-f" class="section level2">
<h2>How do we estimate <span class="math inline">\(f\)</span>?</h2>
<p>There are two major approaches to estimating <span class="math inline">\(f\)</span>: parametric and non-parametric methods.</p>
<div id="parametric-methods" class="section level3">
<h3>Parametric methods</h3>
<p>Parametric methods involve a two-stage process:</p>
<ol style="list-style-type: decimal">
<li>First make an assumption about the functional form of <span class="math inline">\(f\)</span>. For instance, OLS assumes that the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <em>linear</em>. This greatly simplifies the problem of estimating the model because we know a great deal about the properties of linear models.</li>
<li>After a model has been selected, we need to <em>fit</em> or <em>train</em> the model using the actual data. We demonstrated this previously with ordinary least squares. The estimation procedure minimizes the sum of the squares of the differences between the observed responses <span class="math inline">\(Y\)</span> and those predicted by a linear function <span class="math inline">\(\hat{Y}\)</span>.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">method_model &lt;-<span class="st"> </span>function(df) {
  <span class="kw">lm</span>(Sales ~<span class="st"> </span>spend, <span class="dt">data =</span> df)
}

ad_pred &lt;-<span class="st"> </span>advertising %&gt;%
<span class="st">  </span><span class="kw">gather</span>(method, spend, -Sales) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(method) %&gt;%
<span class="st">  </span><span class="kw">nest</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(data, method_model),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, broom::augment)) %&gt;%
<span class="st">  </span><span class="kw">unnest</span>(pred)

plot_ad +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) +
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="dt">data =</span> ad_pred,
                 <span class="kw">aes</span>(<span class="dt">ymin =</span> Sales, <span class="dt">ymax =</span> .fitted),
                 <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>,
                 <span class="dt">alpha =</span> .<span class="dv">5</span>) </code></pre></div>
<p><img src="stat_learning01_files/figure-html/plot_parametric-1.png" width="672" /></p>
<p>This is only one possible estimation procedure, but is popular because it is relatively intuitive. This model-based approach is referred to as <em>parametric</em>, because it simplifies the problem of estimating <span class="math inline">\(f\)</span> to estimating a set of parameters in the function:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_{1}X_1\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the sales, <span class="math inline">\(X_1\)</span> is the advertising spending in a given medium (newspaper, radio, or TV), and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters defining the intercept and slope of the line.</p>
<p>The downside to parametric methods is that they assume a specific functional form of the relationship between the variables. Sometimes relationships really are linear - often however they are not. They could be curvilinear, parbolic, interactive, etc. Unless we know this <em>a priori</em> or test for all of these potential functional forms, it is possible our parametric method will not accurately summarize the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="non-parametric-methods" class="section level3">
<h3>Non-parametric methods</h3>
<p>Non-parametric methods do not make any assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead, they use the data itself to estimate <span class="math inline">\(f\)</span> so that it gets as close as possible to the data points without becoming overly complex. By avoiding any assumptions about the functional form, non-parametric methods avoid the issues caused by parametic models. However, by doing so non-parametric methods require a large set of observations to avoid <em>overfitting</em> the data and obtain an accurate estimate of <span class="math inline">\(f\)</span>.</p>
<p>One non-parametric method is <em>locally weighted scatterplot smoothing</em> (LOWESS or LOESS). This method estimates a regression line based on localized subsets of the data, building up the global function <span class="math inline">\(f\)</span> point-by-point. Here is an example of a LOESS on the <code>ethanol</code> dataset in the <code>lattice</code> package:</p>
<p><img src="stat_learning01_files/figure-html/loess-1.png" width="672" /></p>
<p>The LOESS is built up point-by-point:</p>
<video width="672"  controls loop>
<source src="stat_learning01_files/figure-html/loess_buildup-.webm" />
<p>
video of chunk loess_buildup
</p>
</video>
<p>One important argument you can control with LOESS is the <em>span</em>, or how smooth the LOESS function will become. A larger span will result in a smoother curve, but may not be as accurate.</p>
<video width="672"  controls loop>
<source src="stat_learning01_files/figure-html/loess_span-.webm" />
<p>
video of chunk loess_span
</p>
</video>
</div>
</div>
<div id="supervised-vs.unsupervised-learning" class="section level2">
<h2>Supervised vs. unsupervised learning</h2>
<p>All the examples above implement <em>supervised</em> learning. That is, for each observation we have both the predictor measurements and the response measurements (i.e. an <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>). We seek to fit a model that summarizes the relationship between the predictors and response.</p>
<p>In <em>unsupervised</em> learning, all we have is a set of measurements <span class="math inline">\(X\)</span> for a series of observations, but no corresponding response <span class="math inline">\(Y\)</span>. Without an outcome measure, we cannot fit a linear regression model or employ a similar method. That does not mean we cannot use statistical learning to understand the data better. One example of unsupervised learning is <em>cluster analysis</em>. The goal is to determine whether the observations fall into distinct categories. <a href="text02.html">Latent Direchlet allocation (LDA)</a> is an example of cluster analysis applied to text data. In LDA, the individual words are the features or measurements we use to determine the best fitting clusters.</p>
</div>
</div>
<div id="statistical-learning-vs.machine-learning" class="section level1">
<h1>Statistical learning vs. machine learning</h1>
<ul>
<li>Statistical learning
<ul>
<li>Subfield of statistics</li>
<li>Focused predominantly on <em>inference</em></li>
<li>Identify underlying relationships between variables</li>
<li>Emphasizes models and their interpretability</li>
<li>Concerned with uncertainty and precision</li>
</ul></li>
<li>Machine learning
<ul>
<li>Subfield of computer science</li>
<li>Focused predominantly on <em>prediction</em></li>
<li>Generally larger scale applications (think predictive analytics at Google or Netflix)</li>
<li>Emphasizes prediction accuracy</li>
<li>Will sacrifice model interpretability for better accuracy</li>
</ul></li>
<li>In truth, both are quite similar approaches to inference and prediction
<ul>
<li>Both use the same major methods of modeling (parametric and non-parametric)</li>
<li><em>Different language, speaking the same thing</em></li>
</ul></li>
</ul>
</div>
<div id="classification-vs.regression" class="section level1">
<h1>Classification vs. regression</h1>
<p>Variables can be classified as <em>quantitative</em> or <em>qualitative</em>. Quantitative variables take on numeric values. In contrast, qualitative variables take on different <em>classes</em>, or discrete categories. Qualitative variables can have any number of classes, though binary categories are frequent:</p>
<ul>
<li>Yes/no</li>
<li>Male/female</li>
</ul>
<p>Problems with a quantitative dependent variable are typically called <em>regression</em> problems, whereas qualitative dependent variables are called <em>classification</em> problems. Part of this distinction is merely semantic, but different methods may be employed depending on the type of response variable. For instance, you would not use linear regression on a qualitative response variable. Conceptually, how would you define a linear function for a response variable that takes on the values “male” or “female”? It doesn’t make any conceptual sense. Instead, you can employ classification methods such as <em>logistic regression</em> to estimate the probability that based on a set of predictors a specific observation is part of a response class.</p>
<p>That said, whether predictors are qualitative or quantitative is not important in determining whether the problem is one of regression or classification. As long as qualitative predictors are properly coded before the analysis is conducted, they can be used for either type of problem.</p>
</div>
<div id="classification-problems" class="section level1">
<h1>Classification problems</h1>
<p>The sinking of <a href="https://en.wikipedia.org/wiki/RMS_Titanic">RMS Titanic</a> provided the world with many things:</p>
<ul>
<li>A fundamental shock to the world as its faith in supposedly indestructible technology was shattered by a chunk of ice</li>
<li>Perhaps the best romantic ballad of all time <iframe width="560" height="315" src="https://www.youtube.com/embed/WNIPqafd4As" frameborder="0" allowfullscreen></iframe></li>
<li><p>A tragic love story</p>
<div class="figure">
<img src="http://i.giphy.com/KSeT85Vtym7m.gif" alt="Titanic (1997)" />
<p class="caption"><a href="https://en.wikipedia.org/wiki/Titanic_(1997_film)">Titanic (1997)</a></p>
</div></li>
</ul>
<p>Why did Jack have to die? Why couldn’t he have made it onto a lifeboat like Cal? We may never know the answer, but we can generalize the question a bit: why did some people survive the sinking of the Titanic while others did not?</p>
<p>In essence, we have a classification problem. The response is a binary variable, indicating whether a specific passenger survived. If we combine this with predictors that describe each passenger, we might be able to estimate a general model of survival.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Kaggle is an online platform for predictive modeling and analytics. They run regular competitions where they provide the public with a question and data, and anyone can estimate a predictive model to answer the question. They’ve run a popular contest based on a <a href="https://www.kaggle.com/c/titanic/data">dataset of passengers from the Titanic</a>. Let’s download the dataset and load it into R.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/titanic_train.csv&quot;</span>)

titanic %&gt;%
<span class="st">  </span><span class="kw">head</span>() %&gt;%
<span class="st">  </span>knitr::<span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Moran, Mr. James</td>
<td align="left">male</td>
<td align="right">NA</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">330877</td>
<td align="right">8.4583</td>
<td align="left">NA</td>
<td align="left">Q</td>
</tr>
</tbody>
</table>
<p>The codebook contains the following information on the variables:</p>
<pre><code>VARIABLE DESCRIPTIONS:
Survived        Survival
                (0 = No; 1 = Yes)
Pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
Name            Name
Sex             Sex
Age             Age
SibSp           Number of Siblings/Spouses Aboard
Parch           Number of Parents/Children Aboard
Ticket          Ticket Number
Fare            Passenger Fare
Cabin           Cabin
Embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)

SPECIAL NOTES:
Pclass is a proxy for socio-economic status (SES)
 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1)
 If the Age is Estimated, it is in the form xx.5

With respect to the family relation variables (i.e. sibsp and parch)
some relations were ignored.  The following are the definitions used
for sibsp and parch.

Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:   Mother or Father of Passenger Aboard Titanic
Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws.  Some children travelled
only with a nanny, therefore parch=0 for them.  As well, some
travelled with very close friends or neighbors in a village, however,
the definitions do not support such relations.</code></pre>
<p>So if this is our data, <code>Survived</code> is our response variable, and the remaining variables are predictors, how can we determine who survives and who dies?</p>
<div id="a-linear-regression-approach" class="section level2">
<h2>A linear regression approach</h2>
<p>Let’s concentrate first on the relationship between age and survival. Using the methods we previously learned, we could estimate a linear regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/titanic_ols-1.png" width="672" /></p>
<p>Hmm. Not terrible, but you can immediately notice a couple of things. First, the only possible values for <code>Survival</code> are <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Yet the linear regression model gives us predicted values such as <span class="math inline">\(.4\)</span> and <span class="math inline">\(.25\)</span>. How should we interpret those?</p>
<p>One possibility is that these values are <em>predicted probabilities</em>. That is, the estimated probability a passenger will survive given their age. So someone with a predicted probability of <span class="math inline">\(.4\)</span> has a 40% chance of surviving. Okay, but notice that because the line is linear and continuous, it extends infinitely in both directions of age.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">xlim</span>(<span class="dv">0</span>, <span class="dv">200</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/titanic_ols_old-1.png" width="672" /></p>
<p>What happens if a 200 year old person is on the Titanic? They would have a <span class="math inline">\(-.1\)</span> probability of surviving. You cannot have a probability outside of the <span class="math inline">\([0,1]\)</span> interval! Admittedly this is a trivial example, but in other circumstances this can become a more realistic scenario.</p>
<p>Or what if we didn’t want to predict survival, but instead predict the port from which an individual departed (Cherbourg, Queenstown, or Southampton). We could try and code this as a numeric response variable:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Cherbourg</td>
</tr>
<tr class="even">
<td>2</td>
<td>Queenstown</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Southampton</td>
</tr>
</tbody>
</table>
<p>But why not instead code it:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Queenstown</td>
</tr>
<tr class="even">
<td>2</td>
<td>Cherbourg</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Southampton</td>
</tr>
</tbody>
</table>
<p>Or even:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Southampton</td>
</tr>
<tr class="even">
<td>2</td>
<td>Cherbourg</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Queenstown</td>
</tr>
</tbody>
</table>
<p><em>There is no inherent ordering to this variable</em>. Any claimed linear relationship between a predictor and port of embarkation is completely dependent on how we convert the classes to numeric values.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>Rather than modeling the response <span class="math inline">\(Y\)</span> directly, logistic regression instead models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category. In our first Titanic example, the probability of survival can be written as:</p>
<p><span class="math display">\[P(\text{survival} = \text{Yes} | \text{age})\]</span></p>
<p>The values of <span class="math inline">\(P(\text{survival} = \text{Yes} | \text{age})\)</span> (or simply <span class="math inline">\(p(\text{survival})\)</span> will range between 0 and 1. Given that predicted probability, we could predict anyone with for whom <span class="math inline">\(p(\text{survival}) &gt; .5\)</span> will survive the sinking, and anyone else will die.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>We can estimate the logistic regression model using the <code>glm</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age, <span class="dt">data =</span> titanic, <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Which produces a line that looks like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/titanic_age_glm_plot-1.png" width="672" /></p>
<p>It’s hard to tell, but the line is not perfectly linear. Let’s expand the range of the x-axis to prove this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>) +
<span class="st">  </span><span class="kw">xlim</span>(<span class="dv">0</span>,<span class="dv">200</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/titanic_age_glm_plot_wide-1.png" width="672" /></p>
<p>No more predictions that a 200 year old has a <span class="math inline">\(-.1\)</span> probability of surviving!</p>
<div id="adding-predictions" class="section level3">
<h3>Adding predictions</h3>
<p>To visualise the predictions from a model, we start by generating an evenly spaced grid of values that covers the region where our data lies. First we use <code>modelr::data_grid</code> to create a cleaned data frame of potential values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_age &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">data_grid</span>(Age)
titanic_age</code></pre></div>
<pre><code>## # A tibble: 88 × 1
##      Age
##    &lt;dbl&gt;
## 1   0.42
## 2   0.67
## 3   0.75
## 4   0.83
## 5   0.92
## 6   1.00
## 7   2.00
## 8   3.00
## 9   4.00
## 10  5.00
## # ... with 78 more rows</code></pre>
<p>Next we could use the <code>add_predictions</code> function to produce the predicted probabilities. This worked very well for linear models; unfortunately it is not perfect for logistic regression. This is because, in truth, logistic regression directly estimates the <a href="https://wiki.lesswrong.com/wiki/Log_odds"><em>log-odds</em></a> for the outcome. Instead, we want the plain old predicted probability. To do this, use this custom function to convert from log-odds to predicted probabilties:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span>function(x){
  <span class="kw">exp</span>(x) /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_age &lt;-<span class="st"> </span>titanic_age %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred))
titanic_age</code></pre></div>
<pre><code>## # A tibble: 88 × 2
##      Age      pred
##    &lt;dbl&gt;     &lt;dbl&gt;
## 1   0.42 0.4846727
## 2   0.67 0.4839882
## 3   0.75 0.4837691
## 4   0.83 0.4835501
## 5   0.92 0.4833037
## 6   1.00 0.4830847
## 7   2.00 0.4803475
## 8   3.00 0.4776115
## 9   4.00 0.4748768
## 10  5.00 0.4721436
## # ... with 78 more rows</code></pre>
<p>With this information, we can now plot the logistic regression line using the estimated model (and not just <code>ggplot2::geom_smooth</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age, <span class="kw">aes</span>(Age, pred)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Relationship Between Age and Surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted Probability of Survival&quot;</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/plot_pred-1.png" width="672" /></p>
</div>
</div>
<div id="multiple-predictors" class="section level2">
<h2>Multiple predictors</h2>
<p>But as the old principle of the sea goes, <a href="https://en.wikipedia.org/wiki/Women_and_children_first">“women and children first”</a>. What if age isn’t the only factor effecting survival? Fortunately logistic regression handles multiple predictors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                         <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7405  -0.6885  -0.6558   0.7533   1.8989  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.277273   0.230169   5.549 2.87e-08 ***
## Age         -0.005426   0.006310  -0.860     0.39    
## Sexmale     -2.465920   0.185384 -13.302  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 749.96  on 711  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 755.96
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficients essentially tell us the relationship between each individual predictor and the response, <em>independent of other predictors</em>. So this model tells us the relationship between age and survival, after controlling for the effects of gender. Likewise, it also tells us the relationship between gender and survival, after controlling for the effects of age. To get a better visualization of this, let’s use <code>data_grid</code> and <code>add_predictions</code> again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age_sex &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">data_grid</span>(Age, Sex) %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred))
titanic_age_sex</code></pre></div>
<pre><code>## # A tibble: 176 × 3
##      Age    Sex      pred
##    &lt;dbl&gt;  &lt;chr&gt;     &lt;dbl&gt;
## 1   0.42 female 0.7815965
## 2   0.42   male 0.2330934
## 3   0.67 female 0.7813649
## 4   0.67   male 0.2328510
## 5   0.75 female 0.7812907
## 6   0.75   male 0.2327735
## 7   0.83 female 0.7812165
## 8   0.83   male 0.2326960
## 9   0.92 female 0.7811330
## 10  0.92   male 0.2326088
## # ... with 166 more rows</code></pre>
<p>With these predicted probabilities, we can now plot the separate effects of age and gender:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age_sex, <span class="kw">aes</span>(Age, pred, <span class="dt">color =</span> Sex)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Probability of Surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted Probability of Survival&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Sex&quot;</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/survive_age_woman_plot-1.png" width="672" /></p>
<p>This graph illustrates a key fact about surviving the sinking of the Titanic - age was not really a dominant factor. Instead, one’s gender was much more important. Females survived at much higher rates than males, regardless of age.</p>
</div>
<div id="quadratic-terms" class="section level2">
<h2>Quadratic terms</h2>
<p>Logistic regression, like linear regression, assumes each predictor has an independent and linear relationship with the response. That is, it assumes the relationship takes the form <span class="math inline">\(y = \beta_0 + \beta_{1}x\)</span> looks something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_line &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>),
                   <span class="dt">y =</span> x *<span class="st"> </span><span class="dv">1</span>)

<span class="kw">ggplot</span>(sim_line, <span class="kw">aes</span>(x, y)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="stat_learning01_files/figure-html/straight_line-1.png" width="672" /></p>
<p>But from algebra we know that variables can have non-linear relationships. Perhaps instead the relationship is parabolic like <span class="math inline">\(y = \beta_0 + \beta_{1}x + \beta_{2}x^2\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_line &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
                   <span class="dt">y =</span> x^<span class="dv">2</span> +<span class="st"> </span>x)

<span class="kw">ggplot</span>(sim_line, <span class="kw">aes</span>(x, y)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="stat_learning01_files/figure-html/parabola-1.png" width="672" /></p>
<p>Or a more general quadratic equation <span class="math inline">\(y = \beta_0 + \beta_{1}x + \beta_{2}x^2 + \beta_{3}x^3\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_line &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
                   <span class="dt">y =</span> x^<span class="dv">3</span> +<span class="st"> </span>x^<span class="dv">2</span> +<span class="st"> </span>x)

<span class="kw">ggplot</span>(sim_line, <span class="kw">aes</span>(x, y)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="stat_learning01_files/figure-html/quadratic-1.png" width="672" /></p>
<p>These can be accounted for in a logistic regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_square &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span><span class="kw">I</span>(Age^<span class="dv">2</span>), <span class="dt">data =</span> titanic,
                          <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_square)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + I(Age^2), family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2777  -1.0144  -0.9516   1.3421   1.4278  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  0.2688449  0.2722529   0.987   0.3234  
## Age         -0.0365193  0.0172749  -2.114   0.0345 *
## I(Age^2)     0.0003965  0.0002538   1.562   0.1183  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 957.81  on 711  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 963.81
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred)) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Age, pred)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Relationship Between Age and Surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted Probability of Survival&quot;</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Here the parabolic term (<span class="math inline">\(\text{age}^2\)</span>) is not actually meaningful. In other contexts it may be so.</p>
</div>
<div id="interactive-terms" class="section level2">
<h2>Interactive terms</h2>
<p>Another assumption of linear and logistic regression is that the relationships between predictors and responses are independent from one another. So for the age and gender example, we assume our function <span class="math inline">\(f\)</span> looks something like:<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p><span class="math display">\[f = \beta_{0} + \beta_{1}\text{age} + \beta_{2}\text{gender}\]</span></p>
<p>However once again, that is an assumption. What if the relationship between age and the probability of survival is actually dependent on whether or not the individual is a female? This possibility would take the functional form:</p>
<p><span class="math display">\[f = \beta_{0} + \beta_{1}\text{age} + \beta_{2}\text{gender} + \beta_{3}(\text{age} \times \text{gender})\]</span></p>
<p>This is considered an <em>interaction</em> between age and gender. To estimate this in R, we simply specify <code>Age * Sex</code> in our formula for the <code>glm</code> function:<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>As before, let’s estimate predicted probabilities and plot the interactive effects of age and gender.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age_sex_x &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">data_grid</span>(Age, Sex) %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman_x) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred))
titanic_age_sex_x</code></pre></div>
<pre><code>## # A tibble: 176 × 3
##      Age    Sex      pred
##    &lt;dbl&gt;  &lt;chr&gt;     &lt;dbl&gt;
## 1   0.42 female 0.6461311
## 2   0.42   male 0.3245488
## 3   0.67 female 0.6472564
## 4   0.67   male 0.3233762
## 5   0.75 female 0.6476162
## 6   0.75   male 0.3230014
## 7   0.83 female 0.6479758
## 8   0.83   male 0.3226269
## 9   0.92 female 0.6483802
## 10  0.92   male 0.3222058
## # ... with 166 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age_sex_x, <span class="kw">aes</span>(Age, pred, <span class="dt">color =</span> Sex)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Probability of Surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted Probability of Survival&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Sex&quot;</span>)</code></pre></div>
<p><img src="stat_learning01_files/figure-html/age_woman_plot-1.png" width="672" /></p>
<p>And now our minds are blown once again! For women, as age increases the probability of survival also increases. However for men, we see the opposite relationship: as age increases the probability of survival <em>decreases</em>. Again, the basic principle of saving women and children first can be seen empirically in the estimated probability of survival. Male children are treated similarly to female children, and their survival is prioritized. Even still, the odds of survival are always worse for men than women, but the regression function clearly shows a difference from our previous results.</p>
<p>You may think then that it makes sense to throw in interaction terms (and quadratic terms) willy-nilly to all your regression models since we never know for sure if the relationship is strictly linear and independent. You could do that, but once you start adding more predictors (3, 4, 5, etc.) that will get very difficult to keep track of (five-way interactions are extremely difficult to interpret - even three-way get to be problematic). The best advice is to use theory and your domain knowledge as your guide. Do you have a reason to believe the relationship should be interactive? If so, test for it. If not, don’t.</p>
</div>
<div id="evaluating-model-accuracy" class="section level2">
<h2>Evaluating model accuracy</h2>
<div id="accuracy-of-predictions" class="section level3">
<h3>Accuracy of predictions</h3>
<p>How do we know if a logistic regression model is good or bad? One evalation criteria simply asks: how accurate are the predictions? For instance, how often did our basic model just using age perform? First we need to get the predicted probabilities for each individual in the original dataset, convert the probability to a prediction (I use a <span class="math inline">\(.5\)</span> cut-point), then see what percentage of predictions were the same as the actual survivals?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_accuracy &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred &gt;<span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(age_accuracy$Survived ==<span class="st"> </span>age_accuracy$pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.5938375</code></pre>
<p><span class="math inline">\(59.4\%\)</span> of the predictions based on age were correct. If we flipped a coin to make our predictions, we’d be right about 50% of the time. So this is a decent improvement. Of course, we also know that <span class="math inline">\(61.6\%\)</span> of passengers died in the sinking, so if we just guessed that every passenger died we’d still be right more often than our predictive model. Maybe this model isn’t so great after all. What about our interactive age and gender model?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_accuracy &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman_x) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred &gt;<span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_accuracy$Survived ==<span class="st"> </span>x_accuracy$pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.780112</code></pre>
<p>This model is much better. Just by knowing an individual’s age and gender, we can predict with $ 78%$ whether he/she lives or dies.</p>
</div>
<div id="trainingtest-sets" class="section level3">
<h3>Training/test sets</h3>
<p>One issue with the previous approach is that we are fitting our model to the data, then using the same data to assess the model’s accuracy. This isn’t very appropriate because we will bias our model towards fitting the data that we have. <em>We may fit our function to create the results we expect or desire, rather than the “true” function</em>.</p>
<p>Instead, we can split our data into distinct <em>training</em> and <em>test</em> sets. The training set can be used repeatedly to explore or train different models. Once we have a stable model, we can apply it to the test set of held-out data to determine (unbiasedly) whether the model makes accurate predictions.</p>
<p>Let’s use this workflow instead. We can use <code>resample_partition</code> from <code>modelr</code> to split <code>titanic</code> into a training and test set (we’ll allocate 70% of observations to the training set and 30% to the test set), reproduce the interactive model using only the training data, then make predictions on the test set and compare them to the actual, known survivals and deaths.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
<span class="kw">map</span>(titanic_split, dim)</code></pre></div>
<pre><code>## $test
## [1] 267  12
## 
## $train
## [1] 624  12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_split$train,
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(train_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic_split$train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7541  -0.6539  -0.5867   0.7727   2.1018  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.74926    0.36231   2.068 0.038638 *  
## Age          0.01095    0.01194   0.917 0.359097    
## Sexmale     -1.93347    0.50163  -3.854 0.000116 ***
## Age:Sexmale -0.02230    0.01586  -1.406 0.159580    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 674.37  on 502  degrees of freedom
## Residual deviance: 508.58  on 499  degrees of freedom
##   (121 observations deleted due to missingness)
## AIC: 516.58
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_test_accuracy &lt;-<span class="st"> </span>titanic_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>() %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(train_model) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred &gt;<span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_test_accuracy$Survived ==<span class="st"> </span>x_test_accuracy$pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.7535545</code></pre>
<p>This is reassuring. <span class="math inline">\(75.4\%\)</span> of the test set observations were accurately predicted. Remember, we explicitly set this up so that the model never saw the test observations, and they never factored into our decisionmaking process when we generated the model. This tells us that we did a pretty good job estimating the functional form <span class="math inline">\(f\)</span> for the entire dataset, not just the training set.</p>
</div>
</div>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li>This page is derived in part from <a href="http://varianceexplained.org/files/loess.html">“Creating a LOESS animation with <code>gganimate</code>”</a> by David Robinson.</li>
<li>This page is derived in part from <a href="http://www.infogix.com/blog/machine-learning-vs-statistical-modeling-the-real-difference/">“Machine Learning vs. Statistical Modeling: The Real Difference”</a></li>
<li>For more information on statistical learning and the math behind these methods, see the awesome book <a href="http://link.springer.com.proxy.uchicago.edu/book/10.1007%2F978-1-4614-7138-7"><em>An Introduction to Statistical Learning</em></a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.1 (2016-06-21)
##  system   x86_64, darwin13.4.0        
##  ui       RStudio (1.0.44)            
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2016-11-16                  
## 
##  package      * version date       source                           
##  assertthat     0.1     2013-12-06 CRAN (R 3.3.0)                   
##  broom        * 0.4.1   2016-06-24 CRAN (R 3.3.0)                   
##  codetools      0.2-15  2016-10-05 CRAN (R 3.3.0)                   
##  colorspace     1.2-7   2016-10-11 CRAN (R 3.3.0)                   
##  DBI            0.5-1   2016-09-10 CRAN (R 3.3.0)                   
##  devtools       1.12.0  2016-06-24 CRAN (R 3.3.0)                   
##  digest         0.6.10  2016-08-02 CRAN (R 3.3.0)                   
##  dplyr        * 0.5.0   2016-06-24 CRAN (R 3.3.0)                   
##  evaluate       0.10    2016-10-11 CRAN (R 3.3.0)                   
##  foreign        0.8-67  2016-09-13 CRAN (R 3.3.0)                   
##  formatR        1.4     2016-05-09 CRAN (R 3.3.0)                   
##  gapminder    * 0.2.0   2015-12-31 CRAN (R 3.3.0)                   
##  gganimate    * 0.1     2016-11-11 Github (dgrtwo/gganimate@26ec501)
##  ggplot2      * 2.2.0   2016-11-10 Github (hadley/ggplot2@f442f32)  
##  gtable         0.2.0   2016-02-26 CRAN (R 3.3.0)                   
##  highr          0.6     2016-05-09 CRAN (R 3.3.0)                   
##  htmltools      0.3.5   2016-03-21 CRAN (R 3.3.0)                   
##  knitr          1.15    2016-11-09 CRAN (R 3.3.1)                   
##  labeling       0.3     2014-08-23 CRAN (R 3.3.0)                   
##  lattice      * 0.20-34 2016-09-06 CRAN (R 3.3.0)                   
##  lazyeval       0.2.0   2016-06-12 CRAN (R 3.3.0)                   
##  lubridate    * 1.6.0   2016-09-13 CRAN (R 3.3.0)                   
##  magrittr       1.5     2014-11-22 CRAN (R 3.3.0)                   
##  Matrix         1.2-7.1 2016-09-01 CRAN (R 3.3.0)                   
##  memoise        1.0.0   2016-01-29 CRAN (R 3.3.0)                   
##  mgcv           1.8-16  2016-11-07 CRAN (R 3.3.0)                   
##  mnormt         1.5-5   2016-10-15 CRAN (R 3.3.0)                   
##  modelr       * 0.1.0   2016-08-31 CRAN (R 3.3.0)                   
##  munsell        0.4.3   2016-02-13 CRAN (R 3.3.0)                   
##  nlme           3.1-128 2016-05-10 CRAN (R 3.3.1)                   
##  plyr           1.8.4   2016-06-08 CRAN (R 3.3.0)                   
##  psych          1.6.9   2016-09-17 cran (@1.6.9)                    
##  purrr        * 0.2.2   2016-06-18 CRAN (R 3.3.0)                   
##  R6             2.2.0   2016-10-05 CRAN (R 3.3.0)                   
##  randomForest   4.6-12  2015-10-07 CRAN (R 3.3.0)                   
##  rcfss        * 0.1.0   2016-10-06 local                            
##  Rcpp           0.12.7  2016-09-05 cran (@0.12.7)                   
##  readr        * 1.0.0   2016-08-03 CRAN (R 3.3.0)                   
##  readxl       * 0.1.1   2016-03-28 CRAN (R 3.3.0)                   
##  reshape2       1.4.2   2016-10-22 CRAN (R 3.3.0)                   
##  rmarkdown    * 1.1     2016-10-16 CRAN (R 3.3.1)                   
##  rsconnect      0.5     2016-10-17 CRAN (R 3.3.0)                   
##  rstudioapi     0.6     2016-06-27 CRAN (R 3.3.0)                   
##  scales         0.4.1   2016-11-09 CRAN (R 3.3.1)                   
##  stringi        1.1.2   2016-10-01 CRAN (R 3.3.0)                   
##  stringr      * 1.1.0   2016-08-19 cran (@1.1.0)                    
##  tibble       * 1.2     2016-08-26 cran (@1.2)                      
##  tidyr        * 0.6.0   2016-08-12 CRAN (R 3.3.0)                   
##  tidyverse    * 1.0.0   2016-09-09 CRAN (R 3.3.0)                   
##  withr          1.0.2   2016-06-20 CRAN (R 3.3.0)                   
##  yaml           2.1.13  2014-06-12 CRAN (R 3.3.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>General at least as applied to the Titanic. I’d like to think technology has advanced some since the early 20th century that the same patterns do not apply today. <a href="https://en.wikipedia.org/wiki/Costa_Concordia_disaster">Not that sinking ships have gone away.</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Note that you cannot use <code>Rcurl</code> or <code>downloader</code> to obtain the file since Kaggle blocks automatic downloads.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The threshold can be adjusted depending on how conservative or risky of a prediction you wish to make.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Alternatively, we can use <code>broom::augment</code> to add predicted probabilities for the original dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)

<span class="kw">augment</span>(survive_age, <span class="dt">newdata =</span> titanic_age, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()</code></pre></div>
<pre><code>## # A tibble: 88 × 4
##      Age      pred   .fitted    .se.fit
##    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1   0.42 0.4846727 0.4846727 0.04285304
## 2   0.67 0.4839882 0.4839882 0.04255168
## 3   0.75 0.4837691 0.4837691 0.04245533
## 4   0.83 0.4835501 0.4835501 0.04235903
## 5   0.92 0.4833037 0.4833037 0.04225074
## 6   1.00 0.4830847 0.4830847 0.04215452
## 7   2.00 0.4803475 0.4803475 0.04095580
## 8   3.00 0.4776115 0.4776115 0.03976503
## 9   4.00 0.4748768 0.4748768 0.03858327
## 10  5.00 0.4721436 0.4721436 0.03741165
## # ... with 78 more rows</code></pre>
<ul>
<li><code>newdata = titanic</code> - produces a data frame containing all the original variables + the predicted probability for the observation</li>
<li><code>type.predict = &quot;response&quot;</code> - ensures we get the predicted probabilities, not the logged version</li>
</ul>
<a href="#fnref4">↩</a></li>
<li id="fn5"><p>In mathematical truth, it looks more like: <span class="math display">\[P(\text{survival} = \text{Yes} | \text{age}, \text{gender}) = \frac{1}{1 + e^{-(\beta_{0} + \beta_{1}\text{age} + \beta_{2}\text{gender})}}\]</span><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>R automatically includes constituent terms, so this turns into <code>Age + Sex + Age * Sex</code>. <a href="https://pan-oxfordjournals-org.proxy.uchicago.edu/content/14/1/63.full.pdf+html">Generally you always want to include constituent terms in a regression model with an interaction.</a><a href="#fnref6">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

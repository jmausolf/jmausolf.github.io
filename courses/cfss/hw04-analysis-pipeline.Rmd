---
title: "Homework 04: Building an analysis pipeline"
output: html_document
---

# Overview

Due before class Wednesday October 26th.

Basic goal of the assignment:

* Write three or more R scripts to carry out a small data analysis
* The output of the first script must be the input of the second, and so on
* Something like this:
  - First script: download some data
  - Second script: read the data, clean/tidy/transform the data, perform some exploratory analysis, and write numerical data to file in CSV or feather format
  - Third script: an `.Rmd`, actually, that brings everything together. Present the original data, the statistical summaries, and/or the figures in a little report. Also build and interpret a statistical model.
  - A fourth script to rule them all, i.e. to run the others in sequence.

# Fork the `hw04` repository

Go [here](https://github.com/uc-cfss/hw04) to fork the repo for homework 04.

# How do I do this?

The easiest approach uses the [example pipeline](https://github.com/uc-cfss/pipeline-example) as a template for your own assignment. You can get fancier if you want (e.g. create a Bash shell script that runs the pipeline, rather than another `.R` script), but you don't have to go overboard with it.

# What data should I use?

Whatever you want! The important thing is that the entire analysis is *reproducible*. That is, I will clone your repository on my computer, I will run the script that "rules them all", and it should reproduce your results and analysis without any errors.^[Okay, if I get an error because you used a package that I don't already have, that's my fault. But you know what I mean.]

# I'm not creative and I can't think of anything to analyze!

Okay, then analyze one of the datasets we've used before.

* [Raw data for `gapminder`](https://raw.githubusercontent.com/jennybc/gapminder/master/inst/gapminder.tsv)
* [Raw data for `gun_deaths`](https://github.com/fivethirtyeight/guns-data)
* Use [rscorecard](https://github.com/btskinner/rscorecard) to download your own subset of the Department of Education's College Scorecard data.

## Download the data

Download the raw data. For example, using `gapminder`:

+ Option 1: via an R script using [downloader::download](https://cran.r-project.org/web/packages/downloader/downloader.pdf) or [RCurl::getURL](http://www.omegahat.net/RCurl/installed/RCurl/html/getURL.html).

    ```r
    downloader::download("https://raw.githubusercontent.com/jennybc/gapminder/master/inst/gapminder.tsv")
    cat(file = "gapminder.tsv",
      RCurl::getURL("https://raw.githubusercontent.com/jennybc/gapminder/master/inst/gapminder.tsv"))
    ```

+ Option 2: in a [shell](shell.html) script using `curl` or `wget`.

    ```bash
    curl -O https://raw.githubusercontent.com/jennybc/gapminder/master/inst/gapminder.tsv
    wget https://raw.githubusercontent.com/jennybc/gapminder/master/inst/gapminder.tsv
    ```

## Perform exploratory analysis

* Bring the data in as data frame.
* Save a couple descriptive plots to file with highly informative names.
* Reorder the continents based on life expectancy. You decide the details.
* Sort the actual data in a deliberate fashion. You decide the details, but this should *at least* implement your new continent ordering.
* Write the Gapminder data to file(s), for immediate and future reuse.

## Perform statistical analysis

* Import the data created in the first script.
* Make sure your new continent order is still in force. You decide the details.
* Fit a linear regression of life expectancy on year within each country. Write the estimated intercepts, slopes, and residual error variance (or sd) to file.
* Find the 3 or 4 "worst" and "best" countries for each continent. You decide the details.
* Write the linear regression info for just these countries to file.

## Generate figures

Create a figure for each continent, including data only for the 6-8 "extreme" countries, and write to file. One file per continent, with an informative name. The figure should give scatterplots of life expectancy vs. year, facetting on country, fitted line overlaid.

## Automate the pipeline

Identify and test a method of running your pipeline non-interactively.

You could write a master R script that simply `source()`s the three scripts, one after the other. Tip: you will probably want a second "clean up / reset" script that deletes all the output your scripts leave behind, so you can easily test and refine your strategy, i.e. without repeatedly deleting stuff "by hand". You can run the master script or the cleaning script from a [shell](shell.html) with `Rscript`.

Provide a link to a page (oh hey there `README.md`) that explains how your pipeline works and links to the remaining files. Your peers and myself should be able to go to this landing page and re-run your analysis quickly and easily.

# Aim higher!

* Use a completely unique dataset - preferably something related to your own research interests
    * You will probably need to spend time data cleaning and tidying. Probably would be good in a separate R script.
* Render an R Markdown document with your final analysis.
    * You do not need to stuff everything into the final document. Think of this like a traditional report. You might describe how you obtained and prepared the data, but you won't include all the code and output from that process in the final document. But because it is stored in a separate R script and is part of the data pipeline, everything is still completely reproducible.
    * To emulate RStudio's "Knit" button from a [shell](shell.html):
        `Rscript -e "rmarkdown::render('myAwesomeAnalysis.Rmd')"`
    * To emulate RStudio's "Knit" button within an R script:
        `rmarkdown::render('myAwesomeAnalysis.Rmd)`
* Experiment with running R code saved in a script from within R Markdown. Here's some official documentation on [code externalization](http://yihui.name/knitr/demo/externalization/).
* Embed pre-existing figures in and R Markdown document, i.e. an R script creates the figures, then the report incorporates them.
* Import pre-existing data in an R Markdown document, then format nicely as a table.

# Submit the assignment

Your assignment should be submitted as a set of R scripts, R Markdown documents, data files, figures, etc. Follow instructions on [homework workflow](hw00_homework_guidelines.html#homework_workflow). As part of the pull request, you're encouraged to reflect on what was hard/easy, problems you solved, helpful tutorials you read, etc.

# Rubric

Check minus: Cannot run the pipeline. Scripts require interactive coding to fix. Markdown documents are not generated. Graphs and tables don't have appropriate labels or formatting.

Check: Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). Nothing spectacular, either bad or good.

Check plus: Repository contains a detailed `README.md` explaining how the pipeline works. Displays innovative data analysis or coding skills. Graphs and tables are well labeled. Analysis is insightful. I walk away feeling I learned something.

### Acknowledgments {.toc-ignore}

```{r child='_ack_stat545.Rmd'}
```


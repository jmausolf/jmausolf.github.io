<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Statistical learning: resampling and decision trees</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="site_libs/d3-3.5.6/d3.min.js"></script>
<link href="site_libs/profvis-0.3.2/profvis.css" rel="stylesheet" />
<script src="site_libs/profvis-0.3.2/profvis.js"></script>
<link href="site_libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlight-6.2.0/highlight.js"></script>
<script src="site_libs/profvis-binding-0.3.2/profvis.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<p><strong>This content is from the fall 2016 version of this course. Please go <a href = "https://uc-cfss.github.io">here</a> for the most recent version.</strong></p>

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical learning: resampling and decision trees</h1>

</div>


<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</code></pre></div>
<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define resampling methods</li>
<li>Review the validation set approach using linear regression</li>
<li>Explain leave-one-out cross-validation</li>
<li>Explain <span class="math inline">\(k\)</span>-fold cross-validation</li>
<li>Demonstrate how to conduct cross-validation on generalized linear models</li>
<li>Define a decision tree</li>
<li>Demonstrate how to estimate a decision tree</li>
<li>Define and estimate a random forest</li>
<li>Introduce the <code>caret</code> package for statistical learning in R</li>
</ul>
</div>
<div id="resampling-methods" class="section level1">
<h1>Resampling methods</h1>
<p>Resampling methods are essential to test and evaluate statistical models. Because you likely do not have the resources or capabilities to repeatedly sample from your population of interest, instead you can repeatedly draw from your original sample obtain additional information about your model. For instance, you could repeatedly draw samples from your data, estimate a linear regression model on each sample, and then examine how the estimated model differs across each sample. This allows you to assess the variability and stability of your model in a way not possible if you can only fit the model once.</p>
<div id="validation-set" class="section level2">
<h2>Validation set</h2>
<p>We have already seen the <em>validation set</em> approach in the <a href="stat_learning01.html">previous class</a>. By splitting our data into a <em>training set</em> and <em>test set</em>, we can evaluate the model’s effectiveness at predicting the response variable (in the context of either regression or classification) independently of the data used to estimate the model in the first place.</p>
<div id="classification" class="section level3">
<h3>Classification</h3>
<p>Recall how we used this approach to evaluate the accuracy of our <a href="stat_learning01.html#interactive_terms">interactive model predicting survival during the sinking of the Titanic</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/titanic_train.csv&quot;</span>)

titanic %&gt;%
<span class="st">  </span><span class="kw">head</span>() %&gt;%
<span class="st">  </span>knitr::<span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Moran, Mr. James</td>
<td align="left">male</td>
<td align="right">NA</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">330877</td>
<td align="right">8.4583</td>
<td align="left">NA</td>
<td align="left">Q</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span>function(x){
  <span class="kw">exp</span>(x) /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
<span class="kw">map</span>(titanic_split, dim)</code></pre></div>
<pre><code>## $test
## [1] 267  12
## 
## $train
## [1] 624  12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_split$train,
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(train_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic_split$train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8433  -0.7331  -0.6742   0.7194   1.9246  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.514887   0.282736   5.358 8.42e-08 ***
## Age         -0.009012   0.007550  -1.194    0.233    
## Sexmale     -2.475183   0.221260 -11.187  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 697.35  on 510  degrees of freedom
## Residual deviance: 543.50  on 508  degrees of freedom
##   (113 observations deleted due to missingness)
## AIC: 549.5
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_test_accuracy &lt;-<span class="st"> </span>titanic_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>() %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(train_model) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred &gt;<span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_test_accuracy$Survived ==<span class="st"> </span>x_test_accuracy$pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.7931034</code></pre>
</div>
<div id="regression" class="section level3">
<h3>Regression</h3>
<p>This method also works for regression analysis. Here we will examine the relationship between horsepower and car mileage in the <code>Auto</code> dataset (found in <code>library(ISLR)</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)

Auto &lt;-<span class="st"> </span>Auto %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
Auto</code></pre></div>
<pre><code>## # A tibble: 392 × 9
##      mpg cylinders displacement horsepower weight acceleration  year
## *  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1     18         8          307        130   3504         12.0    70
## 2     15         8          350        165   3693         11.5    70
## 3     18         8          318        150   3436         11.0    70
## 4     16         8          304        150   3433         12.0    70
## 5     17         8          302        140   3449         10.5    70
## 6     15         8          429        198   4341         10.0    70
## 7     14         8          454        220   4354          9.0    70
## 8     14         8          440        215   4312          8.5    70
## 9     14         8          455        225   4425         10.0    70
## 10    15         8          390        190   3850          8.5    70
## # ... with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fctr&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_plot-1.png" width="672" /></p>
<p>The relationship does not appear to be strictly linear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_plot_lm-1.png" width="672" /></p>
<p>Perhaps by adding <a href="stat_learning01.html#quadratic_terms">quadratic terms</a> to the linear regression we could improve overall model fit. To evaluate the model, we will split the data into a training set and test set, estimate a series of higher-order models, and calculate a test statistic summarizing the accuracy of the estimated <code>mpg</code>. Rather than relying on the raw error rate (which makes sense in a classification model), we will instead use <em>mean squared error</em> (<span class="math inline">\(MSE\)</span>), defined as</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i = 1}^{n}{(y_i - \hat{f}(x_i))^2}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i =\)</span> the observed response value for the <span class="math inline">\(i\)</span>th observation</li>
<li><span class="math inline">\(\hat{f}(x_i) =\)</span> the predicted response value for the <span class="math inline">\(i\)</span>th observation given by <span class="math inline">\(\hat{f}\)</span></li>
<li><span class="math inline">\(n =\)</span> the total number of observations</li>
</ul>
<p>Boo math! Actually this is pretty intuitive. All we’re doing is for each observation, calculating the difference between the actual and predicted values for <span class="math inline">\(y\)</span>, squaring that difference, then calculating the average across all observations. An <span class="math inline">\(MSE\)</span> of 0 indicates the model perfectly predicted each observation. The larger the <span class="math inline">\(MSE\)</span>, the more error in the model.</p>
<p>For this task, first we can use <code>modelr::resample_partition</code> to create training and test sets (using a 50/50 split), then estimate a linear regression model without any quadratic terms.</p>
<ul>
<li>I use <code>set.seed()</code> in the beginning - whenever you are writing a script that involves randomization (here, random subsetting of the data), always set the seed at the beginning of the script. This ensures the results can be reproduced precisely.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>I also use the <code>glm</code> function rather than <code>lm</code> - if you don’t change the <code>family</code> parameter, the results of <code>lm</code> and <code>glm</code> are exactly the same. I do this because I want to use a cross-validation function later that only works with results from a <code>glm</code> function.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
auto_train &lt;-<span class="st"> </span>auto_split$train %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
auto_test &lt;-<span class="st"> </span>auto_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span>horsepower, <span class="dt">data =</span> auto_train)
<span class="kw">summary</span>(auto_lm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mpg ~ horsepower, data = auto_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -12.892   -2.864   -0.545    2.793   13.298  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 38.005404   0.921129   41.26   &lt;2e-16 ***
## horsepower  -0.140459   0.007968  -17.63   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 20.48452)
## 
##     Null deviance: 10359.4  on 196  degrees of freedom
## Residual deviance:  3994.5  on 195  degrees of freedom
## AIC: 1157.9
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>To estimate the <span class="math inline">\(MSE\)</span>, I write a brief function that requires two inputs: the dataset and the linear model.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MSE_mpg &lt;-<span class="st"> </span>function(model, data){
  <span class="kw">mean</span>((<span class="kw">predict</span>(model, data) -<span class="st"> </span>data$mpg)^<span class="dv">2</span>)
}

<span class="kw">MSE_mpg</span>(auto_lm, auto_test)</code></pre></div>
<pre><code>## [1] 28.57255</code></pre>
<p>For a strictly linear model, the <span class="math inline">\(MSE\)</span> for the test set is 28.57. How does this compare to a quadratic model? We can use the <code>poly</code> function in conjunction with a <code>map</code> iteration to estimate the <span class="math inline">\(MSE\)</span> for a series of models with higher-order polynomial terms:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_poly &lt;-<span class="st"> </span>function(i){
  <span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> auto_train)
}

auto_poly_results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span>:<span class="dv">5</span>,
           <span class="dt">model =</span> <span class="kw">map</span>(terms, auto_poly),
           <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, MSE_mpg, <span class="dt">data =</span> auto_test))

<span class="kw">ggplot</span>(auto_poly_results, <span class="kw">aes</span>(terms, MSE)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using validation set&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/mse_poly-1.png" width="672" /></p>
<p>Based on the <span class="math inline">\(MSE\)</span> for the validation (test) set, a polynomial model with a quadratic term (<span class="math inline">\(\text{horsepower}^2\)</span>) produces the lowest average error. Adding cubic or higher-order terms is just not necessary.</p>
</div>
<div id="drawbacks-to-validation-sets" class="section level3">
<h3>Drawbacks to validation sets</h3>
<p>There are two main problems with validation sets:</p>
<ol style="list-style-type: decimal">
<li><p>Validation estimates of the test error rates can be highly variable depending on which observations are sampled into the training and test sets. See what happens if we repeat the sampling, estimation, and validation procedure for the <code>Auto</code> data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_variable &lt;-<span class="st"> </span>function(Auto){
  auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
  auto_train &lt;-<span class="st"> </span>auto_split$train %&gt;%
<span class="kw">tbl_df</span>()
  auto_test &lt;-<span class="st"> </span>auto_split$test %&gt;%
<span class="kw">tbl_df</span>()

  auto_poly &lt;-<span class="st"> </span>function(i){
<span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> auto_train)
  }

  results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span>:<span class="dv">5</span>,
                    <span class="dt">model =</span> <span class="kw">map</span>(terms, auto_poly),
                    <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, MSE_mpg, <span class="dt">data =</span> auto_test))

  <span class="kw">return</span>(results)
}

<span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">mse_variable</span>(Auto)) %&gt;%
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> id)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Variability of MSE estimates&quot;</span>,
   <span class="dt">subtitle =</span> <span class="st">&quot;Using the validation set approach&quot;</span>,
   <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
   <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_variable_mse-1.png" width="672" /></p></li>
<li><p>If you don’t have a large data set, you’ll have to dramatically shrink the size of your training set. Most statistical learning methods perform better with more observations - if you don’t have enough data in the training set, you might overestimate the error rate in the test set.</p></li>
</ol>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>Leave-one-out cross-validation</h2>
<p>An alternative method is <em>leave-one-out cross validation</em> (LOOCV). Like with the validation set approach, you split the data into two parts. However the difference is that you only remove one observation for the test set, and keep all remaining observations in the training set. The statistical learning method is fit on the <span class="math inline">\(n-1\)</span> training set. You then use the held-out observation to calculate the <span class="math inline">\(MSE = (y_1 - \hat{y}_1)^2\)</span> which should be an unbiased estimator of the test error. Because this <span class="math inline">\(MSE\)</span> is highly dependent on which observation is held out, <em>we repeat this process for every single observation in the data set</em>. Mathematically, this looks like:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i = 1}^{n}{MSE_i}\]</span></p>
<p>This method produces estimates of the error rate that have minimal bias and relatively steady (i.e. non-varying), unlike the validation set approach where the <span class="math inline">\(MSE\)</span> estimate is highly dependent on the sampling process for training/test sets. LOOCV is also highly flexible and works with any kind of predictive modeling.</p>
<p>Of course the downside is that this method is computationally difficult. You have to estimate <span class="math inline">\(n\)</span> different models - if you have a large <span class="math inline">\(n\)</span> or each individual model takes a long time to compute, you may be stuck waiting a long time for the computer to finish its calculations.</p>
<div id="loocv-in-linear-regression" class="section level3">
<h3>LOOCV in linear regression</h3>
<p>We can use the <code>cv.glm</code> function in the <code>boot</code> library to compute the LOOCV of any linear or logistic regression model. For the <code>Auto</code> dataset, this looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)

auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span>horsepower, <span class="dt">data =</span> Auto)
auto_lm_err &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, auto_lm)
auto_lm_err$delta[<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 24.23151</code></pre>
<p><code>cv.glm</code> produces a list with several components. The two numbers in the <code>delta</code> vector contain the results of the LOOCV. The first number is what we care about the most, and is the LOOCV estimate of the <span class="math inline">\(MSE\)</span> for the dataset.</p>
<p>We can also use this method to compare the optimal number of polynomial terms as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>

for(i in terms){
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
  cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit)$delta[[<span class="dv">1</span>]]
}

cv_mse &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">cv_MSE =</span> cv_error)
cv_mse</code></pre></div>
<pre><code>## # A tibble: 5 × 2
##   terms   cv_MSE
##   &lt;int&gt;    &lt;dbl&gt;
## 1     1 24.23151
## 2     2 19.24821
## 3     3 19.33498
## 4     4 19.42443
## 5     5 19.03321</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(cv_mse, <span class="kw">aes</span>(terms, cv_MSE)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using LOOCV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/loocv_poly-1.png" width="672" /></p>
<p>And arrive at the same conclusion.</p>
</div>
<div id="loocv-in-classification" class="section level3">
<h3>LOOCV in classification</h3>
<p>Let’s use classification to validate the interactive terms model from before. Before we can estimate the LOOCV, we need to first remove any observations from <code>titanic</code> which have missing values for <code>Survived</code>, <code>Age</code>, or <code>Sex</code> (<code>glm</code> does this for us automatically, whereas <code>cv.glm</code> does not. And since <code>cv.glm</code> requires the data frame as its first argument, we can use the pipe <code>%&gt;%</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                     <span class="dt">family =</span> binomial)

titanic_loocv &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age), !<span class="kw">is.na</span>(Sex)) %&gt;%
<span class="st">  </span><span class="kw">cv.glm</span>(titanic_model)
titanic_loocv$delta[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.1703518</code></pre>
<p>In a classification problem, the LOOCV tells us the average error rate based on our predictions. So here, it tells us that the interactive <code>Age * Sex</code> model has a 17% error rate. This is similar to the validation set result (<span class="math inline">\(79.3\%\)</span>)</p>
</div>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>k-fold cross-validation</h2>
<p>A less computationally-intensive approach to cross validation is <span class="math inline">\(k\)</span>-fold cross-validation. Rather than dividing the data into <span class="math inline">\(n\)</span> groups, one divides the observations into <span class="math inline">\(k\)</span> groups, or <em>folds</em>, of approximately equal size. The first fold is treated as the validation set, the model is estimated on the remaining <span class="math inline">\(k-1\)</span> folds. This process is repeated <span class="math inline">\(k\)</span> times, with each fold serving as the validation set precisely once. The <span class="math inline">\(k\)</span>-fold CV estimate is calculated by averaging the <span class="math inline">\(MSE\)</span> values for each fold:</p>
<p><span class="math display">\[CV_{(k)} = \frac{1}{k} \sum_{i = 1}^{k}{MSE_i}\]</span></p>
<p>LOOCV is the special case of <span class="math inline">\(k\)</span>-fold cross-validation where <span class="math inline">\(k = n\)</span>. More typically researchers will use <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> depending on the size of the data set and the complexity of the statistical model.</p>
<div id="k-fold-cv-in-linear-regression" class="section level3">
<h3>k-fold CV in linear regression</h3>
<p>Let’s go back to the <code>Auto</code> data set. Instead of LOOCV, let’s use 10-fold CV to compare the different polynomial models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>

for(i in terms){
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
  cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit, <span class="dt">K =</span> <span class="dv">10</span>)$delta[[<span class="dv">1</span>]]
}

cv_error_fold10</code></pre></div>
<pre><code>## [1] 24.42374 19.28760 19.20702 19.42017 18.96962</code></pre>
<p>How do these results compare to the LOOCV values?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">loocv =</span> cv_error,
           <span class="dt">fold10 =</span> cv_error_fold10) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(method, MSE, loocv:fold10) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> method)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MSE estimates&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;CV Method&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/10_fold_auto_loocv-1.png" width="672" /></p>
<p>Pretty much the same results. But computationally, how long does it take to estimate the 10-fold CV versus LOOCV? We can use the <code>profvis</code> package to profile our code and determine how long it takes to run.</p>
<div id="loocv" class="section level4">
<h4>LOOCV</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>
  
  for (i in terms) {
    glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
    cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit)$delta[[<span class="dv">1</span>]]
  }
})</code></pre></div>
<div id="htmlwidget-e806828ac0a3497cfb73" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-e806828ac0a3497cfb73">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,51,51,51,51,51,51,51,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,56,56,56,56,56,56,57,58,58,58,58,58,58,58,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,60,60,61,61,61,61,61,61,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,69,69,69,69,70,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,81,81,81,81,81,81,81,81,81,81,82,82,82,82,82,82,82,82,82,82,82,82,82,82,83,83,83,83,83,83,84,84,84,84,84,84,84,84,84,84,84,85,85,85,85,85,85,85,85,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,86,87,87,87,87,87,87,87,87,87,87,87,87,88,88,88,88,88,88,88,88,88,88,88,88,88,88,89,89,89,89,89,89,89,89,89,89,89,90,90,90,90,90,90,90,90,91,91,91,91,91,91,91,91,91,91,91,91,91,91,92,92,92,92,92,92,92,92,92,92,92,92,92,92,93,93,93,93,93,93,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,95,96,96,96,96,96,96,96,96,96,97,97,97,97,97,97,97,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,98,99,99,99,99,99,99,99,100,100,100,100,100,100,100,101,101,101,101,101,101,101,101,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,103,104,104,104,104,104,104,104,104,104,104,105,105,105,105,105,105,105,106,106,106,106,106,106,106,107,107,107,107,107,107,108,108,108,108,108,108,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,114,114,114,114,114,114,114,114,114,114,114,114,114,115,115,115,115,115,115,115,115,115,115,115,115,115,115,116,116,116,116,116,116,117,117,117,117,117,117,117,117,118,118,118,118,118,118,118,118,118,118,118,118,118,118,119,119,119,119,119,119,119,119,119,119,119,119,120,120,120,120,120,120,121,121,121,121,121,122,122,122,122,122,122,123,123,123,123,123,123,123,123,123,123,123,123,123,123,123,124,124,124,124,124,124,124,124,125,125,125,125,125,125,126,126,126,126,126,127,127,127,127,127,127,127,127,127,128,128,128,128,128,128,128,128,128,128,129,129,129,129,129,129,129,129,129,129,129,129,129,129,129,130,130,130,130,130,130,130,131,131,131,131,131,131,131,131,132,132,132,132,132,132,132,132,132,133,133,133,133,133,133,134,135,135,135,135,135,135,135,135,136,136,136,136,136,136,136,136,136,137,137,137,137,137,137,137,137,137,138,138,138,138,138,138,138,138,138,138,138,138,138,138,139,139,139,139,139,139,139,139,139,139,139,140,140,140,140,140,140,140,140,140,141,141,141,141,141,141,141,141,141,141,141,141,141,142,142,142,142,142,142,142,142,142,142,142,142,142,142,143,143,143,143,143,143,143,143,143,143,143,143,143,143,144,144,144,144,144,144,145,145,145,145,145,145,145,145,145,145,146,146,146,146,146,146,146,147,147,147,147,147,147,147,147,147,147,147,147,147,147,147,147,147,147,148,148,148,148,148,148,148,148,148,148,148,148,148,149,149,149,149,149,149,149,150,150,150,150,150,150,150,150,150,150,150,150,150,150,150,150,151,151,151,151,151,151,152,152,152,152,152,152,152,152,153,153,153,153,153,153,153,153,154,154,154,154,154,155,155,155,155,155,155,155,155,155,155,155,155,156,156,156,156,156,156,156,156,156,156,156,156,157,157,157,157,157,157,157,157,157,158,158,158,158,158,158,158,158,159,159,159,159,159,159,160,160,160,160,160,160,160,160,160,161,161,161,161,161,161,161,161,161,161,161,161,162,162,162,162,162,162,162,162,162,162,163,163,163,163,163,163,163,163,164,164,164,164,164,164,164,164,165,165,165,165,165,165,165,165,165,166,166,166,166,166,166,166,166,166,167,167,167,167,167,167,167,167,167,167,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,169,169,169,169,169,169,169,169,169,170,170,170,170,170,170,170,170,170,171,171,171,171,171,171,171,171,171,171,171,172,172,172,172,172,172,172,172,172,173,173,173,173,173,173,173,173,173,173,173,173,174,174,174,174,174,174,174,174,174,174,175,175,175,175,175,175,175,176,176,176,176,176,176,176,177,177,177,177,177,177,177,177,178,178,178,178,178,178,178,178,178,178,178,178,178,178,179,179,179,179,179,179,179,179,180,180,180,180,180,180,180,180,180,180,180,180,180,180,181,181,181,181,181,181,181,181,182,182,182,182,182,182,182,182,182,182,182,182,182,182,182,182,183,183,183,183,183,183,183,184,184,184,184,184,184,184,184,184,184,184,185,185,185,185,185,185,185,185,185,185,186,186,186,186,186,186,187,187,187,187,187,187,188,188,188,189,189,189,189,189,189,189,189,189,190,190,190,190,190,191,191,191,191,191,191,191,191,191,191,191,191,191,191,191,192,192,192,192,192,193,193,193,193,193,193,193,193,193,194,194,194,194,194,194,194,194,195,195,195,195,195,195,196,196,196,196,196,196,196,196,196,196,197,197,197,197,197,197,197,197,197,197,197,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,198,199,199,199,199,199,199,199,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,201,201,201,201,201,201,202,202,202,202,202,202,202,202,202,202,202,202,202,202,202,203,203,203,203,203,203,203,203,203,203,203,203,204,204,204,204,204,204,204,204,204,204,204,204,204,204,205,205,205,205,205,205,206,206,206,206,206,206,206,206,206,206,207,207,207,207,207,207,207,207,207,208,208,208,208,208,208,208,209,209,209,209,209,209,209,209,209,209,209,209,209,210,210,210,210,210,210,210,210,210,210,210,210,210,210,211,211,211,211,211,211,211,211,211,211,211,211,211,212,212,212,212,212,212,212,212,212,213,213,213,213,213,213,213,213,213,213,213,213,214,214,214,214,214,214,214,215,215,215,215,215,215,215,215,215,216,216,216,216,216,216,216,216,216,217,217,217,217,217,217,217,217,218,218,218,218,218,218,218,218,218,219,219,219,219,219,219,219,219,219,219,219,219,219,219,219,220,220,220,220,220,220,220,220,220,220,220,220,221,221,221,221,221,221,221,221,221,221,221,221,222,222,222,222,222,222,222,223,223,223,223,223,223,223,223,224,224,224,224,224,224,224,224,225,225,225,225,225,225,225,225,225,225,225,225,226,226,226,226,226,226,226,227,227,227,227,227,227,228,228,228,228,228,228,228,229,229,229,229,229,229,230,230,230,230,230,230,231,231,231,231,231,231,232,232,232,232,232,232,232,232,232,232,232,232,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,233,234,234,234,234,234,234,234,235,235,235,235,235,235,235,235,235,235,235,235,236,236,236,236,236,236,236,236,236,236,236,236,237,237,237,237,237,237,237,237,237,237,237,237,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,238,239,239,239,239,239,239,239,239,239,239,239,239,239,239,240,240,240,240,240,240,240,240,241,241,241,241,241,241,241,241,242,242,242,242,242,242,242,242,242,242,242,242,243,243,243,244,244,244,244,244,244,244,244,244,244,245,245,245,245,245,245,245,246,246,246,246,246,246,246,246,246,247,247,247,247,247,247,247,247,247,247,247,248,248,248,248,248,248,248,248,248,248,248,248,249,249,249,249,249,249,249,250,250,250,250,250,251,251,251,251,251,251,251,252,252,252,252,253,253,253,253,253,253,254,254,254,254,254,254,254,254,254,255,255,255,255,255,255,255,255,256,256,256,256,256,256,256,256,256,256,257,257,257,257,257,257,258,258,258,258,258,258,258,258,259,259,259,259,259,259,259,259,259,259,259,259,259,259,259,259,259,260,260,260,260,260,260,260,260,261,261,261,261,261,261,261,261,261,261,261,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,262,263,263,263,263,263,263,263,263,263,263,264,264,264,264,264,264,264,264,265,265,265,265,265,265,265,266,266,266,266,266,266,266,266,267,267,267,267,267,267,267,267,268,268,269,269,269,269,269,269,269,269,269,269,269,269,270,270,270,270,270,270,270,270,270,270,270,271,271,271,271,271,271,271,271,271,271,271,272,272,272,272,272,272,272,272,273,273,273,273,273,273,273,273,273,273,273,273,273,273,274,274,274,274,274,274,274,274,275,275,275,275,275,275,276,276,276,276,276,276,276,276,276,277,277,277,277,277,277,277,277,277,277,277,277,278,278,278,278,279,279,279,279,279,279,279,279,279,279,279,279,279,280,280,280,280,280,280,280,280,280,280,280,280,281,281,281,281,281,281,281,281,282,282,282,282,282,282,282,282,282,282,282,283,283,283,283,283,283,283,283,284,284,284,284,284,284,284,284,285,285,285,285,285,285,285,286,286,286,286,286,286,286,286,286,286,286,286,286,287,287,287,287,287,287,288,288,288,288,288,289,289,289,289,289,289,289,289,289,289,289,289,289,290,290,290,290,290,290,290,290,290,290,290,290,290,291,291,291,291,291,291,291,291,291,291,291,291,291,292,292,292,292,292,292,292,293,293,293,293,293,293,293,293,293,293,293,293,293,293,294,294,294,294,294,294,294,294,294,295,295,295,295,295,295,295,295,295,295,295,295,296,296,296,296,296,296,296,296,297,297,297,297,297,297,297,297,297,298,298,298,298,298,298,298,298,298,298,298,299,299,299,299,299,299,299,299,299,299,299,299,299,299,300,300,300,300,300,300,300,300,300,300,300,300,301,301,301,301,301,301,301,301,302,302,302,302,302,302,303,303,303,303,304,304,304,304,304,304,304,304,305,305,305,305,305,305,305,305,305,306,306,306,306,306,306,306,307,307,307,307,307,307,308,308,308,308,308,308,308,308,309,309,309,309,309,309,309,309,310,310,310,310,310,310,310,310,310,311,311,311,311,311,311,311,311,311,311,311,311,311,312,312,312,312,312,312,312,312,312,312,312,312,312,312,313,313,313,313,313,313,313,313,313,313,313,313,313,313,314,314,314,314,314,314,314,314,314,314,314,315,315,315,315,315,315,315,315,315,316,316,316,316,316,316,317,317,317,317,317,317,318,318,318,318,318,318,318,318,319,319,319,319,319,319,319,319,319,319,319,319,319,319,319,319,319,320,320,320,320,320,320,320,320,320,321,321,321,321,321,321,321,321,321,322,322,322,322,322,322,322,322,322,322,322,322,323,323,323,323,323,323,323,323,323,323,323,324,324,324,324,324,324,324,324,324,324,324,325,325,325,325,325,325,325,325,326,326,326,326,326,326,326,326,326,326,326,326,326,326,327,327,327,327,327,327,327,327,327,327,327,327,328,328,328,328,328,328,328,328,328,328,328,328,328,328,329,329,329,329,329,329,329,329,329,329,329,329,329,330,330,330,330,330,330,330,330,330,331,331,331,331,331,331,331,331,331,331,331,331,331,332,332,332,332,332,332,332,332,332,332,332,333,333,333,333,333,333,333,333,333,333,333,333,333,333,334,334,334,334,334,334,334,334,334,335,335,335,335,335,335,335,336,336,336,336,336,336,336,336,336,336,336,337,337,337,337,337,337,337,337,338,338,338,338,338,338,338,338,338,338,339,339,339,339,339,339,339,339,339,339,340,340,340,340,340,340,340,340,340,340,340,340,340,341,341,341,341,341,341,341,341,342,342,342,342,342,342,342,342,342,342,342,343,343,343,343,343,343,343,343,343,343,343,343,344,344,344,344,344,344,344,345,345,345,345,345,345,345,345,345,346,346,346,346,346,346,346,346,347,347,347,347,347,347,347,347,348,348,348,348,348,348,348,348,349,349,349,349,349,349,349,349,350,350,350,350,350,350,350,350,350,350,350,350,351,351,351,351,351,351,351,351,351,352,352,352,352,352,352,352,352,352,352,352,352,352,352,353,353,353,353,353,353,353,353,353,353,353,354,354,354,354,354,354,354,354,354,354,354,354,354,354,354,355,355,355,355,355,355,355,355,355,355,355,355,355,356,356,356,356,356,356,356,356,356,356,356,356,357,357,357,357,357,357,357,357,357,357,357,358,358,358,358,358,358,358,358,359,359,359,359,359,359,360,360,360,360,360,360,360,360,361,361,361,361,361,361,361,361,361,361,361,361,361,362,362,362,362,362,362,362,362,362,362,362,362,363,363,363,363,363,363,363,363,363,363,363,363,363,363,364,364,364,364,364,364,364,364,364,364,364,364,365,365,365,365,365,365,365,365,365,365,365,365,365,365,366,366,366,366,366,366,367,367,367,367,367,367,367,367,367,367,367,367,368,368,368,368,368,368,368,368,368,368,368,368,368,368,369,369,369,369,369,369,369,369,369,369,369,369,370,370,370,370,370,370,370,370,370,370,370,370,370,371,371,371,371,371,371,371,371,372,372,372,372,372,372,373,373,373,373,373,373,373,373,373,373,374,374,374,374,374,374,374,374,374,374,374,374,374,375,375,375,375,375,375,375,375,375,375,375,375,376,376,376,376,376,377,377,377,377,377,377,377,377,377,377,377,377,377,377,377,377,378,378,378,378,378,378,378,378,378,379,379,379,379,379,379,379,379,380,380,380,380,380,380,380,380,380,381,381,381,381,381,381,381,381,381,381,381,381,381,381,382,382,382,382,382,382,382,382,382,383,383,383,383,383,383,383,383,383,383,383,384,384,384,384,384,384,384,384,384,384,385,385,385,385,385,385,385,385,385,385,385,385,385,386,386,386,386,386,386,386,386,386,386,386,386,387,387,387,387,387,387,387,388,388,388,388,388,388,388,388,388,389,389,389,389,390,390,390,390,390,390,390,390,390,390,390,390,390,391,391,391,391,391,391,391,391,391,391,391,392,392,392,392,392,392,392,392,392,392,392,393,393,393,393,393,393,393,393,393,393,393,394,394,394,394,394,394,394,394,395,395,395,395,395,395,395,395,396,396,396,396,396,396,396,396,396,396,396,396,396,396,397,397,397,397,397,397,397,397,397,397,397,397,397,398,398,398,398,398,398,398,398,398,398,398,398,398,398,398,398,398,399,399,399,399,399,399,400,400,400,400,400,400,400,400,400,400,400,400,401,401,401,401,401,401,401,401,402,402,402,402,402,402,402,402,402,402,403,403,403,404,404,404,404,404,404,404,404,404,404,404,404,404,404,405,405,405,405,405,405,405,405,405,405,405,406,406,406,406,406,406,406,406,406,406,406,406,407,407,407,407,407,407,407,407,407,407,407,408,408,408,408,408,408,408,408,408,409,409,409,409,409,409,410,410,410,410,410,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,411,412,412,412,412,412,412,412,412,412,413,413,413,413,413,413,413,413,413,413,413,413,413,414,414,414,414,414,414,414,414,414,415,415,415,415,415,415,416,416,416,416,416,416,416,416,416,416,416,416,416,416,417,417,417,417,417,417,417,417,417,418,418,418,418,418,418,418,418,418,418,418,418,419,419,419,419,419,419,419,419,419,419,419,419,420,420,420,420,420,420,420,420,420,420,420,420,421,421,421,421,421,421,421,421,421,421,421,421,422,422,422,422,422,422,422,422,422,422,422,422,423,423,423,423,423,423,423,423,423,423,423,423,424,424,424,424,424,424,424,424,424,424,424,424,425,425,425,425,425,425,426,426,426,426,426,426,426,426,426,427,427,427,427,427,427,427,427,427,428,428,428,428,428,428,428,428,428,428,428,428,428,429,429,429,429,429,429,429,429,429,429,429,429,430,430,430,430,430,430,430,430,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,431,432,432,432,432,432,432,432,432,432,432,433,433,433,433,433,433,433,433,433,434,434,434,434,434,434,434,434,434,434,434,434,434,435,435,435,435,435,435,436,436,436,436,436,436,436,437,437,437,437,437,437,437,437,438,438,438,438,438,438,438,439,439,439,439,439,439,439,439,439,439,439,439,440,440,440,440,440,440,440,440,440,440,440,440,441,441,441,441,441,441,441,441,442,442,442,442,442,442,442,443,443,443,443,443,443,443,443,443,443,443,443,443,443,443,444,444,444,444,444,444,444,444,444,444,444,444,444,444,445,445,445,445,445,445,445,445,445,445,445,445,445,445,445,445,446,446,446,446,446,446,446,446,446,446,446,446,446,446,447,447,447,447,447,447,447,447,447,447,448,448,448,448,448,448,448,448,448,448,448,448,448,448,449,449,449,449,449,449,449,449,449,449,449,449,450,450,450,450,450,450,450,450,450,451,451,451,451,451,451,451,451,451,452,452,452,452,452,452,452,452,453,453,453,453,453,453,453,453,453,454,454,454,454,454,454,455,455,455,455,455,455,455,455,455,456,456,456,456,456,456,456,456,457,457,457,457,457,457,457,458,458,458,458,458,458,459,459,459,459,459,459,459,459,459,459,459,459,459,459,460,460,460,460,460,460,460,460,460,460,460,460,460,460,460,461,461,461,461,461,461,461,462,462,462,462,462,462,462,462,462,462,463,463,463,463,463,463,463,463,464,464,464,464,464,464,464,464,464,464,464,464,464,465,465,465,465,465,465,465,465,465,465,465,465,465,466,466,466,466,466,466,467,467,467,467,467,467,467,468,468,468,468,468,468,468,468,468,468,468,468,468,469,469,469,469,469,469,469,469,469,469,470,470,470,470,470,470,470,470,470,470,470,470,470,471,471,471,471,471,471,471,471,471,471,471,472,472,472,472,472,472,472,472,472,472,472,472,472,473,473,473,473,473,473,473,473,473,473,473,473,474,474,474,474,474,474,474,474,474,474,475,475,475,475,475,475,475,475,475,475,475,475,476,476,476,476,476,476,476,476,476,476,477,477,477,477,477,477,477,477,477,477,477,477,478,478,478,478,478,478,478,478,478,479,479,479,479,479,479,480,480,480,480,480,480,480,480,480,480,480,481,481,481,481,481,481,481,481,481,482,482,482,482,482,482,482,482,482,482,483,483,484,484,484,484,484,484,484,484,485,485,485,485,485,485,485,485,485,485,485,485,485,485,486,486,486,486,486,486,486,486,487,487,487,487,487,487,487,487,487,487,487,487,487,487,487,488,488,488,488,488,488,488,488,488,488,488,489,489,489,489,489,489,489,489,489,489,489,490,490,490,490,491,491,491,491,491,491,491,491,491,491,491,491,491,492,492,492,492,492,492,492,492,492,492,493,493,493,493,493,493,493,493,493,493,493,493,493,494,494,494,494,494,494,494,494,494,494,494,494,494,494,494,495,495,495,495,495,495,495,495,495,495,495,495,496,496,496,496,496,496,496,496,497,497,497,497,497,497,497,497,497,497,497,497,497,497,497,498,498,498,498,498,498,498,498,498,498,498,498,499,499,499,499,499,499,499,499,500,500,500,500,500,500,500,500,500,501,501,501,501,501,501,501,501,501,501,501,501,501,501,502,502,502,502,502,502,502,502,502,502,502,503,503,503,503,503,503,503,503,504,504,504,504,504,504,504,504,504,505,505,505,505,505,505,506,506,506,506,506,506,506,506,506,506,506,506,506,507,507,507,507,507,507,507,507,507,507,507,508,508,508,508,508,508,508,509,509,509,509,509,509,509,509,509,510,510,510,510,510,510,510,510,510,511,511,511,511,511,511,511,511,511,511,511,511,511,511,511,512,512,512,512,512,512,512,512,513,513,513,513,513,513,513,513,513,514,514,514,514,514,514,514,514,514,514,514,514,514,514,514,514,515,515,515,515,515,515,515,515,516,516,516,516,516,516,516,516,516,516,517,517,517,517,517,517,517,517,517,518,518,518,518,518,518,518,518,518,518,518,518,519,519,519,519,519,519,519,519,519,519,519,519,519,519,520,520,520,520,520,521,521,521,521,521,521,521,521,522,522,522,522,522,522,522,522,522,522,522,522,522,522,522,523,523,523,523,523,523,523,523,523,524,524,524,524,524,524,524,524,524,525,526,526,526,526,526,526,526,526,526,527,527,527,527,527,527,527,527,527,528,528,528,528,528,528,528,528,528,529,529,529,529,529,529,529,529,529,529,529,530,530,530,530,530,530,530,530,530,530,530,530,530,530,531,531,531,531,531,531,531,531,531,531,531,531,532,532,532,532,532,532,532,532,533,533,533,533,533,533,533,533,533,533,533,533,533,534,534,534,534,534,534,534,534,535,535,535,535,535,535,535,535,535,535,535,536,536,536,536,536,536,536,536,536,536,536,536,536,536,537,537,537,537,537,537,537,537,537,537,537,537,538,538,538,538,538,538,538,538,538,538,538,538,538,539,539,539,539,539,539,539,539,539,539,540,540,540,540,540,540,540,540,540,540,540,540,540,540,541,541,541,541,541,541,541,541,541,541,541,541,541,541,542,542,542,542,542,542,542,542,543,543,543,543,543,543,543,543,543,543,544,544,544,544,544,544,544,544,544,544,544,544,544,545,545,545,545,545,545,545,546,546,546,546,546,546,546,546,546,546,546,547,547,547,547,547,547,547,547,547,547,547,547,547,547,548,548,548,548,548,548,548,548,548,549,549,549,549,549,549,549,549,549,549,549,549,549,550,550,550,550,550,550,550,550,550,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,551,552,552,552,552,552,552,552,552,552,552,552,552,552,552,552,553,553,553,553,553,553,553,553,554,554,554,554,554,554,554,554,554,554,555,555,555,555,555,555,555,555,556,556,556,556,556,556,556,556,556,556,557,558,558,558,558,558,558,558,558,559,559,559,559,559,559,559,559,560,560,560,560,560,560,560,560,560,560,560,560,560,560,561,561,561,561,561,562,562,562,562,562,562,562,562,562,563,563,563,563,563,563,563,563,564,564,564,564,564,564,564,564,564,565,565,565,565,565,565,565,565,565,566,566,566,566,566,566,566,566,566,566,566,566,567,567,567,567,567,567,567,567,568,568,568,568,568,568,568,568,568,569,569,569,569,569,569,569,569,569,569,569,569,569,569,570,570,570,570,570,570,570,570,571,571,571,571,571,571,571,571,571,571,571,571,571,571,572,572,572,572,572,572,572,572,572,572,572,572,572,572,573,573,573,573,573,573,573,573,573,573,573,573,574,574,574,574,574,574,574,574,574,574,574,575,575,575,575,575,575,575,576,576,576,576,576,576,576,576,576,576,576,577,577,577,577,577,577,577,578,578,578,578,578,578,578,578,578,579,579,579,579,579,579,579,579,579,580,580,580,580,580,580,580,580,580,581,581,581,581,581,581,581,581,582,582,582,582,582,582,582,582,582,582,582,582,583,583,583,583,583,583,583,583,583,583,583,583,583,584,584,584,584,584,584,584,584,584,584,585,585,585,585,585,585,585,585,585,586,586,586,586,586,586,586,586,586,586,586,587,587,587,587,587,587,587,587,587,587,587,587,587,588,588,588,588,588,588,588,588,588,588,589,589,589,589,589,589,589,589,590,590,590,590,590,590,590,590,590,591,591,591,591,591,591,591,591,591,591,591,592,592,592,592,592,592,592,592,592,592,592,592,592,592,593,593,593,593,593,593,593,593,593,593,593,593,593,593,593,594,594,594,594,594,594,594,594,595,595,595,595,595,595,595,595,595,596,596,596,596,596,596,596,596,596,597,597,597,597,597,597,597,598,598,598,598,598,598,599,599,599,599,599,599,599,600,600,600,600,600,600,600,601,601,601,601,601,601,602,602,602,602,602,602,602,602,602,602,602,603,603,603,603,603,603,603,603,604,604,604,604,604,604,604,604,604,604,604,604,604,605,605,605,605,605,605,605,605,605,606,606,606,606,606,606,606,606,606,606,607,607,607,607,607,607,607,607,607,607,607,607,607,607,607,608,608,608,609,609,609,609,609,609,609,609,609,609,609,610,610,610,610,610,610,610,610,611,611,611,611,611,611,611,611,611,611,611,611,612,612,612,612,612,612,612,612,612,612,613,613,613,613,613,613,613,613,613,613,613,613,614,614,614,614,614,614,614,614,614,614,614,614,615,615,615,615,615,615,615,616,616,616,616,616,616,616,616,616,616,616,616,616,616,617,617,617,617,617,617,617,617,618,618,618,618,618,618,618,618,619,619,619,619,619,619,619,619,620,620,620,620,620,620,620,620,621,621,621,621,621,621,621,621,622,622,622,622,622,622,622,622,622,623,623,623,623,623,623,623,623,623,624,624,624,624,624,624,624,624,624,624,624,625,625,625,625,625,625,625,625,625,625,625,625,626,626,626,626,626,626,626,627,627,627,627,627,627,628,628,628,628,628,628,628,628,628,628,628,628,628,628,628,628,628,629,629,629,629,629,629,629,629,630,630,630,630,630,630,630,630,630,630,630,630,631,631,631,631,631,631,631,631,631,631,631,631,631,631,631,631,632,632,632,632,632,632,632,632,633,633,633,633,633,633,633,633,634,634,634,635,635,635,635,635,635,635,635,635,635,635,635,635,635,635,636,636,636,636,636,636,636,636,636,636,636,636,637,637,637,637,637,637,637,637,637,637,637,637,637,637,638,638,638,638,638,638,638,638],"depth":[11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,4,3,2,1,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1],"label":["poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","-","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","colnames<-","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unlist","simplify2array","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","[[","$.data.frame","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","dim","dim","ncol","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","%||%","raw_rownames","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","attr","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean.default","mean","cost","cv.glm","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.control","do.call","glm","eval","eval","eval.parent","cv.glm","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","attributes","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","sys.call","match","%in%","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.matrix","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","getOption","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","cv.glm","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","is.ordered","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.fun","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","family","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","anyDuplicated","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","[[<-","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.formula","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","attributes","structure","family","glm","eval","eval","eval.parent","cv.glm","get","match.fun","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.extract","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.double","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","<GC>","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[.data.frame","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","get","match.fun","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","do.call","glm","eval","eval","eval.parent","cv.glm","glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)","$","qr.lm","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$.data.frame","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.numeric","is.numeric","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","isTRUE","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","c","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","%*%","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","names","names","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","length","length","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","family","glm","eval","eval","eval.parent","cv.glm","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean.default","mean","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.null","%||%","raw_rownames","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","sys.call","match","%in%","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","list","glm.control","do.call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","mode","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","family","predict.glm","predict","mean","cost","cv.glm","family","glm","eval","eval","eval.parent","cv.glm","[.tbl_df","[","cv.glm","list","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$<-","predict.glm","predict","mean","cost","cv.glm","match.fun","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","sys.call","match","%in%","[[.data.frame","[[","model.response","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","get","match.fun","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","length","length","dim.data.frame","dim","dim","ncol","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","family.glm","family","predict.glm","predict","mean","cost","cv.glm","[[.data.frame","[[","model.response","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","terms","predict.lm","predict.glm","predict","mean","cost","cv.glm","$<-","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","do.call","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","ifelse","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.tbl_df","[","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","lapply","[.tbl_df","[","cv.glm","make.link","family","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","cv.glm","as.list.data.frame","as.list","lapply","[.tbl_df","[","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","rep.int","variance","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","is.numeric","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","qr.lm","predict.lm","predict.glm","predict","mean","cost","cv.glm","[","cv.glm","dim","ncol","paste","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","cv.glm","<GC>","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","any","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","predict.glm","predict","mean","cost","cv.glm","match","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","attributes","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","family","glm","eval","eval","eval.parent","cv.glm","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","lapply","[.tbl_df","[","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","$","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","isTRUE","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$<-","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","c","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","family","glm","eval","eval","eval.parent","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","is.na","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","is.matrix","as.matrix.default","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","dim","dim","ncol","paste","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","unlist","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","attr","[.factor","FUN","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","%in%",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[.data.frame","[[","$.data.frame","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","::","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","max","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","environment","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean.default","mean","cost","cv.glm","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list.data.frame","as.list","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unclass","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","lapply","[.tbl_df","[","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","sys.function","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","terms.default","terms","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","ncol","paste","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm","eval","match.arg","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","c","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.matrix","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","rep.int","eval","eval","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","rep.int","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","match.arg","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","anyDuplicated","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","[[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","[[.data.frame","[[","model.response","glm","eval","eval","eval.parent","cv.glm","dim","dim","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list.data.frame","as.list","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list.data.frame","as.list","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","qr.lm","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","lapply","[.tbl_df","[","cv.glm","c","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","lengths","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","sqrt","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm","match","%in%","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","rep.int","eval","eval","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","structure","family","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","getNamespace","asNamespace","make.link","family","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.fun","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.factor","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","length","length","[.tbl_df","[","cv.glm","<GC>","attr","model.response","glm","eval","eval","eval.parent","cv.glm","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","getOption","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","cv.glm","$","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","terms","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list.data.frame","as.list","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","logical","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.array","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.matrix","is.matrix","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","terms.terms","terms","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[.data.frame","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.fun","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","isTRUE","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","%*%","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean.default","mean","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","family.glm","family","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","colnames<-","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".subset2","<Anonymous>","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","isBaseNamespace","getExportedValue","::","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","do.call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list.data.frame","as.list","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".row_names_info","dim.data.frame","dim","dim","nrow","[.tbl_df","[","cv.glm","is.factor","FUN","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","colnames<-","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm","eval","match.arg","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.atomic","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","[.tbl_df","[","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.factor","FUN","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[.data.frame","[[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","sys.parent","sys.function","match.call","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm"],"filenum":[null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,1,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1],"linenum":[null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,9,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,8,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9],"memalloc":[19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,19.775520324707,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,21.5042724609375,23.4894027709961,23.4894027709961,23.4894027709961,23.4894027709961,23.4894027709961,23.4894027709961,25.5840377807617,25.5840377807617,25.5840377807617,25.5840377807617,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,27.6841049194336,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,29.4358596801758,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,31.4613876342773,33.5577392578125,33.5577392578125,33.5577392578125,33.5577392578125,33.5577392578125,33.5577392578125,35.6814117431641,35.6814117431641,35.6814117431641,35.6814117431641,35.6814117431641,35.6814117431641,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,37.7406692504883,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,39.8566055297852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,41.9952774047852,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,44.0879898071289,46.031867980957,46.031867980957,46.031867980957,46.031867980957,46.031867980957,46.031867980957,46.031867980957,46.031867980957,48.1283874511719,48.1283874511719,48.1283874511719,48.1283874511719,48.1283874511719,48.1283874511719,48.1283874511719,48.1283874511719,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,50.1703186035156,52.1847076416016,52.1847076416016,52.1847076416016,52.1847076416016,52.1847076416016,52.1847076416016,54.2744979858398,54.2744979858398,54.2744979858398,54.2744979858398,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,55.8733673095703,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7516632080078,57.7523498535156,57.7523498535156,57.7523498535156,57.7523498535156,57.7523498535156,57.7523498535156,58.5295715332031,58.5295715332031,58.5295715332031,58.5295715332031,58.5295715332031,58.5295715332031,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.1274261474609,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,60.9588775634766,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,62.0653533935547,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,63.5375595092773,65.3317718505859,65.3317718505859,65.3317718505859,65.3317718505859,65.3317718505859,65.3317718505859,65.3317718505859,65.3317718505859,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,67.081916809082,68.6667327880859,68.6667327880859,68.6667327880859,68.6667327880859,68.6667327880859,68.6667327880859,68.6667327880859,68.6667327880859,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,70.3479843139648,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,71.7683868408203,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,73.5279769897461,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,75.1740188598633,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,77.1042251586914,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,78.7726745605469,80.6581268310547,80.6581268310547,80.6581268310547,80.6581268310547,80.6581268310547,80.6581268310547,80.6581268310547,80.6581268310547,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,82.3408050537109,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,83.7892227172852,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,47.1564559936523,19.3050689697266,19.3050689697266,19.3050689697266,19.3050689697266,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,21.1258773803711,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,23.0995254516602,25.0794067382812,25.0794067382812,25.0794067382812,25.0794067382812,25.0794067382812,25.0794067382812,25.0794067382812,25.0794067382812,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,27.1868896484375,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,29.3227233886719,31.151123046875,31.151123046875,31.151123046875,31.151123046875,31.151123046875,31.151123046875,31.151123046875,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,33.1328735351562,35.072265625,35.072265625,35.072265625,35.072265625,35.072265625,35.072265625,35.072265625,35.072265625,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,37.2140884399414,39.3060684204102,39.3060684204102,39.3060684204102,39.3060684204102,39.3060684204102,39.3060684204102,39.3060684204102,39.3060684204102,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,41.1838836669922,42.9954605102539,42.9954605102539,42.9954605102539,42.9954605102539,42.9954605102539,42.9954605102539,42.9954605102539,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,44.3892974853516,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,45.8172073364258,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,47.2962951660156,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,48.9388656616211,50.8273391723633,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,52.4852905273438,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,54.4194946289062,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,56.2565002441406,58.1619338989258,58.1619338989258,58.1619338989258,58.1619338989258,58.1619338989258,58.1619338989258,59.9987945556641,59.9987945556641,59.9987945556641,59.9987945556641,59.9987945556641,59.9987945556641,59.9987945556641,59.9987945556641,61.9857864379883,61.9857864379883,61.9857864379883,61.9857864379883,61.9857864379883,61.9857864379883,61.9857864379883,61.9857864379883,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,64.7112274169922,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,68.4395599365234,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,70.8988342285156,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,74.8771057128906,77.0278930664062,77.0278930664062,77.0278930664062,77.0278930664062,77.0278930664062,77.0278930664062,77.0278930664062,77.0278930664062,78.8742141723633,78.8742141723633,78.8742141723633,78.8742141723633,81.0162811279297,83.130729675293,83.130729675293,83.130729675293,83.130729675293,83.130729675293,83.130729675293,83.130729675293,83.130729675293,83.130729675293,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,18.6529312133789,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,20.9418334960938,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,23.4067687988281,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,25.564338684082,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,27.7587966918945,30.0223159790039,30.0223159790039,30.0223159790039,30.0223159790039,30.0223159790039,30.0223159790039,30.0223159790039,32.347282409668,32.347282409668,32.347282409668,32.347282409668,32.347282409668,32.347282409668,32.347282409668,32.347282409668,32.347282409668,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,35.1570358276367,37.4255447387695,37.4255447387695,37.4255447387695,37.4255447387695,37.4255447387695,37.4255447387695,37.4255447387695,37.4255447387695,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,39.8902282714844,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,42.0496368408203,44.2540893554688,44.2540893554688,44.2540893554688,44.2540893554688,44.2540893554688,44.2540893554688,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,46.3655242919922,48.5445709228516,48.5445709228516,48.5445709228516,48.5445709228516,48.5445709228516,48.5445709228516,48.5445709228516,48.5445709228516,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,52.7981414794922,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,54.9853134155273,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,57.0964279174805,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,60.347282409668,62.7147598266602,62.7147598266602,62.7147598266602,62.7147598266602,62.7147598266602,62.7147598266602,62.7147598266602,62.7147598266602,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,64.4819030761719,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,66.6561660766602,68.692985534668,68.692985534668,68.692985534668,68.692985534668,68.692985534668,68.692985534668,70.7093505859375,70.7093505859375,70.7093505859375,70.7093505859375,70.7093505859375,70.7093505859375,70.7093505859375,70.7093505859375,72.8661804199219,72.8661804199219,72.8661804199219,72.8661804199219,72.8661804199219,72.8661804199219,72.8661804199219,72.8661804199219,76.113655090332,76.113655090332,76.113655090332,76.113655090332,76.113655090332,76.113655090332,76.113655090332,76.113655090332,76.113655090332,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,77.1599807739258,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,80.9617309570312,83.2878875732422,83.2878875732422,83.2878875732422,83.2878875732422,83.2878875732422,83.2878875732422,83.2878875732422,42.3456878662109,42.3456878662109,42.3456878662109,42.3456878662109,42.3456878662109,42.3456878662109,42.3456878662109,22.1261291503906,22.1261291503906,22.1261291503906,22.1261291503906,22.1261291503906,22.1261291503906,22.1261291503906,22.1261291503906,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,24.345329284668,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,28.823600769043,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,31.2839431762695,33.4302597045898,33.4302597045898,33.4302597045898,33.4302597045898,33.4302597045898,33.4302597045898,33.4302597045898,35.5845031738281,35.5845031738281,35.5845031738281,35.5845031738281,35.5845031738281,35.5845031738281,35.5845031738281,37.4234848022461,37.4234848022461,37.4234848022461,37.4234848022461,37.4234848022461,37.4234848022461,39.8777084350586,39.8777084350586,39.8777084350586,39.8777084350586,39.8777084350586,39.8777084350586,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,42.0311737060547,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,44.1838073730469,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,46.7175750732422,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,49.5132141113281,52.0727005004883,52.0727005004883,52.0727005004883,52.0727005004883,52.0727005004883,52.0727005004883,52.0727005004883,52.0727005004883,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,54.9288482666016,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,57.0859375,59.3691177368164,59.3691177368164,59.3691177368164,59.3691177368164,59.3691177368164,59.3691177368164,61.9668273925781,61.9668273925781,61.9668273925781,61.9668273925781,61.9668273925781,61.9668273925781,61.9668273925781,61.9668273925781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,64.5263977050781,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,67.1245269775391,69.4255294799805,69.4255294799805,69.4255294799805,69.4255294799805,69.4255294799805,69.4255294799805,71.4211196899414,71.4211196899414,71.4211196899414,71.4211196899414,71.4211196899414,73.5598678588867,73.5598678588867,73.5598678588867,73.5598678588867,73.5598678588867,73.5598678588867,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,76.0779876708984,78.2613525390625,78.2613525390625,78.2613525390625,78.2613525390625,78.2613525390625,78.2613525390625,78.2613525390625,78.2613525390625,80.7263793945312,80.7263793945312,80.7263793945312,80.7263793945312,80.7263793945312,80.7263793945312,82.8875427246094,82.8875427246094,82.8875427246094,82.8875427246094,82.8875427246094,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,24.9454498291016,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,22.2499771118164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,24.5800552368164,26.8937911987305,26.8937911987305,26.8937911987305,26.8937911987305,26.8937911987305,26.8937911987305,26.8937911987305,29.1863021850586,29.1863021850586,29.1863021850586,29.1863021850586,29.1863021850586,29.1863021850586,29.1863021850586,29.1863021850586,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,33.7136306762695,36.2025604248047,36.2025604248047,36.2025604248047,36.2025604248047,36.2025604248047,36.2025604248047,38.6291961669922,41.994514465332,41.994514465332,41.994514465332,41.994514465332,41.994514465332,41.994514465332,41.994514465332,41.994514465332,45.043212890625,45.043212890625,45.043212890625,45.043212890625,45.043212890625,45.043212890625,45.043212890625,45.043212890625,45.043212890625,49.26904296875,49.26904296875,49.26904296875,49.26904296875,49.26904296875,49.26904296875,49.26904296875,49.26904296875,49.26904296875,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,52.4998092651367,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,55.1609497070312,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,57.6968383789062,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,61.0779190063477,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,64.2976303100586,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,66.9786224365234,71.5697021484375,71.5697021484375,71.5697021484375,71.5697021484375,71.5697021484375,71.5697021484375,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,74.9462661743164,80.0294647216797,80.0294647216797,80.0294647216797,80.0294647216797,80.0294647216797,80.0294647216797,80.0294647216797,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,83.4167175292969,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,21.0281829833984,23.6498184204102,23.6498184204102,23.6498184204102,23.6498184204102,23.6498184204102,23.6498184204102,23.6498184204102,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,26.1863555908203,28.7393112182617,28.7393112182617,28.7393112182617,28.7393112182617,28.7393112182617,28.7393112182617,31.2743530273438,31.2743530273438,31.2743530273438,31.2743530273438,31.2743530273438,31.2743530273438,31.2743530273438,31.2743530273438,34.0240020751953,34.0240020751953,34.0240020751953,34.0240020751953,34.0240020751953,34.0240020751953,34.0240020751953,34.0240020751953,36.6703109741211,36.6703109741211,36.6703109741211,36.6703109741211,36.6703109741211,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,41.7490463256836,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,44.2902297973633,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,46.6150665283203,49.0133361816406,49.0133361816406,49.0133361816406,49.0133361816406,49.0133361816406,49.0133361816406,49.0133361816406,49.0133361816406,51.1542816162109,51.1542816162109,51.1542816162109,51.1542816162109,51.1542816162109,51.1542816162109,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,53.6614074707031,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,57.4613418579102,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,60.0000610351562,62.6151809692383,62.6151809692383,62.6151809692383,62.6151809692383,62.6151809692383,62.6151809692383,62.6151809692383,62.6151809692383,65.1689529418945,65.1689529418945,65.1689529418945,65.1689529418945,65.1689529418945,65.1689529418945,65.1689529418945,65.1689529418945,67.607536315918,67.607536315918,67.607536315918,67.607536315918,67.607536315918,67.607536315918,67.607536315918,67.607536315918,67.607536315918,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,70.1433410644531,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,73.0231018066406,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,75.8131942749023,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,77.7749938964844,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,79.8741760253906,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,82.3384399414062,57.145378112793,57.145378112793,57.145378112793,57.145378112793,57.145378112793,57.145378112793,57.145378112793,57.145378112793,57.145378112793,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,21.5286712646484,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,24.1359024047852,26.7884902954102,26.7884902954102,26.7884902954102,26.7884902954102,26.7884902954102,26.7884902954102,26.7884902954102,29.9659271240234,29.9659271240234,29.9659271240234,29.9659271240234,29.9659271240234,29.9659271240234,29.9659271240234,33.3118286132812,33.3118286132812,33.3118286132812,33.3118286132812,33.3118286132812,33.3118286132812,33.3118286132812,33.3118286132812,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,36.1530532836914,40.2841186523438,40.2841186523438,40.2841186523438,40.2841186523438,40.2841186523438,40.2841186523438,40.2841186523438,40.2841186523438,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,44.0135192871094,49.4094085693359,49.4094085693359,49.4094085693359,49.4094085693359,49.4094085693359,49.4094085693359,49.4094085693359,49.4094085693359,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,51.9521636962891,54.9850616455078,54.9850616455078,54.9850616455078,54.9850616455078,54.9850616455078,54.9850616455078,54.9850616455078,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,59.2293701171875,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,62.3474960327148,64.7423324584961,64.7423324584961,64.7423324584961,64.7423324584961,64.7423324584961,64.7423324584961,67.3614120483398,67.3614120483398,67.3614120483398,67.3614120483398,67.3614120483398,67.3614120483398,71.5589294433594,71.5589294433594,71.5589294433594,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,74.7629013061523,77.299072265625,77.299072265625,77.299072265625,77.299072265625,77.299072265625,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,79.8419189453125,82.3706130981445,82.3706130981445,82.3706130981445,82.3706130981445,82.3706130981445,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,59.2949295043945,21.7789077758789,21.7789077758789,21.7789077758789,21.7789077758789,21.7789077758789,21.7789077758789,21.7789077758789,21.7789077758789,24.3767318725586,24.3767318725586,24.3767318725586,24.3767318725586,24.3767318725586,24.3767318725586,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,26.9127349853516,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,29.9449005126953,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,33.6819686889648,36.2219848632812,36.2219848632812,36.2219848632812,36.2219848632812,36.2219848632812,36.2219848632812,36.2219848632812,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,38.8306884765625,41.496826171875,41.496826171875,41.496826171875,41.496826171875,41.496826171875,41.496826171875,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,44.0895538330078,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,46.7847366333008,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,49.3952178955078,53.1156997680664,53.1156997680664,53.1156997680664,53.1156997680664,53.1156997680664,53.1156997680664,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,55.6535491943359,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,60.6528015136719,62.9184722900391,62.9184722900391,62.9184722900391,62.9184722900391,62.9184722900391,62.9184722900391,62.9184722900391,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,65.4572219848633,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,67.84814453125,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,70.4204025268555,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,72.8166656494141,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,75.3649139404297,80.3961715698242,80.3961715698242,80.3961715698242,80.3961715698242,80.3961715698242,80.3961715698242,80.3961715698242,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,83.2085418701172,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,33.1756973266602,22.0971832275391,22.0971832275391,22.0971832275391,22.0971832275391,22.0971832275391,22.0971832275391,22.0971832275391,22.0971832275391,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,25.0424728393555,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,27.9974212646484,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,30.4875335693359,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,33.0577850341797,36.0638809204102,36.0638809204102,36.0638809204102,36.0638809204102,36.0638809204102,36.0638809204102,36.0638809204102,38.9783248901367,38.9783248901367,38.9783248901367,38.9783248901367,38.9783248901367,38.9783248901367,38.9783248901367,38.9783248901367,43.2292556762695,43.2292556762695,43.2292556762695,43.2292556762695,43.2292556762695,43.2292556762695,43.2292556762695,43.2292556762695,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,46.0950927734375,48.6359558105469,48.6359558105469,48.6359558105469,48.6359558105469,48.6359558105469,48.6359558105469,48.6359558105469,52.0170822143555,52.0170822143555,52.0170822143555,52.0170822143555,52.0170822143555,52.0170822143555,55.3969192504883,55.3969192504883,55.3969192504883,55.3969192504883,55.3969192504883,55.3969192504883,55.3969192504883,57.5743637084961,57.5743637084961,57.5743637084961,57.5743637084961,57.5743637084961,57.5743637084961,60.5763168334961,60.5763168334961,60.5763168334961,60.5763168334961,60.5763168334961,60.5763168334961,63.197395324707,63.197395324707,63.197395324707,63.197395324707,63.197395324707,63.197395324707,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,65.7898788452148,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,68.1507720947266,70.7717819213867,70.7717819213867,70.7717819213867,70.7717819213867,70.7717819213867,70.7717819213867,70.7717819213867,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,73.1522445678711,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,75.6880950927734,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,78.2154235839844,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,80.7530364990234,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,83.5492782592773,22.0963592529297,22.0963592529297,22.0963592529297,22.0963592529297,22.0963592529297,22.0963592529297,22.0963592529297,22.0963592529297,24.9563446044922,24.9563446044922,24.9563446044922,24.9563446044922,24.9563446044922,24.9563446044922,24.9563446044922,24.9563446044922,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,27.5521926879883,30.1511840820312,30.1511840820312,30.1511840820312,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,34.2501602172852,38.1296997070312,38.1296997070312,38.1296997070312,38.1296997070312,38.1296997070312,38.1296997070312,38.1296997070312,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,40.6832885742188,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,43.5684585571289,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,46.1013107299805,50.3069229125977,50.3069229125977,50.3069229125977,50.3069229125977,50.3069229125977,50.3069229125977,50.3069229125977,53.1534576416016,53.1534576416016,53.1534576416016,53.1534576416016,53.1534576416016,56.1404647827148,56.1404647827148,56.1404647827148,56.1404647827148,56.1404647827148,56.1404647827148,56.1404647827148,58.9916763305664,58.9916763305664,58.9916763305664,58.9916763305664,61.9105224609375,61.9105224609375,61.9105224609375,61.9105224609375,61.9105224609375,61.9105224609375,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,64.5753555297852,67.294563293457,67.294563293457,67.294563293457,67.294563293457,67.294563293457,67.294563293457,67.294563293457,67.294563293457,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,71.4696655273438,74.4001693725586,74.4001693725586,74.4001693725586,74.4001693725586,74.4001693725586,74.4001693725586,77.3170700073242,77.3170700073242,77.3170700073242,77.3170700073242,77.3170700073242,77.3170700073242,77.3170700073242,77.3170700073242,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,80.2233505249023,83.0198745727539,83.0198745727539,83.0198745727539,83.0198745727539,83.0198745727539,83.0198745727539,83.0198745727539,83.0198745727539,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,25.7534637451172,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,24.6688461303711,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,29.0357131958008,34.4170303344727,34.4170303344727,34.4170303344727,34.4170303344727,34.4170303344727,34.4170303344727,34.4170303344727,34.4170303344727,37.7928161621094,37.7928161621094,37.7928161621094,37.7928161621094,37.7928161621094,37.7928161621094,37.7928161621094,41.6872787475586,41.6872787475586,41.6872787475586,41.6872787475586,41.6872787475586,41.6872787475586,41.6872787475586,41.6872787475586,44.6214828491211,44.6214828491211,44.6214828491211,44.6214828491211,44.6214828491211,44.6214828491211,44.6214828491211,44.6214828491211,50.4979248046875,50.4979248046875,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,53.3605041503906,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,56.2798309326172,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,59.2007827758789,62.1581878662109,62.1581878662109,62.1581878662109,62.1581878662109,62.1581878662109,62.1581878662109,62.1581878662109,62.1581878662109,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,64.9292984008789,70.6930389404297,70.6930389404297,70.6930389404297,70.6930389404297,70.6930389404297,70.6930389404297,70.6930389404297,70.6930389404297,73.3374938964844,73.3374938964844,73.3374938964844,73.3374938964844,73.3374938964844,73.3374938964844,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,76.2565536499023,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,79.0218811035156,81.6474685668945,81.6474685668945,81.6474685668945,81.6474685668945,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,37.9222717285156,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,22.9838790893555,28.5390701293945,28.5390701293945,28.5390701293945,28.5390701293945,28.5390701293945,28.5390701293945,28.5390701293945,28.5390701293945,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,31.4700698852539,34.0920257568359,34.0920257568359,34.0920257568359,34.0920257568359,34.0920257568359,34.0920257568359,34.0920257568359,34.0920257568359,36.9396743774414,36.9396743774414,36.9396743774414,36.9396743774414,36.9396743774414,36.9396743774414,36.9396743774414,36.9396743774414,42.7576446533203,42.7576446533203,42.7576446533203,42.7576446533203,42.7576446533203,42.7576446533203,42.7576446533203,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,45.6685256958008,48.1948394775391,48.1948394775391,48.1948394775391,48.1948394775391,48.1948394775391,48.1948394775391,51.8768920898438,51.8768920898438,51.8768920898438,51.8768920898438,51.8768920898438,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,54.8914108276367,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,59.0148239135742,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,61.9703598022461,66.6801910400391,66.6801910400391,66.6801910400391,66.6801910400391,66.6801910400391,66.6801910400391,66.6801910400391,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,70.4801559448242,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,74.0268173217773,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,77.1776504516602,81.075325012207,81.075325012207,81.075325012207,81.075325012207,81.075325012207,81.075325012207,81.075325012207,81.075325012207,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,83.7914047241211,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,25.1128082275391,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,28.0134201049805,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,30.9064254760742,33.5862884521484,33.5862884521484,33.5862884521484,33.5862884521484,33.5862884521484,33.5862884521484,33.5862884521484,33.5862884521484,36.476432800293,36.476432800293,36.476432800293,36.476432800293,36.476432800293,36.476432800293,39.3565216064453,39.3565216064453,39.3565216064453,39.3565216064453,45.0352020263672,45.0352020263672,45.0352020263672,45.0352020263672,45.0352020263672,45.0352020263672,45.0352020263672,45.0352020263672,47.951545715332,47.951545715332,47.951545715332,47.951545715332,47.951545715332,47.951545715332,47.951545715332,47.951545715332,47.951545715332,52.4450225830078,52.4450225830078,52.4450225830078,52.4450225830078,52.4450225830078,52.4450225830078,52.4450225830078,54.7530899047852,54.7530899047852,54.7530899047852,54.7530899047852,54.7530899047852,54.7530899047852,60.5202407836914,60.5202407836914,60.5202407836914,60.5202407836914,60.5202407836914,60.5202407836914,60.5202407836914,60.5202407836914,63.3877334594727,63.3877334594727,63.3877334594727,63.3877334594727,63.3877334594727,63.3877334594727,63.3877334594727,63.3877334594727,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,67.3471603393555,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,70.6601104736328,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,73.539924621582,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,76.5525512695312,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,81.634880065918,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,48.7898406982422,25.8420333862305,25.8420333862305,25.8420333862305,25.8420333862305,25.8420333862305,25.8420333862305,28.9067306518555,28.9067306518555,28.9067306518555,28.9067306518555,28.9067306518555,28.9067306518555,31.7945404052734,31.7945404052734,31.7945404052734,31.7945404052734,31.7945404052734,31.7945404052734,31.7945404052734,31.7945404052734,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,34.7137222290039,37.634521484375,37.634521484375,37.634521484375,37.634521484375,37.634521484375,37.634521484375,37.634521484375,37.634521484375,37.634521484375,41.93212890625,41.93212890625,41.93212890625,41.93212890625,41.93212890625,41.93212890625,41.93212890625,41.93212890625,41.93212890625,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,46.3894882202148,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,50.2809829711914,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,53.0995712280273,56.8492584228516,56.8492584228516,56.8492584228516,56.8492584228516,56.8492584228516,56.8492584228516,56.8492584228516,56.8492584228516,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,60.3502960205078,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,63.2936019897461,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,66.0344390869141,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,70.9385528564453,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,73.7580490112305,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,78.0134735107422,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,82.3897171020508,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,25.7177505493164,26.414176940918,26.414176940918,26.414176940918,26.414176940918,26.414176940918,26.414176940918,26.414176940918,26.414176940918,26.414176940918,29.3331146240234,29.3331146240234,29.3331146240234,29.3331146240234,29.3331146240234,29.3331146240234,29.3331146240234,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,32.1887741088867,38.0543441772461,38.0543441772461,38.0543441772461,38.0543441772461,38.0543441772461,38.0543441772461,38.0543441772461,38.0543441772461,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,41.0097122192383,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,43.9286651611328,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,46.8576354980469,50.7498779296875,50.7498779296875,50.7498779296875,50.7498779296875,50.7498779296875,50.7498779296875,50.7498779296875,50.7498779296875,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,53.6682510375977,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,56.5818176269531,59.5022888183594,59.5022888183594,59.5022888183594,59.5022888183594,59.5022888183594,59.5022888183594,59.5022888183594,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,65.3442001342773,68.2634811401367,68.2634811401367,68.2634811401367,68.2634811401367,68.2634811401367,68.2634811401367,68.2634811401367,68.2634811401367,72.0750427246094,72.0750427246094,72.0750427246094,72.0750427246094,72.0750427246094,72.0750427246094,72.0750427246094,72.0750427246094,76.9095687866211,76.9095687866211,76.9095687866211,76.9095687866211,76.9095687866211,76.9095687866211,76.9095687866211,76.9095687866211,79.7223587036133,79.7223587036133,79.7223587036133,79.7223587036133,79.7223587036133,79.7223587036133,79.7223587036133,79.7223587036133,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,82.3705825805664,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,28.9908294677734,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,26.950569152832,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,30.3165588378906,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,33.1793594360352,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,36.0985946655273,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,39.0735015869141,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,42.5538482666016,46.7937088012695,46.7937088012695,46.7937088012695,46.7937088012695,46.7937088012695,46.7937088012695,46.7937088012695,46.7937088012695,50.6900405883789,50.6900405883789,50.6900405883789,50.6900405883789,50.6900405883789,50.6900405883789,54.7816696166992,54.7816696166992,54.7816696166992,54.7816696166992,54.7816696166992,54.7816696166992,54.7816696166992,54.7816696166992,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,58.4756393432617,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,62.5852203369141,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,66.1127014160156,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,70.1864700317383,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,76.2194137573242,79.1888809204102,79.1888809204102,79.1888809204102,79.1888809204102,79.1888809204102,79.1888809204102,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,82.8790893554688,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,33.0522308349609,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,28.628173828125,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,31.7053527832031,34.9344482421875,34.9344482421875,34.9344482421875,34.9344482421875,34.9344482421875,34.9344482421875,34.9344482421875,34.9344482421875,39.04736328125,39.04736328125,39.04736328125,39.04736328125,39.04736328125,39.04736328125,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,42.3564224243164,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,47.3435134887695,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,50.3968505859375,53.616340637207,53.616340637207,53.616340637207,53.616340637207,53.616340637207,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,56.8225555419922,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,60.1273803710938,63.3027648925781,63.3027648925781,63.3027648925781,63.3027648925781,63.3027648925781,63.3027648925781,63.3027648925781,63.3027648925781,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,66.5206298828125,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,70.6091842651367,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,74.0667572021484,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,77.7083358764648,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,81.2608184814453,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,83.6311645507812,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,26.2720108032227,32.7168197631836,32.7168197631836,32.7168197631836,32.7168197631836,32.7168197631836,32.7168197631836,32.7168197631836,36.00244140625,36.00244140625,36.00244140625,36.00244140625,36.00244140625,36.00244140625,36.00244140625,36.00244140625,36.00244140625,39.1743469238281,39.1743469238281,39.1743469238281,39.1743469238281,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,41.7040634155273,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,44.7648696899414,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,51.2954711914062,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,54.5966949462891,60.8290634155273,60.8290634155273,60.8290634155273,60.8290634155273,60.8290634155273,60.8290634155273,60.8290634155273,60.8290634155273,64.1328353881836,64.1328353881836,64.1328353881836,64.1328353881836,64.1328353881836,64.1328353881836,64.1328353881836,64.1328353881836,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,67.400764465332,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,71.1352767944336,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,73.6734085083008,76.8100662231445,76.8100662231445,76.8100662231445,76.8100662231445,76.8100662231445,76.8100662231445,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,81.0651779174805,83.1213989257812,83.1213989257812,83.1213989257812,83.1213989257812,83.1213989257812,83.1213989257812,83.1213989257812,83.1213989257812,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,24.236328125,29.7574768066406,29.7574768066406,29.7574768066406,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,33.0610809326172,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,36.3667373657227,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,39.6684188842773,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,42.9726791381836,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,46.1705093383789,50.8552093505859,50.8552093505859,50.8552093505859,50.8552093505859,50.8552093505859,50.8552093505859,55.3093719482422,55.3093719482422,55.3093719482422,55.3093719482422,55.3093719482422,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,58.4101486206055,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,61.7010726928711,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,67.2318801879883,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,69.6237182617188,72.7483596801758,72.7483596801758,72.7483596801758,72.7483596801758,72.7483596801758,72.7483596801758,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,78.9205017089844,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,82.4136962890625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,52.0582275390625,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,18.0351791381836,20.2198791503906,20.2198791503906,20.2198791503906,20.2198791503906,20.2198791503906,20.2198791503906,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,23.5116577148438,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,25.7224273681641,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,28.264518737793,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,31.248046875,34.527458190918,34.527458190918,34.527458190918,34.527458190918,34.527458190918,34.527458190918,34.527458190918,34.527458190918,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,37.0819854736328,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,40.0654220581055,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,43.3461227416992,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,45.8993301391602,48.8709869384766,48.8709869384766,48.8709869384766,48.8709869384766,48.8709869384766,48.8709869384766,51.2693023681641,51.2693023681641,51.2693023681641,51.2693023681641,51.2693023681641,51.2693023681641,51.2693023681641,54.3826522827148,54.3826522827148,54.3826522827148,54.3826522827148,54.3826522827148,54.3826522827148,54.3826522827148,54.3826522827148,56.8357009887695,56.8357009887695,56.8357009887695,56.8357009887695,56.8357009887695,56.8357009887695,56.8357009887695,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,59.8950119018555,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,62.2453384399414,65.0448989868164,65.0448989868164,65.0448989868164,65.0448989868164,65.0448989868164,65.0448989868164,65.0448989868164,65.0448989868164,67.6216735839844,67.6216735839844,67.6216735839844,67.6216735839844,67.6216735839844,67.6216735839844,67.6216735839844,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,70.5355987548828,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,73.1309432983398,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,75.3395767211914,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,78.0592727661133,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,80.8451232910156,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,83.5709609985352,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,30.8995513916016,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,23.0725402832031,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,25.4199295043945,29.325553894043,29.325553894043,29.325553894043,29.325553894043,29.325553894043,29.325553894043,29.325553894043,29.325553894043,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,32.6295623779297,36.4808502197266,36.4808502197266,36.4808502197266,36.4808502197266,36.4808502197266,36.4808502197266,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,41.0382385253906,44.9025955200195,44.9025955200195,44.9025955200195,44.9025955200195,44.9025955200195,44.9025955200195,44.9025955200195,44.9025955200195,48.4306182861328,48.4306182861328,48.4306182861328,48.4306182861328,48.4306182861328,48.4306182861328,48.4306182861328,53.0128555297852,53.0128555297852,53.0128555297852,53.0128555297852,53.0128555297852,53.0128555297852,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,56.1400451660156,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,59.0606994628906,61.8928146362305,61.8928146362305,61.8928146362305,61.8928146362305,61.8928146362305,61.8928146362305,61.8928146362305,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,64.9531326293945,67.900260925293,67.900260925293,67.900260925293,67.900260925293,67.900260925293,67.900260925293,67.900260925293,67.900260925293,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,71.0981292724609,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,73.8951797485352,78.3515319824219,78.3515319824219,78.3515319824219,78.3515319824219,78.3515319824219,78.3515319824219,81.5002746582031,81.5002746582031,81.5002746582031,81.5002746582031,81.5002746582031,81.5002746582031,81.5002746582031,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,54.0520401000977,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,21.3253402709961,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,23.9280014038086,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,26.1388549804688,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,32.02294921875,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,36.755973815918,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,39.5007629394531,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,43.313102722168,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,47.2172470092773,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,50.3746795654297,53.439811706543,53.439811706543,53.439811706543,53.439811706543,53.439811706543,53.439811706543,53.439811706543,53.439811706543,53.439811706543,59.3654937744141,59.3654937744141,59.3654937744141,59.3654937744141,59.3654937744141,59.3654937744141,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,62.5038681030273,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,65.4592666625977,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,68.6298675537109,73.5262756347656,73.5262756347656,76.6985321044922,76.6985321044922,76.6985321044922,76.6985321044922,76.6985321044922,76.6985321044922,76.6985321044922,76.6985321044922,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,79.4232940673828,82.3551788330078,82.3551788330078,82.3551788330078,82.3551788330078,82.3551788330078,82.3551788330078,82.3551788330078,82.3551788330078,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,43.8910217285156,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,22.9501190185547,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,29.4765319824219,32.9996871948242,32.9996871948242,32.9996871948242,32.9996871948242,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,36.0943145751953,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,39.3922119140625,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,44.2574615478516,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,48.3608474731445,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,51.6376342773438,55.137336730957,55.137336730957,55.137336730957,55.137336730957,55.137336730957,55.137336730957,55.137336730957,55.137336730957,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,58.0594100952148,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,63.8535003662109,67.0730590820312,67.0730590820312,67.0730590820312,67.0730590820312,67.0730590820312,67.0730590820312,67.0730590820312,67.0730590820312,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,70.0909576416016,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,76.6294708251953,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,79.9350280761719,83.2768325805664,83.2768325805664,83.2768325805664,83.2768325805664,83.2768325805664,83.2768325805664,83.2768325805664,83.2768325805664,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,31.3454055786133,23.5183181762695,23.5183181762695,23.5183181762695,23.5183181762695,23.5183181762695,23.5183181762695,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,28.4311065673828,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,32.1627426147461,35.2786407470703,35.2786407470703,35.2786407470703,35.2786407470703,35.2786407470703,35.2786407470703,35.2786407470703,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,38.8642501831055,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,42.6924209594727,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,46.080924987793,49.4619293212891,49.4619293212891,49.4619293212891,49.4619293212891,49.4619293212891,49.4619293212891,49.4619293212891,49.4619293212891,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,52.5334854125977,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,55.923698425293,59.6004638671875,59.6004638671875,59.6004638671875,59.6004638671875,59.6004638671875,59.6004638671875,59.6004638671875,59.6004638671875,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,65.9316864013672,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,69.4313507080078,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,72.718864440918,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,77.2706832885742,81.9904479980469,81.9904479980469,81.9904479980469,81.9904479980469,81.9904479980469,40.4334411621094,40.4334411621094,40.4334411621094,40.4334411621094,40.4334411621094,40.4334411621094,40.4334411621094,40.4334411621094,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,23.9440231323242,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,28.6820449829102,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,31.3273544311523,35.1241836547852,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,38.7088623046875,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,42.2281112670898,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,47.1464996337891,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,49.7778549194336,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,54.1001586914062,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,57.1086502075195,60.3635406494141,60.3635406494141,60.3635406494141,60.3635406494141,60.3635406494141,60.3635406494141,60.3635406494141,60.3635406494141,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,63.857177734375,66.8331604003906,66.8331604003906,66.8331604003906,66.8331604003906,66.8331604003906,66.8331604003906,66.8331604003906,66.8331604003906,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,70.5867538452148,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,75.9996643066406,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,79.9758224487305,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,83.2454299926758,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,26.0974426269531,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,25.7900848388672,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,32.5790252685547,37.4953002929688,37.4953002929688,37.4953002929688,37.4953002929688,37.4953002929688,37.4953002929688,37.4953002929688,37.4953002929688,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,42.4212036132812,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,48.9927368164062,53.4953918457031,53.4953918457031,53.4953918457031,53.4953918457031,53.4953918457031,53.4953918457031,53.4953918457031,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,58.5826416015625,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,64.9855270385742,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,70.9533462524414,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,74.3930053710938,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,77.6823883056641,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,80.9230804443359,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,83.7913360595703,22.9686431884766,22.9686431884766,22.9686431884766,22.9686431884766,22.9686431884766,22.9686431884766,22.9686431884766,22.9686431884766,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,25.9269332885742,29.4433517456055,29.4433517456055,29.4433517456055,29.4433517456055,29.4433517456055,29.4433517456055,29.4433517456055,29.4433517456055,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,33.3082427978516,36.8292541503906,42.5895919799805,42.5895919799805,42.5895919799805,42.5895919799805,42.5895919799805,42.5895919799805,42.5895919799805,42.5895919799805,46.3306503295898,46.3306503295898,46.3306503295898,46.3306503295898,46.3306503295898,46.3306503295898,46.3306503295898,46.3306503295898,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,49.6315307617188,52.8218765258789,52.8218765258789,52.8218765258789,52.8218765258789,52.8218765258789,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,56.3808441162109,61.020149230957,61.020149230957,61.020149230957,61.020149230957,61.020149230957,61.020149230957,61.020149230957,61.020149230957,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,65.1194534301758,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,68.5675964355469,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,71.9393997192383,74.9640426635742,74.9640426635742,74.9640426635742,74.9640426635742,74.9640426635742,74.9640426635742,74.9640426635742,74.9640426635742,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,78.5647888183594,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,81.481071472168,62.5949325561523,62.5949325561523,62.5949325561523,62.5949325561523,62.5949325561523,62.5949325561523,62.5949325561523,62.5949325561523,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,23.4503326416016,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,26.9575347900391,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,32.7414093017578,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,36.5267181396484,39.1092834472656,39.1092834472656,39.1092834472656,39.1092834472656,39.1092834472656,39.1092834472656,39.1092834472656,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,42.5731582641602,45.853385925293,45.853385925293,45.853385925293,45.853385925293,45.853385925293,45.853385925293,45.853385925293,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,51.1802062988281,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,54.4681167602539,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,59.7301559448242,63.477424621582,63.477424621582,63.477424621582,63.477424621582,63.477424621582,63.477424621582,63.477424621582,63.477424621582,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,67.2999649047852,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,70.4836196899414,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,73.6964569091797,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,77.1951599121094,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,80.7962646484375,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,83.7850646972656,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,21.9179611206055,25.2646102905273,25.2646102905273,25.2646102905273,25.2646102905273,25.2646102905273,25.2646102905273,25.2646102905273,25.2646102905273,28.575927734375,28.575927734375,28.575927734375,28.575927734375,28.575927734375,28.575927734375,28.575927734375,28.575927734375,28.575927734375,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,31.5958633422852,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,34.2491455078125,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,37.112434387207,40.2732772827148,40.2732772827148,40.2732772827148,40.2732772827148,40.2732772827148,40.2732772827148,40.2732772827148,40.2732772827148,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,43.7157745361328,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,47.3467254638672,51.1005401611328,51.1005401611328,51.1005401611328,51.1005401611328,51.1005401611328,51.1005401611328,51.1005401611328,54.3835525512695,54.3835525512695,54.3835525512695,54.3835525512695,54.3835525512695,54.3835525512695,57.2607498168945,57.2607498168945,57.2607498168945,57.2607498168945,57.2607498168945,57.2607498168945,57.2607498168945,60.5345458984375,60.5345458984375,60.5345458984375,60.5345458984375,60.5345458984375,60.5345458984375,60.5345458984375,62.9950103759766,62.9950103759766,62.9950103759766,62.9950103759766,62.9950103759766,62.9950103759766,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,64.7024841308594,67.0121154785156,67.0121154785156,67.0121154785156,67.0121154785156,67.0121154785156,67.0121154785156,67.0121154785156,67.0121154785156,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,68.3257369995117,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,69.5610122680664,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,71.1544342041016,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,73.2396392822266,74.7583236694336,74.7583236694336,74.7583236694336,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,76.9369812011719,78.3408355712891,78.3408355712891,78.3408355712891,78.3408355712891,78.3408355712891,78.3408355712891,78.3408355712891,78.3408355712891,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,80.6298065185547,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,82.681510925293,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,83.7900390625,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,30.0042266845703,23.2647705078125,23.2647705078125,23.2647705078125,23.2647705078125,23.2647705078125,23.2647705078125,23.2647705078125,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,25.0991668701172,26.9550857543945,26.9550857543945,26.9550857543945,26.9550857543945,26.9550857543945,26.9550857543945,26.9550857543945,26.9550857543945,29.2480697631836,29.2480697631836,29.2480697631836,29.2480697631836,29.2480697631836,29.2480697631836,29.2480697631836,29.2480697631836,30.4785614013672,30.4785614013672,30.4785614013672,30.4785614013672,30.4785614013672,30.4785614013672,30.4785614013672,30.4785614013672,32.9223861694336,32.9223861694336,32.9223861694336,32.9223861694336,32.9223861694336,32.9223861694336,32.9223861694336,32.9223861694336,35.0089263916016,35.0089263916016,35.0089263916016,35.0089263916016,35.0089263916016,35.0089263916016,35.0089263916016,35.0089263916016,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,36.6130142211914,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,39.0856094360352,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,41.53759765625,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,44.0065689086914,48.515251159668,48.515251159668,48.515251159668,48.515251159668,48.515251159668,48.515251159668,48.515251159668,51.3882064819336,51.3882064819336,51.3882064819336,51.3882064819336,51.3882064819336,51.3882064819336,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,55.0688781738281,58.3828887939453,58.3828887939453,58.3828887939453,58.3828887939453,58.3828887939453,58.3828887939453,58.3828887939453,58.3828887939453,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,61.8606796264648,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,65.6960830688477,68.8909301757812,68.8909301757812,68.8909301757812,68.8909301757812,68.8909301757812,68.8909301757812,68.8909301757812,68.8909301757812,72.0344085693359,72.0344085693359,72.0344085693359,72.0344085693359,72.0344085693359,72.0344085693359,72.0344085693359,72.0344085693359,74.9897842407227,74.9897842407227,74.9897842407227,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,79.2713470458984,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,82.2991104125977,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,48.0694427490234,24.603385925293,24.603385925293,24.603385925293,24.603385925293,24.603385925293,24.603385925293,24.603385925293,24.603385925293],"meminc":[0,0,0,0,0,0,0,0,0,0,0,1.72875213623047,0,0,0,0,0,0,0,0,0,0,0,0,0,1.98513031005859,0,0,0,0,0,2.09463500976562,0,0,0,2.10006713867188,0,0,0,0,0,0,0,0,0,0,1.75175476074219,0,0,0,0,0,0,0,0,0,0,0,2.02552795410156,0,0,0,0,0,0,0,0,0,0,0,0,0,2.09635162353516,0,0,0,0,0,2.12367248535156,0,0,0,0,0,2.05925750732422,0,0,0,0,0,0,0,0,2.11593627929688,0,0,0,0,0,0,0,0,0,0,0,2.138671875,0,0,0,0,0,0,0,0,0,0,2.09271240234375,0,0,0,0,0,0,0,0,1.94387817382812,0,0,0,0,0,0,0,2.09651947021484,0,0,0,0,0,0,0,2.04193115234375,0,0,0,0,0,0,0,0,0,0,0,0,0,2.01438903808594,0,0,0,0,0,2.08979034423828,0,0,0,1.59886932373047,0,0,0,0,0,0,0,0,0,0,0,1.8782958984375,0,0,0,0,0,0,0,0,0.0006866455078125,0,0,0,0,0,0.7772216796875,0,0,0,0,0,1.59785461425781,0,0,0,0,0,0,0,0,0,0,0,0,0,0.831451416015625,0,0,0,0,0,0,0,0,0,1.10647583007812,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.47220611572266,0,0,0,0,0,0,0,0,0,1.79421234130859,0,0,0,0,0,0,0,1.75014495849609,0,0,0,0,0,0,0,0,0,0,1.58481597900391,0,0,0,0,0,0,0,1.68125152587891,0,0,0,0,0,0,0,0,0,1.42040252685547,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.75959014892578,0,0,0,0,0,0,0,0,0,0,1.64604187011719,0,0,0,0,0,0,0,0,1.93020629882812,0,0,0,0,0,0,0,0,0,0,0,1.66844940185547,0,0,0,0,0,0,0,0,0,0,0,0,1.88545227050781,0,0,0,0,0,0,0,1.68267822265625,0,0,0,0,0,0,0,0,0,1.44841766357422,0,0,0,0,0,0,0,0,0,0,0,0,0,-36.6327667236328,0,0,0,0,0,0,0,0,0,0,0,0,0,-27.8513870239258,0,0,0,1.82080841064453,0,0,0,0,0,0,0,0,0,1.97364807128906,0,0,0,0,0,0,0,0,0,1.97988128662109,0,0,0,0,0,0,0,2.10748291015625,0,0,0,0,0,0,0,0,2.13583374023438,0,0,0,0,0,0,0,0,1.82839965820312,0,0,0,0,0,0,1.98175048828125,0,0,0,0,0,0,0,0,0,0,1.93939208984375,0,0,0,0,0,0,0,2.14182281494141,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.09197998046875,0,0,0,0,0,0,0,1.87781524658203,0,0,0,0,0,0,0,0,0,0,0,0,0,1.81157684326172,0,0,0,0,0,0,1.39383697509766,0,0,0,0,0,0,0,0,0,0,1.42790985107422,0,0,0,0,0,0,0,0,0,1.47908782958984,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.64257049560547,0,0,0,0,0,0,0,0,0,0,0,0,0,1.88847351074219,1.65795135498047,0,0,0,0,0,0,0,0,0,0,0,0,0,1.9342041015625,0,0,0,0,0,0,0,0,1.83700561523438,0,0,0,0,0,0,0,0,1.90543365478516,0,0,0,0,0,1.83686065673828,0,0,0,0,0,0,0,1.98699188232422,0,0,0,0,0,0,0,2.72544097900391,0,0,0,0,0,0,0,0,0,0,0,3.72833251953125,0,0,0,0,0,0,0,0,2.45927429199219,0,0,0,0,0,0,0,0,0,0,0,3.978271484375,0,0,0,0,0,0,0,0,0,0,0,0,2.15078735351562,0,0,0,0,0,0,0,1.84632110595703,0,0,0,2.14206695556641,2.11444854736328,0,0,0,0,0,0,0,0,-64.4777984619141,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.28890228271484,0,0,0,0,0,0,0,0,0,0,2.46493530273438,0,0,0,0,0,0,0,0,0,0,2.15756988525391,0,0,0,0,0,0,0,0,0,0,2.1944580078125,0,0,0,0,0,0,0,0,0,2.26351928710938,0,0,0,0,0,0,2.32496643066406,0,0,0,0,0,0,0,0,2.80975341796875,0,0,0,0,0,0,0,0,0,0,0,0,2.26850891113281,0,0,0,0,0,0,0,2.46468353271484,0,0,0,0,0,0,0,0,0,2.15940856933594,0,0,0,0,0,0,0,0,0,0,0,0,0,2.20445251464844,0,0,0,0,0,2.11143493652344,0,0,0,0,0,0,0,0,0,0,2.17904663085938,0,0,0,0,0,0,0,4.25357055664062,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.18717193603516,0,0,0,0,0,0,0,0,0,0,0,2.11111450195312,0,0,0,0,0,0,0,0,0,0,0,0,0,3.2508544921875,0,0,0,0,0,0,0,0,0,0,2.36747741699219,0,0,0,0,0,0,0,1.76714324951172,0,0,0,0,0,0,0,0,0,0,0,0,0,2.17426300048828,0,0,0,0,0,0,0,0,0,0,0,0,0,2.03681945800781,0,0,0,0,0,2.01636505126953,0,0,0,0,0,0,0,2.15682983398438,0,0,0,0,0,0,0,3.24747467041016,0,0,0,0,0,0,0,0,1.04632568359375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.80175018310547,0,0,0,0,0,0,0,0,0,2.32615661621094,0,0,0,0,0,0,-40.9421997070312,0,0,0,0,0,0,-20.2195587158203,0,0,0,0,0,0,0,2.21920013427734,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.478271484375,0,0,0,0,0,0,0,0,0,2.46034240722656,0,0,0,0,0,0,0,0,0,2.14631652832031,0,0,0,0,0,0,2.15424346923828,0,0,0,0,0,0,1.83898162841797,0,0,0,0,0,2.4542236328125,0,0,0,0,0,2.15346527099609,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.15263366699219,0,0,0,0,0,0,0,0,2.53376770019531,0,0,0,0,0,0,0,0,2.79563903808594,0,0,0,0,0,0,0,0,0,0,0,2.55948638916016,0,0,0,0,0,0,0,2.85614776611328,0,0,0,0,0,0,0,0,0,0,0,0,2.15708923339844,0,0,0,0,0,0,0,0,0,0,0,0,0,2.28318023681641,0,0,0,0,0,2.59770965576172,0,0,0,0,0,0,0,2.5595703125,0,0,0,0,0,0,0,0,0,0,0,0,0,2.59812927246094,0,0,0,0,0,0,0,0,0,0,0,2.30100250244141,0,0,0,0,0,1.99559020996094,0,0,0,0,2.13874816894531,0,0,0,0,0,2.51811981201172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.18336486816406,0,0,0,0,0,0,0,2.46502685546875,0,0,0,0,0,2.16116333007812,0,0,0,0,-57.9420928955078,0,0,0,0,0,0,0,0,-2.69547271728516,0,0,0,0,0,0,0,0,0,2.330078125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.31373596191406,0,0,0,0,0,0,2.29251098632812,0,0,0,0,0,0,0,4.52732849121094,0,0,0,0,0,0,0,0,2.48892974853516,0,0,0,0,0,2.4266357421875,3.36531829833984,0,0,0,0,0,0,0,3.04869842529297,0,0,0,0,0,0,0,0,4.225830078125,0,0,0,0,0,0,0,0,3.23076629638672,0,0,0,0,0,0,0,0,0,0,0,0,0,2.66114044189453,0,0,0,0,0,0,0,0,0,0,2.535888671875,0,0,0,0,0,0,0,0,3.38108062744141,0,0,0,0,0,0,0,0,0,0,0,0,3.21971130371094,0,0,0,0,0,0,0,0,0,0,0,0,0,2.68099212646484,0,0,0,0,0,0,0,0,0,0,0,0,0,4.59107971191406,0,0,0,0,0,3.37656402587891,0,0,0,0,0,0,0,0,0,5.08319854736328,0,0,0,0,0,0,3.38725280761719,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-62.3885345458984,0,0,0,0,0,0,0,0,0,0,0,0,2.62163543701172,0,0,0,0,0,0,2.53653717041016,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.55295562744141,0,0,0,0,0,2.53504180908203,0,0,0,0,0,0,0,2.74964904785156,0,0,0,0,0,0,0,2.64630889892578,0,0,0,0,5.0787353515625,0,0,0,0,0,0,0,0,0,0,0,2.54118347167969,0,0,0,0,0,0,0,0,0,0,0,2.32483673095703,0,0,0,0,0,0,0,0,2.39826965332031,0,0,0,0,0,0,0,2.14094543457031,0,0,0,0,0,2.50712585449219,0,0,0,0,0,0,0,0,3.79993438720703,0,0,0,0,0,0,0,0,0,0,0,2.53871917724609,0,0,0,0,0,0,0,0,0,2.61511993408203,0,0,0,0,0,0,0,2.55377197265625,0,0,0,0,0,0,0,2.43858337402344,0,0,0,0,0,0,0,0,2.53580474853516,0,0,0,0,0,0,0,0,2.8797607421875,0,0,0,0,0,0,0,0,0,2.79009246826172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.96179962158203,0,0,0,0,0,0,0,0,2.09918212890625,0,0,0,0,0,0,0,0,2.46426391601562,0,0,0,0,0,0,0,0,0,0,-25.1930618286133,0,0,0,0,0,0,0,0,-35.6167068481445,0,0,0,0,0,0,0,0,0,0,0,2.60723114013672,0,0,0,0,0,0,0,0,0,2.652587890625,0,0,0,0,0,0,3.17743682861328,0,0,0,0,0,0,3.34590148925781,0,0,0,0,0,0,0,2.84122467041016,0,0,0,0,0,0,0,0,0,0,0,0,0,4.13106536865234,0,0,0,0,0,0,0,3.72940063476562,0,0,0,0,0,0,0,0,0,0,0,0,0,5.39588928222656,0,0,0,0,0,0,0,2.54275512695312,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.03289794921875,0,0,0,0,0,0,4.24430847167969,0,0,0,0,0,0,0,0,0,0,3.11812591552734,0,0,0,0,0,0,0,0,0,2.39483642578125,0,0,0,0,0,2.61907958984375,0,0,0,0,0,4.19751739501953,0,0,3.20397186279297,0,0,0,0,0,0,0,0,2.53617095947266,0,0,0,0,2.5428466796875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.52869415283203,0,0,0,0,-23.07568359375,0,0,0,0,0,0,0,0,-37.5160217285156,0,0,0,0,0,0,0,2.59782409667969,0,0,0,0,0,2.53600311279297,0,0,0,0,0,0,0,0,0,3.03216552734375,0,0,0,0,0,0,0,0,0,0,3.73706817626953,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.54001617431641,0,0,0,0,0,0,2.60870361328125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.6661376953125,0,0,0,0,0,2.59272766113281,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.69518280029297,0,0,0,0,0,0,0,0,0,0,0,2.61048126220703,0,0,0,0,0,0,0,0,0,0,0,0,0,3.72048187255859,0,0,0,0,0,2.53784942626953,0,0,0,0,0,0,0,0,0,4.99925231933594,0,0,0,0,0,0,0,0,2.26567077636719,0,0,0,0,0,0,2.53874969482422,0,0,0,0,0,0,0,0,0,0,0,0,2.39092254638672,0,0,0,0,0,0,0,0,0,0,0,0,0,2.57225799560547,0,0,0,0,0,0,0,0,0,0,0,0,2.39626312255859,0,0,0,0,0,0,0,0,2.54824829101562,0,0,0,0,0,0,0,0,0,0,0,5.03125762939453,0,0,0,0,0,0,2.81237030029297,0,0,0,0,0,0,0,0,-50.032844543457,0,0,0,0,0,0,0,0,-11.0785140991211,0,0,0,0,0,0,0,2.94528961181641,0,0,0,0,0,0,0,0,2.95494842529297,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.4901123046875,0,0,0,0,0,0,0,0,0,0,0,2.57025146484375,0,0,0,0,0,0,0,0,0,0,0,3.00609588623047,0,0,0,0,0,0,2.91444396972656,0,0,0,0,0,0,0,4.25093078613281,0,0,0,0,0,0,0,2.86583709716797,0,0,0,0,0,0,0,0,0,0,0,2.54086303710938,0,0,0,0,0,0,3.38112640380859,0,0,0,0,0,3.37983703613281,0,0,0,0,0,0,2.17744445800781,0,0,0,0,0,3.001953125,0,0,0,0,0,2.62107849121094,0,0,0,0,0,2.59248352050781,0,0,0,0,0,0,0,0,0,0,0,2.36089324951172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.62100982666016,0,0,0,0,0,0,2.38046264648438,0,0,0,0,0,0,0,0,0,0,0,2.53585052490234,0,0,0,0,0,0,0,0,0,0,0,2.52732849121094,0,0,0,0,0,0,0,0,0,0,0,2.53761291503906,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.79624176025391,0,0,0,0,0,0,0,0,0,0,0,0,0,-61.4529190063477,0,0,0,0,0,0,0,2.8599853515625,0,0,0,0,0,0,0,2.59584808349609,0,0,0,0,0,0,0,0,0,0,0,2.59899139404297,0,0,4.09897613525391,0,0,0,0,0,0,0,0,0,3.87953948974609,0,0,0,0,0,0,2.5535888671875,0,0,0,0,0,0,0,0,2.88516998291016,0,0,0,0,0,0,0,0,0,0,2.53285217285156,0,0,0,0,0,0,0,0,0,0,0,4.20561218261719,0,0,0,0,0,0,2.84653472900391,0,0,0,0,2.98700714111328,0,0,0,0,0,0,2.85121154785156,0,0,0,2.91884613037109,0,0,0,0,0,2.66483306884766,0,0,0,0,0,0,0,0,2.71920776367188,0,0,0,0,0,0,0,4.17510223388672,0,0,0,0,0,0,0,0,0,2.93050384521484,0,0,0,0,0,2.91690063476562,0,0,0,0,0,0,0,2.90628051757812,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.79652404785156,0,0,0,0,0,0,0,-57.2664108276367,0,0,0,0,0,0,0,0,0,0,-1.08461761474609,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.36686706542969,0,0,0,0,0,0,0,0,0,5.38131713867188,0,0,0,0,0,0,0,3.37578582763672,0,0,0,0,0,0,3.89446258544922,0,0,0,0,0,0,0,2.9342041015625,0,0,0,0,0,0,0,5.87644195556641,0,2.86257934570312,0,0,0,0,0,0,0,0,0,0,0,2.91932678222656,0,0,0,0,0,0,0,0,0,0,2.92095184326172,0,0,0,0,0,0,0,0,0,0,2.95740509033203,0,0,0,0,0,0,0,2.77111053466797,0,0,0,0,0,0,0,0,0,0,0,0,0,5.76374053955078,0,0,0,0,0,0,0,2.64445495605469,0,0,0,0,0,2.91905975341797,0,0,0,0,0,0,0,0,2.76532745361328,0,0,0,0,0,0,0,0,0,0,0,2.62558746337891,0,0,0,-43.7251968383789,0,0,0,0,0,0,0,0,0,0,0,0,-14.9383926391602,0,0,0,0,0,0,0,0,0,0,0,5.55519104003906,0,0,0,0,0,0,0,2.93099975585938,0,0,0,0,0,0,0,0,0,0,2.62195587158203,0,0,0,0,0,0,0,2.84764862060547,0,0,0,0,0,0,0,5.81797027587891,0,0,0,0,0,0,2.91088104248047,0,0,0,0,0,0,0,0,0,0,0,0,2.52631378173828,0,0,0,0,0,3.68205261230469,0,0,0,0,3.01451873779297,0,0,0,0,0,0,0,0,0,0,0,0,4.1234130859375,0,0,0,0,0,0,0,0,0,0,0,0,2.95553588867188,0,0,0,0,0,0,0,0,0,0,0,0,4.70983123779297,0,0,0,0,0,0,3.79996490478516,0,0,0,0,0,0,0,0,0,0,0,0,0,3.54666137695312,0,0,0,0,0,0,0,0,3.15083312988281,0,0,0,0,0,0,0,0,0,0,0,3.89767456054688,0,0,0,0,0,0,0,2.71607971191406,0,0,0,0,0,0,0,0,-58.678596496582,0,0,0,0,0,0,0,0,0,0,2.90061187744141,0,0,0,0,0,0,0,0,0,0,0,0,0,2.89300537109375,0,0,0,0,0,0,0,0,0,0,0,2.67986297607422,0,0,0,0,0,0,0,2.89014434814453,0,0,0,0,0,2.88008880615234,0,0,0,5.67868041992188,0,0,0,0,0,0,0,2.91634368896484,0,0,0,0,0,0,0,0,4.49347686767578,0,0,0,0,0,0,2.30806732177734,0,0,0,0,0,5.76715087890625,0,0,0,0,0,0,0,2.86749267578125,0,0,0,0,0,0,0,3.95942687988281,0,0,0,0,0,0,0,0,3.31295013427734,0,0,0,0,0,0,0,0,0,0,0,0,2.87981414794922,0,0,0,0,0,0,0,0,0,0,0,0,0,3.01262664794922,0,0,0,0,0,0,0,0,0,0,0,0,0,5.08232879638672,0,0,0,0,0,0,0,0,0,0,-32.8450393676758,0,0,0,0,0,0,0,0,-22.9478073120117,0,0,0,0,0,3.064697265625,0,0,0,0,0,2.88780975341797,0,0,0,0,0,0,0,2.91918182373047,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.92079925537109,0,0,0,0,0,0,0,0,4.297607421875,0,0,0,0,0,0,0,0,4.45735931396484,0,0,0,0,0,0,0,0,0,0,0,3.89149475097656,0,0,0,0,0,0,0,0,0,0,2.81858825683594,0,0,0,0,0,0,0,0,0,0,3.74968719482422,0,0,0,0,0,0,0,3.50103759765625,0,0,0,0,0,0,0,0,0,0,0,0,0,2.94330596923828,0,0,0,0,0,0,0,0,0,0,0,2.74083709716797,0,0,0,0,0,0,0,0,0,0,0,0,0,4.90411376953125,0,0,0,0,0,0,0,0,0,0,0,0,2.81949615478516,0,0,0,0,0,0,0,0,4.25542449951172,0,0,0,0,0,0,0,0,0,0,0,0,4.37624359130859,0,0,0,0,0,0,0,0,0,0,-56.6719665527344,0,0,0,0,0,0,0,0,0,0,0,0,0,0.696426391601562,0,0,0,0,0,0,0,0,2.91893768310547,0,0,0,0,0,0,2.85565948486328,0,0,0,0,0,0,0,0,0,0,5.86557006835938,0,0,0,0,0,0,0,2.95536804199219,0,0,0,0,0,0,0,0,0,2.91895294189453,0,0,0,0,0,0,0,0,0,2.92897033691406,0,0,0,0,0,0,0,0,0,0,0,0,3.89224243164062,0,0,0,0,0,0,0,2.91837310791016,0,0,0,0,0,0,0,0,0,0,2.91356658935547,0,0,0,0,0,0,0,0,0,0,0,2.92047119140625,0,0,0,0,0,0,5.84191131591797,0,0,0,0,0,0,0,0,2.91928100585938,0,0,0,0,0,0,0,3.81156158447266,0,0,0,0,0,0,0,4.83452606201172,0,0,0,0,0,0,0,2.81278991699219,0,0,0,0,0,0,0,2.64822387695312,0,0,0,0,0,0,0,0,0,0,0,-53.379753112793,0,0,0,0,0,0,0,0,-2.04026031494141,0,0,0,0,0,0,0,0,0,0,0,0,0,3.36598968505859,0,0,0,0,0,0,0,0,0,0,2.86280059814453,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.91923522949219,0,0,0,0,0,0,0,0,0,0,0,0,2.97490692138672,0,0,0,0,0,0,0,0,0,0,0,3.4803466796875,0,0,0,0,0,0,0,0,0,0,4.23986053466797,0,0,0,0,0,0,0,3.89633178710938,0,0,0,0,0,4.09162902832031,0,0,0,0,0,0,0,3.6939697265625,0,0,0,0,0,0,0,0,0,0,0,0,4.10958099365234,0,0,0,0,0,0,0,0,0,0,0,3.52748107910156,0,0,0,0,0,0,0,0,0,0,0,0,0,4.07376861572266,0,0,0,0,0,0,0,0,0,0,0,6.03294372558594,0,0,0,0,0,0,0,0,0,0,0,0,0,2.96946716308594,0,0,0,0,0,3.69020843505859,0,0,0,0,0,0,0,0,0,0,0,-49.8268585205078,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.42405700683594,0,0,0,0,0,0,0,0,0,0,0,3.07717895507812,0,0,0,0,0,0,0,0,0,0,0,0,3.22909545898438,0,0,0,0,0,0,0,4.1129150390625,0,0,0,0,0,3.30905914306641,0,0,0,0,0,0,0,0,0,4.98709106445312,0,0,0,0,0,0,0,0,0,0,0,0,3.05333709716797,0,0,0,0,0,0,0,0,0,0,0,3.21949005126953,0,0,0,0,3.20621490478516,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.30482482910156,0,0,0,0,0,0,0,0,3.17538452148438,0,0,0,0,0,0,0,3.21786499023438,0,0,0,0,0,0,0,0,4.08855438232422,0,0,0,0,0,0,0,0,0,0,0,0,0,3.45757293701172,0,0,0,0,0,0,0,0,3.64157867431641,0,0,0,0,0,0,0,0,0,0,3.55248260498047,0,0,0,0,0,0,0,0,0,2.37034606933594,0,0,0,0,0,0,0,0,0,0,0,0,-57.3591537475586,0,0,0,0,0,0,0,0,0,0,0,6.44480895996094,0,0,0,0,0,0,3.28562164306641,0,0,0,0,0,0,0,0,3.17190551757812,0,0,0,2.52971649169922,0,0,0,0,0,0,0,0,0,0,0,0,3.06080627441406,0,0,0,0,0,0,0,0,0,0,6.53060150146484,0,0,0,0,0,0,0,0,0,0,3.30122375488281,0,0,0,0,0,0,0,0,0,0,6.23236846923828,0,0,0,0,0,0,0,3.30377197265625,0,0,0,0,0,0,0,3.26792907714844,0,0,0,0,0,0,0,0,0,0,0,0,0,3.73451232910156,0,0,0,0,0,0,0,0,0,0,0,0,2.53813171386719,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.13665771484375,0,0,0,0,0,4.25511169433594,0,0,0,0,0,0,0,0,0,0,0,2.05622100830078,0,0,0,0,0,0,0,-58.8850708007812,0,0,0,0,0,0,0,0,0,5.52114868164062,0,0,3.30360412597656,0,0,0,0,0,0,0,0,0,0,0,0,0,3.30565643310547,0,0,0,0,0,0,0,0,0,0,3.30168151855469,0,0,0,0,0,0,0,0,0,0,0,3.30426025390625,0,0,0,0,0,0,0,0,0,0,3.19783020019531,0,0,0,0,0,0,0,0,4.68470001220703,0,0,0,0,0,4.45416259765625,0,0,0,0,3.10077667236328,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.29092407226562,0,0,0,0,0,0,0,0,5.53080749511719,0,0,0,0,0,0,0,0,0,0,0,0,2.39183807373047,0,0,0,0,0,0,0,0,3.12464141845703,0,0,0,0,0,6.17214202880859,0,0,0,0,0,0,0,0,0,0,0,0,0,3.49319458007812,0,0,0,0,0,0,0,0,-30.35546875,0,0,0,0,0,0,0,0,0,0,0,-34.0230484008789,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.18470001220703,0,0,0,0,0,3.29177856445312,0,0,0,0,0,0,0,0,2.21076965332031,0,0,0,0,0,0,0,0,2.54209136962891,0,0,0,0,0,0,0,0,0,0,0,0,2.98352813720703,0,0,0,0,0,0,0,0,0,0,0,3.27941131591797,0,0,0,0,0,0,0,2.55452728271484,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.98343658447266,0,0,0,0,0,0,0,0,0,3.28070068359375,0,0,0,0,0,0,0,0,2.55320739746094,0,0,0,0,0,0,0,0,0,0,0,0,2.97165679931641,0,0,0,0,0,2.3983154296875,0,0,0,0,0,0,3.11334991455078,0,0,0,0,0,0,0,2.45304870605469,0,0,0,0,0,0,3.05931091308594,0,0,0,0,0,0,0,0,0,0,0,2.35032653808594,0,0,0,0,0,0,0,0,0,0,0,2.799560546875,0,0,0,0,0,0,0,2.57677459716797,0,0,0,0,0,0,2.91392517089844,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.59534454345703,0,0,0,0,0,0,0,0,0,0,0,0,0,2.20863342285156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.71969604492188,0,0,0,0,0,0,0,0,0,0,0,0,0,2.78585052490234,0,0,0,0,0,0,0,0,0,2.72583770751953,0,0,0,0,0,0,0,0,0,0,0,0,0,-52.6714096069336,0,0,0,0,0,0,0,0,0,0,0,-7.82701110839844,0,0,0,0,0,0,0,0,2.34738922119141,0,0,0,0,0,0,0,0,3.90562438964844,0,0,0,0,0,0,0,3.30400848388672,0,0,0,0,0,0,0,0,3.85128784179688,0,0,0,0,0,4.55738830566406,0,0,0,0,0,0,0,0,3.86435699462891,0,0,0,0,0,0,0,3.52802276611328,0,0,0,0,0,0,4.58223724365234,0,0,0,0,0,3.12718963623047,0,0,0,0,0,0,0,0,0,0,0,0,0,2.920654296875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.83211517333984,0,0,0,0,0,0,3.06031799316406,0,0,0,0,0,0,0,0,0,2.94712829589844,0,0,0,0,0,0,0,3.19786834716797,0,0,0,0,0,0,0,0,0,0,0,0,2.79705047607422,0,0,0,0,0,0,0,0,0,0,0,0,4.45635223388672,0,0,0,0,0,3.14874267578125,0,0,0,0,0,0,-27.4482345581055,0,0,0,0,0,0,0,0,0,0,0,0,-32.7266998291016,0,0,0,0,0,0,0,0,0,2.6026611328125,0,0,0,0,0,0,0,0,0,0,0,0,2.21085357666016,0,0,0,0,0,0,0,0,0,0,5.88409423828125,0,0,0,0,0,0,0,0,0,0,0,0,4.73302459716797,0,0,0,0,0,0,0,0,0,0,0,2.74478912353516,0,0,0,0,0,0,0,0,0,3.81233978271484,0,0,0,0,0,0,0,0,0,0,0,3.90414428710938,0,0,0,0,0,0,0,0,0,3.15743255615234,0,0,0,0,0,0,0,0,0,0,0,3.06513214111328,0,0,0,0,0,0,0,0,5.92568206787109,0,0,0,0,0,3.13837432861328,0,0,0,0,0,0,0,0,0,0,2.95539855957031,0,0,0,0,0,0,0,0,3.17060089111328,0,0,0,0,0,0,0,0,0,4.89640808105469,0,3.17225646972656,0,0,0,0,0,0,0,2.72476196289062,0,0,0,0,0,0,0,0,0,0,0,0,0,2.931884765625,0,0,0,0,0,0,0,-38.4641571044922,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-20.9409027099609,0,0,0,0,0,0,0,0,0,0,6.52641296386719,0,0,0,0,0,0,0,0,0,0,3.52315521240234,0,0,0,3.09462738037109,0,0,0,0,0,0,0,0,0,0,0,0,3.29789733886719,0,0,0,0,0,0,0,0,0,4.86524963378906,0,0,0,0,0,0,0,0,0,0,0,0,4.10338592529297,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.27678680419922,0,0,0,0,0,0,0,0,0,0,0,3.49970245361328,0,0,0,0,0,0,0,2.92207336425781,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5.79409027099609,0,0,0,0,0,0,0,0,0,0,0,3.21955871582031,0,0,0,0,0,0,0,3.01789855957031,0,0,0,0,0,0,0,0,6.53851318359375,0,0,0,0,0,0,0,0,0,0,0,0,0,3.30555725097656,0,0,0,0,0,0,0,0,0,0,3.34180450439453,0,0,0,0,0,0,0,-51.9314270019531,0,0,0,0,0,0,0,0,-7.82708740234375,0,0,0,0,0,4.91278839111328,0,0,0,0,0,0,0,0,0,0,0,0,3.73163604736328,0,0,0,0,0,0,0,0,0,0,3.11589813232422,0,0,0,0,0,0,3.58560943603516,0,0,0,0,0,0,0,0,3.82817077636719,0,0,0,0,0,0,0,0,3.38850402832031,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.38100433349609,0,0,0,0,0,0,0,3.07155609130859,0,0,0,0,0,0,0,0,3.39021301269531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.67676544189453,0,0,0,0,0,0,0,6.33122253417969,0,0,0,0,0,0,0,0,0,3.49966430664062,0,0,0,0,0,0,0,0,3.28751373291016,0,0,0,0,0,0,0,0,0,0,0,4.55181884765625,0,0,0,0,0,0,0,0,0,0,0,0,0,4.71976470947266,0,0,0,0,-41.5570068359375,0,0,0,0,0,0,0,-16.4894180297852,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4.73802185058594,0,0,0,0,0,0,0,0,2.64530944824219,0,0,0,0,0,0,0,0,3.79682922363281,3.58467864990234,0,0,0,0,0,0,0,0,3.51924896240234,0,0,0,0,0,0,0,0,4.91838836669922,0,0,0,0,0,0,0,0,2.63135528564453,0,0,0,0,0,0,0,0,0,0,4.32230377197266,0,0,0,0,0,0,0,0,0,0,0,0,0,3.00849151611328,0,0,0,0,0,0,0,0,0,0,0,3.25489044189453,0,0,0,0,0,0,0,3.49363708496094,0,0,0,0,0,0,0,0,0,0,0,0,2.97598266601562,0,0,0,0,0,0,0,3.75359344482422,0,0,0,0,0,0,0,0,0,0,5.41291046142578,0,0,0,0,0,0,0,0,0,0,0,0,0,3.97615814208984,0,0,0,0,0,0,0,0,0,0,0,3.26960754394531,0,0,0,0,0,0,0,0,0,0,0,0,-57.1479873657227,0,0,0,0,0,0,0,0,0,-0.307357788085938,0,0,0,0,0,0,0,0,0,0,0,0,0,6.7889404296875,0,0,0,0,0,0,0,0,0,0,0,0,0,4.91627502441406,0,0,0,0,0,0,0,4.9259033203125,0,0,0,0,0,0,0,0,0,6.571533203125,0,0,0,0,0,0,0,0,0,0,0,0,4.50265502929688,0,0,0,0,0,0,5.08724975585938,0,0,0,0,0,0,0,0,0,0,6.40288543701172,0,0,0,0,0,0,0,0,0,0,0,0,0,5.96781921386719,0,0,0,0,0,0,0,0,3.43965911865234,0,0,0,0,0,0,0,0,0,0,0,0,3.28938293457031,0,0,0,0,0,0,0,0,3.24069213867188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.86825561523438,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-60.8226928710938,0,0,0,0,0,0,0,2.95829010009766,0,0,0,0,0,0,0,0,0,3.51641845703125,0,0,0,0,0,0,0,3.86489105224609,0,0,0,0,0,0,0,0,0,3.52101135253906,5.76033782958984,0,0,0,0,0,0,0,3.74105834960938,0,0,0,0,0,0,0,3.30088043212891,0,0,0,0,0,0,0,0,0,0,0,0,0,3.19034576416016,0,0,0,0,3.55896759033203,0,0,0,0,0,0,0,0,4.63930511474609,0,0,0,0,0,0,0,4.09930419921875,0,0,0,0,0,0,0,0,3.44814300537109,0,0,0,0,0,0,0,0,3.37180328369141,0,0,0,0,0,0,0,0,0,0,0,3.02464294433594,0,0,0,0,0,0,0,3.60074615478516,0,0,0,0,0,0,0,0,2.91628265380859,0,0,0,0,0,0,0,0,0,0,0,0,0,-18.8861389160156,0,0,0,0,0,0,0,-39.1445999145508,0,0,0,0,0,0,0,0,0,0,0,0,0,3.5072021484375,0,0,0,0,0,0,0,0,0,0,0,0,0,5.78387451171875,0,0,0,0,0,0,0,0,0,0,0,3.78530883789062,0,0,0,0,0,0,0,0,0,0,2.58256530761719,0,0,0,0,0,0,3.46387481689453,0,0,0,0,0,0,0,0,0,0,3.28022766113281,0,0,0,0,0,0,5.32682037353516,0,0,0,0,0,0,0,0,3.28791046142578,0,0,0,0,0,0,0,0,5.26203918457031,0,0,0,0,0,0,0,0,3.74726867675781,0,0,0,0,0,0,0,3.82254028320312,0,0,0,0,0,0,0,0,0,0,0,3.18365478515625,0,0,0,0,0,0,0,0,0,0,0,0,3.21283721923828,0,0,0,0,0,0,0,0,0,3.49870300292969,0,0,0,0,0,0,0,0,3.60110473632812,0,0,0,0,0,0,0,0,0,0,2.98880004882812,0,0,0,0,0,0,0,0,0,0,0,0,-61.8671035766602,0,0,0,0,0,0,0,0,0,3.34664916992188,0,0,0,0,0,0,0,3.31131744384766,0,0,0,0,0,0,0,0,3.01993560791016,0,0,0,0,0,0,0,0,0,0,2.65328216552734,0,0,0,0,0,0,0,0,0,0,0,0,0,2.86328887939453,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.16084289550781,0,0,0,0,0,0,0,3.44249725341797,0,0,0,0,0,0,0,0,3.63095092773438,0,0,0,0,0,0,0,0,3.75381469726562,0,0,0,0,0,0,3.28301239013672,0,0,0,0,0,2.877197265625,0,0,0,0,0,0,3.27379608154297,0,0,0,0,0,0,2.46046447753906,0,0,0,0,0,1.70747375488281,0,0,0,0,0,0,0,0,0,0,2.30963134765625,0,0,0,0,0,0,0,1.31362152099609,0,0,0,0,0,0,0,0,0,0,0,0,1.23527526855469,0,0,0,0,0,0,0,0,1.59342193603516,0,0,0,0,0,0,0,0,0,2.085205078125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.51868438720703,0,0,2.17865753173828,0,0,0,0,0,0,0,0,0,0,1.40385437011719,0,0,0,0,0,0,0,2.28897094726562,0,0,0,0,0,0,0,0,0,0,0,2.05170440673828,0,0,0,0,0,0,0,0,0,1.10852813720703,0,0,0,0,0,0,0,0,0,0,0,-53.7858123779297,0,0,0,0,0,0,0,0,0,0,0,-6.73945617675781,0,0,0,0,0,0,1.83439636230469,0,0,0,0,0,0,0,0,0,0,0,0,0,1.85591888427734,0,0,0,0,0,0,0,2.29298400878906,0,0,0,0,0,0,0,1.23049163818359,0,0,0,0,0,0,0,2.44382476806641,0,0,0,0,0,0,0,2.08654022216797,0,0,0,0,0,0,0,1.60408782958984,0,0,0,0,0,0,0,0,2.47259521484375,0,0,0,0,0,0,0,0,2.45198822021484,0,0,0,0,0,0,0,0,0,0,2.46897125244141,0,0,0,0,0,0,0,0,0,0,0,4.50868225097656,0,0,0,0,0,0,2.87295532226562,0,0,0,0,0,3.68067169189453,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.31401062011719,0,0,0,0,0,0,0,3.47779083251953,0,0,0,0,0,0,0,0,0,0,0,3.83540344238281,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.19484710693359,0,0,0,0,0,0,0,3.14347839355469,0,0,0,0,0,0,0,2.95537567138672,0,0,4.28156280517578,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.02776336669922,0,0,0,0,0,0,0,0,0,0,0,-34.2296676635742,0,0,0,0,0,0,0,0,0,0,0,0,0,-23.4660568237305,0,0,0,0,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for (i in terms) {\n    glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv_error[[i]] <- cv.glm(Auto, glm_fit)$delta[[1]]\n  }\n})","normpath":"<expr>"}],"prof_output":"/Users/soltoffbc/Projects/Computing for Social Sciences/uc-cfss.github.io/.Rproj.user/38B1138C/profiles-cache/file13a59359325a4.Rprof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="fold-cv" class="section level4">
<h4>10-fold CV</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>
  
  for (i in terms) {
    glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
    cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit, <span class="dt">K =</span> <span class="dv">10</span>)$delta[[<span class="dv">1</span>]]
  }
})</code></pre></div>
<div id="htmlwidget-cfa19873ee74caef45a0" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-cfa19873ee74caef45a0">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27],"depth":[6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"label":["$<-","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","attr","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","length","dim.data.frame","dim","dim","ncol","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","$","predict.glm","predict","mean","cost","cv.glm","is.ordered","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","sample.int","sample0","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.double","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm"],"filenum":[null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1],"linenum":[null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9],"memalloc":[19.9932022094727,19.9932022094727,19.9932022094727,19.9932022094727,19.9932022094727,19.9932022094727,21.2762832641602,21.2762832641602,21.2762832641602,21.2762832641602,21.2762832641602,21.2762832641602,21.2762832641602,21.2762832641602,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,22.3658905029297,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,23.9692001342773,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,25.4250564575195,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,26.7613143920898,28.3602752685547,28.3602752685547,28.3602752685547,28.3602752685547,28.3602752685547,28.3602752685547,28.3602752685547,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,30.4131622314453,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,31.7802581787109,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,33.8904418945312,36.1036605834961,36.1036605834961,36.1036605834961,36.1036605834961,36.1036605834961,36.1036605834961,36.1036605834961,36.1036605834961,37.3291091918945,37.3291091918945,37.3291091918945,37.3291091918945,37.3291091918945,37.3291091918945,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,39.2933197021484,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,41.1313247680664,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,42.4606628417969,44.6987533569336,44.6987533569336,44.6987533569336,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,46.7585601806641,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,47.9969635009766,50.4370651245117,50.4370651245117,50.4370651245117,50.4370651245117,50.4370651245117,50.4370651245117,50.4370651245117,50.4370651245117,52.8445892333984,52.8445892333984,52.8445892333984,52.8445892333984,52.8445892333984,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,54.239387512207,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,57.6217803955078,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,59.8208236694336,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,61.6182250976562,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,63.6129989624023,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,65.9293823242188,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859,23.8298187255859],"meminc":[0,0,0,0,0,0,1.2830810546875,0,0,0,0,0,0,0,1.08960723876953,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.60330963134766,0,0,0,0,0,0,0,0,1.45585632324219,0,0,0,0,0,0,0,0,0,0,0,1.33625793457031,0,0,0,0,0,0,0,0,0,0,0,0,1.59896087646484,0,0,0,0,0,0,2.05288696289062,0,0,0,0,0,0,0,0,0,0,0,0,1.36709594726562,0,0,0,0,0,0,0,0,0,0,0,0,0,2.11018371582031,0,0,0,0,0,0,0,0,0,2.21321868896484,0,0,0,0,0,0,0,1.22544860839844,0,0,0,0,0,1.96421051025391,0,0,0,0,0,0,0,0,0,0,1.83800506591797,0,0,0,0,0,0,0,0,1.32933807373047,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.23809051513672,0,0,2.05980682373047,0,0,0,0,0,0,0,0,0,0,0,1.2384033203125,0,0,0,0,0,0,0,0,0,0,0,0,2.44010162353516,0,0,0,0,0,0,0,2.40752410888672,0,0,0,0,1.39479827880859,0,0,0,0,0,0,0,0,0,3.38239288330078,0,0,0,0,0,0,0,0,0,0,0,0,2.19904327392578,0,0,0,0,0,0,0,0,0,0,0,0,0,1.79740142822266,0,0,0,0,0,0,0,0,0,0,0,0,1.99477386474609,0,0,0,0,0,0,0,0,2.31638336181641,0,0,0,0,0,0,0,0,0,0,0,0,-42.0995635986328,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"filename":[null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error_fold10 <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for (i in terms) {\n    glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv_error_fold10[[i]] <- cv.glm(Auto, glm_fit, K = 10)$delta[[1]]\n  }\n})","normpath":"<expr>"}],"prof_output":"/Users/soltoffbc/Projects/Computing for Social Sciences/uc-cfss.github.io/.Rproj.user/38B1138C/profiles-cache/file13a594590546a.Rprof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>On my machine, 10-fold CV was about 40 times faster than LOOCV. Again, estimating <span class="math inline">\(k=10\)</span> models is going to be much easier than estimating <span class="math inline">\(k=392\)</span> models.</p>
</div>
</div>
<div id="k-fold-cv-in-logistic-regression" class="section level3">
<h3>k-fold CV in logistic regression</h3>
<p>You’ve gotten the idea by now, but let’s do it one more time on our interactive Titanic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_kfold &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age), !<span class="kw">is.na</span>(Sex)) %&gt;%
<span class="st">  </span><span class="kw">cv.glm</span>(titanic_model, <span class="dt">K =</span> <span class="dv">10</span>)
titanic_kfold$delta[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.1708052</code></pre>
<p>Not a large difference from the LOOCV approach, but it take much less time to compute.</p>
</div>
</div>
</div>
<div id="decision-trees" class="section level1">
<h1>Decision trees</h1>
<div class="figure">
<img src="https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700" />

</div>
<div class="figure">
<img src="https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg" />

</div>
<div class="figure">
<img src="http://data.iwastesomuchtime.com/November-26-2012-17-34-05-cookie.gif" alt="Should I Have a Cookie?" />
<p class="caption"><a href="http://iwastesomuchtime.com/58217">Should I Have a Cookie?</a></p>
</div>
<p><em>Decision trees</em> are intuitive concepts for making decisions. They are also useful methods for regression and classification. They work by splitting the observations into a number of regions, and predictions are made based on the mean or mode of the training observations in that region.</p>
<div id="interpreting-a-decision-tree" class="section level2">
<h2>Interpreting a decision tree</h2>
<p>Let’s start with the Titanic data. I want to predict who lives and who dies during this event. Instead of using logistic regression, I’m going to calculate a decision tree based on a passenger’s age and gender. Here’s what that decision tree looks like:</p>
<p><img src="stat_learning02_files/figure-html/titanic_tree-1.png" width="672" /></p>
<p>Some key terminology:</p>
<ul>
<li>Each outcome (survived or died) is a <em>terminal node</em> or a <em>leaf</em></li>
<li>Splits occur at <em>internal nodes</em></li>
<li>The segments connecting each node are called <em>branches</em></li>
</ul>
<p>To make a prediction for a specific passenger, we start the decision tree from the top node and follow the appropriate branches down until we reach a terminal node. At each internal node, if our observation matches the condition, then travel down the left branch. If our observation does not match the condition, then travel down the right branch.</p>
<p>So for a 50 year old female passenger:</p>
<ul>
<li>Start at the first internal node. The passenger in question is a female, so take the branch to the left.</li>
<li>We reach a terminal node (“Survived”). We would predict the passenger in question survived the sinking of the Titanic.</li>
</ul>
<p>For a 20 year old male passenger:</p>
<ul>
<li>Start at the first internal node - the passenger in question is a male, so take the branch to the right.</li>
<li>The passenger in question is not less than 13 years old (R would say the condition is <code>FALSE</code>), so take the branch to the right.</li>
<li>We reach a terminal node (“Died”). We would predict the passenger in question died in the sinking of the Titanic.</li>
</ul>
</div>
<div id="estimating-a-decision-tree" class="section level2">
<h2>Estimating a decision tree</h2>
<p>First we need to load the <code>tree</code> library and prepare the data. <code>tree</code> is somewhat finicky about how data must be formatted in order to estimate the tree. For the Titanic data, we need to convert all qualitiative variables to <a href="http://r4ds.had.co.nz/factors.html">factors</a> using the <code>as.factor</code> function. To make interpretation easier, I also recode <code>Survived</code> from its <code>0/1</code> coding to explicitly identify which passengers survived and which died.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tree)

titanic_tree_data &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Survived =</span> <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;Survived&quot;</span>,
                           <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;Died&quot;</span>, <span class="ot">NA</span>)),
         <span class="dt">Survived =</span> <span class="kw">as.factor</span>(Survived),
         <span class="dt">Sex =</span> <span class="kw">as.factor</span>(Sex))
titanic_tree_data</code></pre></div>
<pre><code>## # A tibble: 891 × 12
##    PassengerId Survived Pclass
##          &lt;int&gt;   &lt;fctr&gt;  &lt;int&gt;
## 1            1     Died      3
## 2            2 Survived      1
## 3            3 Survived      3
## 4            4 Survived      1
## 5            5     Died      3
## 6            6     Died      3
## 7            7     Died      1
## 8            8     Died      3
## 9            9 Survived      3
## 10          10 Survived      2
## # ... with 881 more rows, and 9 more variables: Name &lt;chr&gt;, Sex &lt;fctr&gt;,
## #   Age &lt;dbl&gt;, SibSp &lt;int&gt;, Parch &lt;int&gt;, Ticket &lt;chr&gt;, Fare &lt;dbl&gt;,
## #   Cabin &lt;chr&gt;, Embarked &lt;chr&gt;</code></pre>
<p>Now we can use the <code>tree</code> function to estimate the model. The format looks exactly like <code>lm</code> or <code>glm</code> - first we specify the formula that defines the model, then we specify where the data is stored:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_tree_data)
<span class="kw">summary</span>(titanic_tree)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Age + Sex, data = titanic_tree_data)
## Number of terminal nodes:  3 
## Residual mean deviance:  1.019 = 724.7 / 711 
## Misclassification error rate: 0.2129 = 152 / 714</code></pre>
<p>The <code>summary</code> function provides several important statistics:</p>
<ul>
<li>There are three terminal nodes in the tree</li>
<li><em>Residual mean deviance</em> is an estimate of model fit. It is usually helpful in comparing the effectiveness of different models.</li>
<li>This decision tree misclassifies <span class="math inline">\(21.3\%\)</span> of the training set observations (note that we did not create a validation set - this model is based on all the original data)</li>
</ul>
<p>That’s all well in good, but decision trees are meant to be viewed. Let’s plot it!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree)
<span class="kw">text</span>(titanic_tree, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_plot-1.png" width="672" /></p>
<p><code>tree</code> does not use <code>ggplot2</code> to graph the results; instead it relies on the base <code>graphics</code> package. <code>plot(titanic_tree)</code> draws the branches and <code>text(titanic_tree, pretty = 0)</code> adds the text labeling each node.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<div id="build-a-more-complex-tree" class="section level3">
<h3>Build a more complex tree</h3>
<p>Since we have a lot of other variables in our Titanic data set, let’s estimate a more complex model that accounts for all the information we have.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> We’ll have to format all our columns this time before we can estimate the model. Because there are multiple qualitative variables as predictors, I will use <code>mutate_each</code> to apply <code>as.factor</code> to all of these variables in one line of code (another type of iterative operation):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree_full_data &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Survived =</span> <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;Survived&quot;</span>,
                           <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;Died&quot;</span>, <span class="ot">NA</span>))) %&gt;%
<span class="st">  </span><span class="kw">mutate_each</span>(<span class="kw">funs</span>(as.factor), Survived, Pclass, Sex, Embarked)

titanic_tree_full &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                       </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked, <span class="dt">data =</span> titanic_tree_full_data)
<span class="kw">summary</span>(titanic_tree_full)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked, data = titanic_tree_full_data)
## Variables actually used in tree construction:
## [1] &quot;Sex&quot;    &quot;Pclass&quot; &quot;Fare&quot;   &quot;Age&quot;    &quot;SibSp&quot; 
## Number of terminal nodes:  7 
## Residual mean deviance:  0.7995 = 563.6 / 705 
## Misclassification error rate: 0.1742 = 124 / 712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree_full)
<span class="kw">text</span>(titanic_tree_full, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_full-1.png" width="672" /></p>
<p>Now we’ve built a more complicated decision tree. Fortunately it is still pretty interpretable. Notice that some of the variables we included in the model (<code>Parch</code> and <code>Embarked</code>) ended up being dropped from the final model. This is because to build the tree and ensure it is not overly complicated, the algorithm goes through a process of iteration and <em>pruning</em> to remove twigs or branches that result in a complicated model that does not provide significant improvement in overall model accuracy. You can tweak these parameters to ensure the model keeps all the variables, but could result in a nasty looking picture:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree_messy &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                       </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked,
                       <span class="dt">data =</span> titanic_tree_full_data,
                       <span class="dt">control =</span> <span class="kw">tree.control</span>(<span class="dt">nobs =</span> <span class="kw">nrow</span>(titanic_tree_full_data),
                                              <span class="dt">mindev =</span> <span class="dv">0</span>, <span class="dt">minsize =</span> <span class="dv">10</span>))
<span class="kw">summary</span>(titanic_tree_messy)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked, data = titanic_tree_full_data, control = tree.control(nobs = nrow(titanic_tree_full_data), 
##     mindev = 0, minsize = 10))
## Number of terminal nodes:  76 
## Residual mean deviance:  0.5164 = 328.4 / 636 
## Misclassification error rate: 0.1124 = 80 / 712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree_messy)
<span class="kw">text</span>(titanic_tree_messy, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_complicated-1.png" width="672" /></p>
<p>The misclassification error rate for this model is much lower than the previous versions, but it is also much less interpretable. Depending on your audience and how you want to present the results of your statistical model, you need to determine the optimal trade-off between accuracy and interpretability.</p>
</div>
</div>
<div id="benefitsdrawbacks-to-decision-trees" class="section level2">
<h2>Benefits/drawbacks to decision trees</h2>
<p>Decision trees are an entirely different method of estimating functional forms as compared to linear regression. There are some benefits to trees:</p>
<ul>
<li>They are easy to explain. Most people, even if they lack statistical training, can understand decision trees.</li>
<li>They are easily presented as visualizations, and pretty interpretable.</li>
<li>Qualitative predictors are easily handled without the need to create a long series of dummy variables.</li>
</ul>
<p>However there are also drawbacks to trees:</p>
<ul>
<li>Their accuracy rates are generally lower than other regression and classification approaches.</li>
<li>Trees can be non-robust. That is, a small change in the data or inclusion/exclusion of a handful of observations can dramatically alter the final estimated tree.</li>
</ul>
<p>Fortuntately, there is an easy way to improve on these poor predictions: by aggregating many decision trees and averaging across them, we can substantially improve performance.</p>
</div>
<div id="random-forests" class="section level2">
<h2>Random forests</h2>
<p>One method of aggregating trees is the <em>random forest</em> approach. This uses the concept of <em>bootstrapping</em> build a forest of trees using the same underlying data set. Bootstrapping is standard resampling process whereby you repeatedly <em>sample with replacement</em> from a data set. So if you have a dataset of 500 observations, you might draw a sample of 500 observations from the data. But by sampling with replacement, some observations may be sampled multiple times and some observations may never be sampled. This essentially treats your data as a population of interest. You repeat this process many times (say 1000), then estimate your quantity or model of interest on each sample. Then finally you average across all the bootstrapped samples to calculate the final model or statistical estimator.</p>
<p>As with other resampling methods, each individual sample will have some degree of bias to it. However by averaging across all the bootstrapped samples you cancel out much of this bias. Most importantly, averaging a set of observations reduces <em>variance</em> - this is what LOOCV and <span class="math inline">\(k\)</span>-fold cross-validation do. You achieve stable estimates of the prediction accuracy or overall model error.</p>
<p>In the context of decision trees, this means we draw repeated samples from the original dataset and estimate a decision tree model on each sample. To make predictions, we estimate the outcome using each tree and average across all of them to obtain the final prediction. Rather than being a binary outcome (<span class="math inline">\([0,1]\)</span>, survived/died), the average prediction will be a probability of the given outcome (i.e. the probability of survival). This process is called <em>bagging</em>. Random forests go a step further: when building individual decision trees, each time a split in the tree is considered a random sample of predictors is selected as the candidates for the split. <em>Random forests specifically exclude a portion of the predictor variables when building individual trees</em>. Why throw away good data? This ensures each decision tree is not correlated with one another. If one specific variable was a strong predictor in the data set (say gender in the Titanic data set), it could potentially dominate every decision tree and the result would be nearly-identical trees regardless of the sampling procedure. By forcibly excluding a random subset of variables, individual trees in random forests will not have strong correlations with one another. Therefore the average predictions will be more <em>reliable</em>.</p>
</div>
</div>
<div id="estimating-statistical-models-using-caret" class="section level1">
<h1>Estimating statistical models using <code>caret</code></h1>
<p>To estimate a random forest, we move outside the world of <code>tree</code> and into a new package in R: <a href="https://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>. <code>caret</code> is a package in R for training and plotting a wide variety of statistical learning models. It is outside of the <code>tidyverse</code> so can be a bit more difficult to master. <code>caret</code> does not contain the estimation algorithms itself; instead it creates a unified interface to approximately <a href="https://topepo.github.io/caret/available-models.html">233 different models</a> from various packages in R. To install <code>caret</code> and make sure you install all the related packages it relies on, run the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;caret&quot;</span>, <span class="dt">dependencies =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>The basic function to train models is <code>train</code>. We can train regression and classification models using one of <a href="https://topepo.github.io/caret/available-models.html">these models</a>. For instance, rather than using <code>glm</code> to estimate a logistic regression model, we could use <code>caret</code> and the <code>&quot;glm&quot;</code> method instead. Note that <code>caret</code> is extremely picky about preparing data for analysis. For instance, we have to remove all missing values before training a model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

titanic_clean &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age))

caret_glm &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Age, <span class="dt">data =</span> titanic_clean,
                   <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                   <span class="dt">family =</span> binomial,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;none&quot;</span>))
<span class="kw">summary</span>(caret_glm)</code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<ul>
<li><code>trControl = trainControl(method = &quot;none&quot;)</code> - by default <code>caret</code> implements a bootstrap resampling procedure to validate the results of the model. For our purposes here I want to turn that off by setting the resampling method to <code>&quot;none&quot;</code>.</li>
</ul>
<p>The results are identical to those obtained by the <code>glm</code> function:<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age, <span class="dt">data =</span> titanic_clean, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(glm_glm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age, family = &quot;binomial&quot;, data = titanic_clean)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div id="estimating-a-random-forest" class="section level2">
<h2>Estimating a random forest</h2>
<p>We will reuse <code>titanic_tree_full_data</code> with the adjustment that we need to remove observations with missing values. In the process, let’s pare the data frame down to only columns that will be used the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_rf_data &lt;-<span class="st"> </span>titanic_tree_full_data %&gt;%
<span class="st">  </span><span class="kw">select</span>(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %&gt;%
<span class="st">  </span><span class="kw">na.omit</span>()
titanic_rf_data</code></pre></div>
<pre><code>## # A tibble: 712 × 8
##    Survived Pclass    Sex   Age SibSp Parch    Fare Embarked
##      &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;fctr&gt;
## 1      Died      3   male    22     1     0  7.2500        S
## 2  Survived      1 female    38     1     0 71.2833        C
## 3  Survived      3 female    26     0     0  7.9250        S
## 4  Survived      1 female    35     1     0 53.1000        S
## 5      Died      3   male    35     0     0  8.0500        S
## 6      Died      1   male    54     0     0 51.8625        S
## 7      Died      3   male     2     3     1 21.0750        S
## 8  Survived      3 female    27     0     2 11.1333        S
## 9  Survived      2 female    14     1     0 30.0708        C
## 10 Survived      3 female     4     1     1 16.7000        S
## # ... with 702 more rows</code></pre>
<p>Now that the data is prepped, let’s estimate the model. To start, we’ll estimate a simple model that only uses age and gender. Again we use the <code>train</code> function but this time we will use the <code>rf</code> method.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> To start with, I will estimate a forest with 200 trees (the default is 500 trees) and set the <code>trainControl</code> method to <code>&quot;oob&quot;</code> (I will explain this shortly):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_rf_data,
                   <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                   <span class="dt">ntree =</span> <span class="dv">200</span>,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))</code></pre></div>
<pre><code>## note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf</code></pre></div>
<pre><code>## Random Forest 
## 
## 712 samples
##   2 predictor
##   2 classes: &#39;Died&#39;, &#39;Survived&#39; 
## 
## No pre-processing
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7598315  0.4938641
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 2
## </code></pre>
<p>Hmm. What have we generated here? How can we analyze the results?</p>
<div id="structure-of-train-object" class="section level3">
<h3>Structure of <code>train</code> object</h3>
<p>The object generated by <code>train</code> is a named list:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(age_sex_rf, <span class="dt">max.level =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## List of 24
##  $ method      : chr &quot;rf&quot;
##  $ modelInfo   :List of 15
##  $ modelType   : chr &quot;Classification&quot;
##  $ results     :&#39;data.frame&#39;:    1 obs. of  3 variables:
##  $ pred        : NULL
##  $ bestTune    :&#39;data.frame&#39;:    1 obs. of  1 variable:
##  $ call        : language train.formula(form = Survived ~ Age + Sex, data = titanic_rf_data,      method = &quot;rf&quot;, ntree = 200, trControl = trainControl(method = &quot;oob&quot;))
##  $ dots        :List of 1
##  $ metric      : chr &quot;Accuracy&quot;
##  $ control     :List of 26
##  $ finalModel  :List of 22
##   ..- attr(*, &quot;class&quot;)= chr &quot;randomForest&quot;
##  $ preProcess  : NULL
##  $ trainingData:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    712 obs. of  3 variables:
##  $ resample    : NULL
##  $ resampledCM : NULL
##  $ perfNames   : chr [1:2] &quot;Accuracy&quot; &quot;Kappa&quot;
##  $ maximize    : logi TRUE
##  $ yLimits     : NULL
##  $ times       :List of 3
##  $ levels      : atomic [1:2] Died Survived
##   ..- attr(*, &quot;ordered&quot;)= logi FALSE
##  $ terms       :Classes &#39;terms&#39;, &#39;formula&#39;  language Survived ~ Age + Sex
##   .. ..- attr(*, &quot;variables&quot;)= language list(Survived, Age, Sex)
##   .. ..- attr(*, &quot;factors&quot;)= int [1:3, 1:2] 0 1 0 0 0 1
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..- attr(*, &quot;term.labels&quot;)= chr [1:2] &quot;Age&quot; &quot;Sex&quot;
##   .. ..- attr(*, &quot;order&quot;)= int [1:2] 1 1
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(Survived, Age, Sex)
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:3] &quot;factor&quot; &quot;numeric&quot; &quot;factor&quot;
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Survived&quot; &quot;Age&quot; &quot;Sex&quot;
##  $ coefnames   : chr [1:2] &quot;Age&quot; &quot;Sexmale&quot;
##  $ contrasts   :List of 1
##  $ xlevels     :List of 1
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;train&quot; &quot;train.formula&quot;</code></pre>
<p>The model itself is always stored in the <code>finalModel</code> element. So to use the model in other functions, we would refer to it as <code>age_sex_rf$finalModel</code>.</p>
</div>
<div id="model-statistics" class="section level3">
<h3>Model statistics</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf$finalModel</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 200, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 200
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 24.3%
## Confusion matrix:
##          Died Survived class.error
## Died      352       72   0.1698113
## Survived  101      187   0.3506944</code></pre>
<p>This tells us some important things:</p>
<ul>
<li>We used 200 trees</li>
<li>At every potential branch, the model randomly used one of 2 variables to define the split</li>
<li><p>The <em>out-of-bag</em> (OOB) error rate</p>
<p>This requires further explanation. Because each tree is built from a bootstrapped sample, for any given tree approximately one-third of the observations are not used to build the tree. In essence, we have a natural validation set for each tree. For each observation, we predict the outcome of interest using all trees where the observation was not used to build the tree, then average across these predictions. For any observation, we should have <span class="math inline">\(K/3\)</span> validation predictions where <span class="math inline">\(K\)</span> is the total number of trees in the forest. Averaging across these predictions gives us an out-of-bag error rate for every observation (even if they are derived from different combinations of trees). Because the OOB estimate is built only using trees that were not fit to the observation, this is a valid estimate of the test error for the random forest.</p>
Here we get an OOB estimate of the error rate of 24%. This means for test observations, the model misclassifies the individual’s survival 24% of the time.</li>
<li><p>The <em>confusion matrix</em> - this compares the predictions to the actual known outcomes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr::<span class="kw">kable</span>(age_sex_rf$finalModel$confusion)</code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Died</th>
<th align="right">Survived</th>
<th align="right">class.error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Died</td>
<td align="right">352</td>
<td align="right">72</td>
<td align="right">0.1698113</td>
</tr>
<tr class="even">
<td>Survived</td>
<td align="right">101</td>
<td align="right">187</td>
<td align="right">0.3506944</td>
</tr>
</tbody>
</table>
<p>The rows indicate the actual known outcomes, and the columns indicate the predictions. A perfect model would have 0s on the off-diagonal cells because every prediction is perfect. Clearly that is not the case. Not only is there substantial error, most it comes from misclassifying survivors. The error rate for those who actually died is much smaller than for those who actually survived.</p></li>
</ul>
</div>
<div id="look-at-an-individual-tree" class="section level3">
<h3>Look at an individual tree</h3>
<p>We could look at one tree generated by the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">randomForest::<span class="kw">getTree</span>(age_sex_rf$finalModel, <span class="dt">labelVar =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##     left daughter right daughter split var split point status prediction
## 1               2              3   Sexmale        0.50      1       &lt;NA&gt;
## 2               4              5       Age       21.50      1       &lt;NA&gt;
## 3               6              7       Age        6.50      1       &lt;NA&gt;
## 4               8              9       Age       19.50      1       &lt;NA&gt;
## 5              10             11       Age       37.00      1       &lt;NA&gt;
## 6              12             13       Age        0.96      1       &lt;NA&gt;
## 7              14             15       Age       23.75      1       &lt;NA&gt;
## 8              16             17       Age        1.50      1       &lt;NA&gt;
## 9              18             19       Age       20.50      1       &lt;NA&gt;
## 10             20             21       Age       32.25      1       &lt;NA&gt;
## 11             22             23       Age       39.50      1       &lt;NA&gt;
## 12              0              0      &lt;NA&gt;        0.00     -1   Survived
## 13             24             25       Age        5.00      1       &lt;NA&gt;
## 14             26             27       Age       13.00      1       &lt;NA&gt;
## 15             28             29       Age       51.50      1       &lt;NA&gt;
## 16              0              0      &lt;NA&gt;        0.00     -1   Survived
## 17             30             31       Age        2.50      1       &lt;NA&gt;
## 18              0              0      &lt;NA&gt;        0.00     -1       Died
## 19              0              0      &lt;NA&gt;        0.00     -1       Died
## 20             32             33       Age       30.25      1       &lt;NA&gt;
## 21              0              0      &lt;NA&gt;        0.00     -1   Survived
## 22              0              0      &lt;NA&gt;        0.00     -1       Died
## 23             34             35       Age       56.50      1       &lt;NA&gt;
## 24             36             37       Age        3.50      1       &lt;NA&gt;
## 25              0              0      &lt;NA&gt;        0.00     -1   Survived
## 26             38             39       Age       10.00      1       &lt;NA&gt;
## 27             40             41       Age       22.50      1       &lt;NA&gt;
## 28             42             43       Age       44.50      1       &lt;NA&gt;
## 29             44             45       Age       77.00      1       &lt;NA&gt;
## 30              0              0      &lt;NA&gt;        0.00     -1       Died
## 31             46             47       Age        5.50      1       &lt;NA&gt;
## 32             48             49       Age       24.50      1       &lt;NA&gt;
## 33             50             51       Age       30.75      1       &lt;NA&gt;
## 34             52             53       Age       50.50      1       &lt;NA&gt;
## 35             54             55       Age       57.50      1       &lt;NA&gt;
## 36             56             57       Age        2.50      1       &lt;NA&gt;
## 37              0              0      &lt;NA&gt;        0.00     -1       Died
## 38             58             59       Age        8.50      1       &lt;NA&gt;
## 39             60             61       Age       11.50      1       &lt;NA&gt;
## 40             62             63       Age       16.50      1       &lt;NA&gt;
## 41              0              0      &lt;NA&gt;        0.00     -1       Died
## 42             64             65       Age       39.50      1       &lt;NA&gt;
## 43             66             67       Age       45.25      1       &lt;NA&gt;
## 44             68             69       Age       63.00      1       &lt;NA&gt;
## 45              0              0      &lt;NA&gt;        0.00     -1   Survived
## 46             70             71       Age        3.50      1       &lt;NA&gt;
## 47             72             73       Age       12.50      1       &lt;NA&gt;
## 48             74             75       Age       22.50      1       &lt;NA&gt;
## 49             76             77       Age       25.50      1       &lt;NA&gt;
## 50              0              0      &lt;NA&gt;        0.00     -1       Died
## 51             78             79       Age       31.50      1       &lt;NA&gt;
## 52             80             81       Age       43.50      1       &lt;NA&gt;
## 53              0              0      &lt;NA&gt;        0.00     -1   Survived
## 54              0              0      &lt;NA&gt;        0.00     -1       Died
## 55              0              0      &lt;NA&gt;        0.00     -1   Survived
## 56             82             83       Age        1.50      1       &lt;NA&gt;
## 57              0              0      &lt;NA&gt;        0.00     -1   Survived
## 58             84             85       Age        7.50      1       &lt;NA&gt;
## 59              0              0      &lt;NA&gt;        0.00     -1       Died
## 60              0              0      &lt;NA&gt;        0.00     -1   Survived
## 61              0              0      &lt;NA&gt;        0.00     -1   Survived
## 62              0              0      &lt;NA&gt;        0.00     -1       Died
## 63             86             87       Age       17.50      1       &lt;NA&gt;
## 64             88             89       Age       33.50      1       &lt;NA&gt;
## 65             90             91       Age       43.50      1       &lt;NA&gt;
## 66              0              0      &lt;NA&gt;        0.00     -1   Survived
## 67             92             93       Age       47.50      1       &lt;NA&gt;
## 68             94             95       Age       59.50      1       &lt;NA&gt;
## 69              0              0      &lt;NA&gt;        0.00     -1       Died
## 70              0              0      &lt;NA&gt;        0.00     -1   Survived
## 71              0              0      &lt;NA&gt;        0.00     -1   Survived
## 72             96             97       Age        7.50      1       &lt;NA&gt;
## 73             98             99       Age       14.25      1       &lt;NA&gt;
## 74              0              0      &lt;NA&gt;        0.00     -1   Survived
## 75            100            101       Age       23.50      1       &lt;NA&gt;
## 76              0              0      &lt;NA&gt;        0.00     -1       Died
## 77            102            103       Age       26.50      1       &lt;NA&gt;
## 78              0              0      &lt;NA&gt;        0.00     -1   Survived
## 79              0              0      &lt;NA&gt;        0.00     -1   Survived
## 80            104            105       Age       41.00      1       &lt;NA&gt;
## 81            106            107       Age       46.50      1       &lt;NA&gt;
## 82              0              0      &lt;NA&gt;        0.00     -1   Survived
## 83              0              0      &lt;NA&gt;        0.00     -1       Died
## 84              0              0      &lt;NA&gt;        0.00     -1       Died
## 85              0              0      &lt;NA&gt;        0.00     -1   Survived
## 86              0              0      &lt;NA&gt;        0.00     -1       Died
## 87            108            109       Age       18.50      1       &lt;NA&gt;
## 88            110            111       Age       32.50      1       &lt;NA&gt;
## 89            112            113       Age       35.50      1       &lt;NA&gt;
## 90            114            115       Age       40.25      1       &lt;NA&gt;
## 91              0              0      &lt;NA&gt;        0.00     -1       Died
## 92              0              0      &lt;NA&gt;        0.00     -1       Died
## 93            116            117       Age       49.50      1       &lt;NA&gt;
## 94            118            119       Age       53.00      1       &lt;NA&gt;
## 95            120            121       Age       60.50      1       &lt;NA&gt;
## 96            122            123       Age        6.50      1       &lt;NA&gt;
## 97              0              0      &lt;NA&gt;        0.00     -1       Died
## 98              0              0      &lt;NA&gt;        0.00     -1   Survived
## 99            124            125       Age       14.75      1       &lt;NA&gt;
## 100             0              0      &lt;NA&gt;        0.00     -1   Survived
## 101             0              0      &lt;NA&gt;        0.00     -1   Survived
## 102             0              0      &lt;NA&gt;        0.00     -1   Survived
## 103           126            127       Age       27.50      1       &lt;NA&gt;
## 104             0              0      &lt;NA&gt;        0.00     -1   Survived
## 105             0              0      &lt;NA&gt;        0.00     -1   Survived
## 106           128            129       Age       44.50      1       &lt;NA&gt;
## 107           130            131       Age       49.50      1       &lt;NA&gt;
## 108             0              0      &lt;NA&gt;        0.00     -1       Died
## 109           132            133       Age       19.50      1       &lt;NA&gt;
## 110           134            135       Age       31.50      1       &lt;NA&gt;
## 111             0              0      &lt;NA&gt;        0.00     -1       Died
## 112           136            137       Age       34.50      1       &lt;NA&gt;
## 113           138            139       Age       36.50      1       &lt;NA&gt;
## 114             0              0      &lt;NA&gt;        0.00     -1       Died
## 115           140            141       Age       41.50      1       &lt;NA&gt;
## 116           142            143       Age       48.50      1       &lt;NA&gt;
## 117           144            145       Age       50.50      1       &lt;NA&gt;
## 118             0              0      &lt;NA&gt;        0.00     -1       Died
## 119             0              0      &lt;NA&gt;        0.00     -1       Died
## 120             0              0      &lt;NA&gt;        0.00     -1       Died
## 121           146            147       Age       61.50      1       &lt;NA&gt;
## 122             0              0      &lt;NA&gt;        0.00     -1       Died
## 123             0              0      &lt;NA&gt;        0.00     -1   Survived
## 124             0              0      &lt;NA&gt;        0.00     -1       Died
## 125           148            149       Age       15.50      1       &lt;NA&gt;
## 126             0              0      &lt;NA&gt;        0.00     -1   Survived
## 127           150            151       Age       29.50      1       &lt;NA&gt;
## 128             0              0      &lt;NA&gt;        0.00     -1   Survived
## 129             0              0      &lt;NA&gt;        0.00     -1   Survived
## 130           152            153       Age       48.50      1       &lt;NA&gt;
## 131             0              0      &lt;NA&gt;        0.00     -1   Survived
## 132             0              0      &lt;NA&gt;        0.00     -1       Died
## 133           154            155       Age       20.75      1       &lt;NA&gt;
## 134           156            157       Age       29.50      1       &lt;NA&gt;
## 135             0              0      &lt;NA&gt;        0.00     -1       Died
## 136             0              0      &lt;NA&gt;        0.00     -1       Died
## 137             0              0      &lt;NA&gt;        0.00     -1       Died
## 138             0              0      &lt;NA&gt;        0.00     -1       Died
## 139           158            159       Age       37.50      1       &lt;NA&gt;
## 140             0              0      &lt;NA&gt;        0.00     -1       Died
## 141           160            161       Age       42.50      1       &lt;NA&gt;
## 142             0              0      &lt;NA&gt;        0.00     -1       Died
## 143             0              0      &lt;NA&gt;        0.00     -1   Survived
## 144             0              0      &lt;NA&gt;        0.00     -1       Died
## 145             0              0      &lt;NA&gt;        0.00     -1       Died
## 146             0              0      &lt;NA&gt;        0.00     -1       Died
## 147             0              0      &lt;NA&gt;        0.00     -1       Died
## 148             0              0      &lt;NA&gt;        0.00     -1   Survived
## 149           162            163       Age       18.50      1       &lt;NA&gt;
## 150           164            165       Age       28.50      1       &lt;NA&gt;
## 151             0              0      &lt;NA&gt;        0.00     -1   Survived
## 152             0              0      &lt;NA&gt;        0.00     -1   Survived
## 153             0              0      &lt;NA&gt;        0.00     -1   Survived
## 154           166            167       Age       20.25      1       &lt;NA&gt;
## 155           168            169       Age       21.50      1       &lt;NA&gt;
## 156           170            171       Age       26.50      1       &lt;NA&gt;
## 157           172            173       Age       30.75      1       &lt;NA&gt;
## 158             0              0      &lt;NA&gt;        0.00     -1   Survived
## 159           174            175       Age       38.50      1       &lt;NA&gt;
## 160             0              0      &lt;NA&gt;        0.00     -1       Died
## 161             0              0      &lt;NA&gt;        0.00     -1       Died
## 162           176            177       Age       17.50      1       &lt;NA&gt;
## 163             0              0      &lt;NA&gt;        0.00     -1   Survived
## 164             0              0      &lt;NA&gt;        0.00     -1   Survived
## 165             0              0      &lt;NA&gt;        0.00     -1   Survived
## 166             0              0      &lt;NA&gt;        0.00     -1       Died
##  [ reached getOption(&quot;max.print&quot;) -- omitted 25 rows ]</code></pre>
<p>Unfortunately there is no easy plotting mechanism for the result of <code>getTree</code>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<p>Yikes. Clearly this tree is pretty complicated. Not something we want to examine directly.</p>
</div>
<div id="variable-importance" class="section level3">
<h3>Variable importance</h3>
<p>Another method of interpreting random forests looks at the importance of individual variables in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(age_sex_rf$finalModel)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/rf_import-1.png" width="672" /></p>
<p>This tells us how much each variable decreases the average <em>Gini index</em>, a measure of how important the variable is to the model. Essentially, it estimates the impact a variable has on the model by comparing prediction accuracy rates for models with and without the variable. Larger values indicate higher importance of the variable. Here we see that the gender variable <code>Sexmale</code> is most important.</p>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>We can also use random forests to make predictions on an explicit validation set, rather than relying on OOB estimates. Let’s split our Titanic data into training and test sets, train the full random forest model on the training set, then use that model to predict outcomes in the test set. Instead of using a bootstrapped resampling method, again let’s use <code>&quot;oob&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic_rf_data,
                                    <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
titanic_train &lt;-<span class="st"> </span>titanic_split$train %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
titanic_test &lt;-<span class="st"> </span>titanic_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()

rf_full &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                   </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked,
                 <span class="dt">data =</span> titanic_train,
                 <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                 <span class="dt">ntree =</span> <span class="dv">500</span>,
                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))
rf_full$finalModel</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 500, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 19.04%
## Confusion matrix:
##          Died Survived class.error
## Died      270       28  0.09395973
## Survived   67      134  0.33333333</code></pre>
<ul>
<li>We used 500 trees</li>
<li>At every potential branch, the model randomly used one of 2 variables to define the split</li>
<li>For OOB test observations, the model misclassifies the individual’s survival 19% of the time.</li>
<li>This model is somewhat better at predicting survivors compared to the age + gender model, but is still worse at predicting survivors from the deceased. This is not terribly surprising since our classes are <em>unbalanced</em>. That is, there were a lot fewer survivors (342) than deceased (549). Because of this, the model has more information on those that died than those that lived, so it is natural to have better predictions for those that died.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(rf_full$finalModel)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/rf_validate_imp-1.png" width="672" /></p>
<p>Note that gender and age are important predictors in the random forest, but so too is the fare an individual paided. This is a proxy for socioeconomic status; recall that <a href="https://www.youtube.com/watch?v=NfDZO9QAiEM">the wealthy had better access to lifeboats</a>.</p>
<p>To make predictions for the validation set, we use the <code>predict</code> function. By setting <code>type = &quot;prob&quot;</code> we will get predicted probabilities for each possible outcome, rather than just a raw prediction of “Survived” or “Died”:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_pred &lt;-<span class="st"> </span>titanic_test %&gt;%
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(rf_full, <span class="dt">newdata =</span> titanic_test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>) %&gt;%
<span class="st">              </span><span class="kw">rename</span>(<span class="dt">prob_dead =</span> Died,
                     <span class="dt">prob_survive =</span> Survived))
titanic_pred</code></pre></div>
<pre><code>## # A tibble: 213 × 10
##    Survived Pclass    Sex   Age SibSp Parch     Fare Embarked prob_dead
##      &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;fctr&gt;     &lt;dbl&gt;
## 1  Survived      3 female    26     0     0   7.9250        S     0.536
## 2      Died      3   male    35     0     0   8.0500        S     0.996
## 3  Survived      2 female    14     1     0  30.0708        C     0.110
## 4      Died      3   male    20     0     0   8.0500        S     0.998
## 5      Died      2   male    35     0     0  26.0000        S     0.940
## 6  Survived      2   male    34     0     0  13.0000        S     0.972
## 7  Survived      3 female    38     1     5  31.3875        S     0.910
## 8      Died      1   male    19     3     2 263.0000        S     0.584
## 9      Died      1   male    40     0     0  27.7208        C     0.734
## 10     Died      2   male    66     0     0  10.5000        S     0.864
## # ... with 203 more rows, and 1 more variables: prob_survive &lt;dbl&gt;</code></pre>
</div>
</div>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li>For more information on statistical learning and the math behind these methods, see the awesome book <a href="http://link.springer.com.proxy.uchicago.edu/book/10.1007%2F978-1-4614-7138-7"><em>An Introduction to Statistical Learning</em></a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.1 (2016-06-21)
##  system   x86_64, darwin13.4.0        
##  ui       RStudio (1.0.44)            
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2016-11-16                  
## 
##  package      * version date       source                           
##  assertthat     0.1     2013-12-06 CRAN (R 3.3.0)                   
##  boot         * 1.3-18  2016-02-23 CRAN (R 3.3.1)                   
##  broom        * 0.4.1   2016-06-24 CRAN (R 3.3.0)                   
##  car            2.1-3   2016-08-11 CRAN (R 3.3.0)                   
##  caret        * 6.0-73  2016-11-10 CRAN (R 3.3.2)                   
##  class          7.3-14  2015-08-30 CRAN (R 3.3.1)                   
##  codetools      0.2-15  2016-10-05 CRAN (R 3.3.0)                   
##  colorspace     1.2-7   2016-10-11 CRAN (R 3.3.0)                   
##  DBI            0.5-1   2016-09-10 CRAN (R 3.3.0)                   
##  devtools       1.12.0  2016-06-24 CRAN (R 3.3.0)                   
##  digest         0.6.10  2016-08-02 CRAN (R 3.3.0)                   
##  dplyr        * 0.5.0   2016-06-24 CRAN (R 3.3.0)                   
##  e1071          1.6-7   2015-08-05 CRAN (R 3.3.0)                   
##  evaluate       0.10    2016-10-11 CRAN (R 3.3.0)                   
##  foreach        1.4.3   2015-10-13 CRAN (R 3.3.0)                   
##  foreign        0.8-67  2016-09-13 CRAN (R 3.3.0)                   
##  formatR        1.4     2016-05-09 CRAN (R 3.3.0)                   
##  gapminder    * 0.2.0   2015-12-31 CRAN (R 3.3.0)                   
##  gganimate    * 0.1     2016-11-11 Github (dgrtwo/gganimate@26ec501)
##  ggplot2      * 2.2.0   2016-11-10 Github (hadley/ggplot2@f442f32)  
##  gtable         0.2.0   2016-02-26 CRAN (R 3.3.0)                   
##  highr          0.6     2016-05-09 CRAN (R 3.3.0)                   
##  htmltools      0.3.5   2016-03-21 CRAN (R 3.3.0)                   
##  htmlwidgets    0.8     2016-11-09 CRAN (R 3.3.1)                   
##  ISLR         * 1.0     2013-06-11 CRAN (R 3.3.0)                   
##  iterators      1.0.8   2015-10-13 CRAN (R 3.3.0)                   
##  jsonlite       1.1     2016-09-14 CRAN (R 3.3.0)                   
##  knitr          1.15    2016-11-09 CRAN (R 3.3.1)                   
##  labeling       0.3     2014-08-23 CRAN (R 3.3.0)                   
##  lattice      * 0.20-34 2016-09-06 CRAN (R 3.3.0)                   
##  lazyeval       0.2.0   2016-06-12 CRAN (R 3.3.0)                   
##  lme4           1.1-12  2016-04-16 cran (@1.1-12)                   
##  lubridate    * 1.6.0   2016-09-13 CRAN (R 3.3.0)                   
##  magrittr       1.5     2014-11-22 CRAN (R 3.3.0)                   
##  MASS           7.3-45  2016-04-21 CRAN (R 3.3.1)                   
##  Matrix         1.2-7.1 2016-09-01 CRAN (R 3.3.0)                   
##  MatrixModels   0.4-1   2015-08-22 CRAN (R 3.3.0)                   
##  memoise        1.0.0   2016-01-29 CRAN (R 3.3.0)                   
##  mgcv           1.8-16  2016-11-07 CRAN (R 3.3.0)                   
##  minqa          1.2.4   2014-10-09 cran (@1.2.4)                    
##  mnormt         1.5-5   2016-10-15 CRAN (R 3.3.0)                   
##  ModelMetrics   1.1.0   2016-08-26 CRAN (R 3.3.0)                   
##  modelr       * 0.1.0   2016-08-31 CRAN (R 3.3.0)                   
##  munsell        0.4.3   2016-02-13 CRAN (R 3.3.0)                   
##  nlme           3.1-128 2016-05-10 CRAN (R 3.3.1)                   
##  nloptr         1.0.4   2014-08-04 cran (@1.0.4)                    
##  nnet           7.3-12  2016-02-02 CRAN (R 3.3.1)                   
##  pbkrtest       0.4-6   2016-01-27 CRAN (R 3.3.0)                   
##  plyr           1.8.4   2016-06-08 CRAN (R 3.3.0)                   
##  profvis      * 0.3.2   2016-05-19 CRAN (R 3.3.0)                   
##  psych          1.6.9   2016-09-17 cran (@1.6.9)                    
##  purrr        * 0.2.2   2016-06-18 CRAN (R 3.3.0)                   
##  quantreg       5.29    2016-09-04 CRAN (R 3.3.0)                   
##  R6             2.2.0   2016-10-05 CRAN (R 3.3.0)                   
##  randomForest * 4.6-12  2015-10-07 CRAN (R 3.3.0)                   
##  rcfss        * 0.1.0   2016-10-06 local                            
##  Rcpp           0.12.7  2016-09-05 cran (@0.12.7)                   
##  readr        * 1.0.0   2016-08-03 CRAN (R 3.3.0)                   
##  readxl       * 0.1.1   2016-03-28 CRAN (R 3.3.0)                   
##  reshape2       1.4.2   2016-10-22 CRAN (R 3.3.0)                   
##  rmarkdown    * 1.1     2016-10-16 CRAN (R 3.3.1)                   
##  rsconnect      0.5     2016-10-17 CRAN (R 3.3.0)                   
##  rstudioapi     0.6     2016-06-27 CRAN (R 3.3.0)                   
##  scales         0.4.1   2016-11-09 CRAN (R 3.3.1)                   
##  SparseM        1.72    2016-09-06 CRAN (R 3.3.0)                   
##  stringi        1.1.2   2016-10-01 CRAN (R 3.3.0)                   
##  stringr      * 1.1.0   2016-08-19 cran (@1.1.0)                    
##  tibble       * 1.2     2016-08-26 cran (@1.2)                      
##  tidyr        * 0.6.0   2016-08-12 CRAN (R 3.3.0)                   
##  tidyverse    * 1.0.0   2016-09-09 CRAN (R 3.3.0)                   
##  tree         * 1.0-37  2016-01-21 CRAN (R 3.3.0)                   
##  withr          1.0.2   2016-06-20 CRAN (R 3.3.0)                   
##  yaml           2.1.13  2014-06-12 CRAN (R 3.3.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The actual value you use is irrelevant. Just be sure to set it in the script, otherwise R will randomly pick one each time you start a new session.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Because <code>modelr</code> is a work in progress, it doesn’t always play nicely with base R functions. Note how I refer directly to the test<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><code>pretty = 0</code> cleans up the formatting of the text some.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Specifically passenger class, gender, age, number of sibling/spouses aboard, number of parents/children aboard, fare, and port of embarkation.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Because behind the scenes, <code>caret</code> is simply using the <code>glm</code> function to train the model.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="https://topepo.github.io/caret/train-models-by-tag.html#random-forest">There are many packages that use algorithms to estimate random forests.</a> They all do the same basic thing, though with some notable differences. The <code>rf</code> method is generally popular, so I use it here.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Remember that it was not generated by the <code>tree</code> library, but instead by a function in <code>randomForest</code>. Because of that we cannot just call <code>plot(age_sex_rf$finalModel)</code>.<a href="#fnref7">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
